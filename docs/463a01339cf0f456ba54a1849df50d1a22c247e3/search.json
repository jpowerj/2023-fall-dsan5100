[
  {
    "objectID": "w01/index.html",
    "href": "w01/index.html",
    "title": "Week 1: Welcome to DSAN 5100!",
    "section": "",
    "text": "(Week 1 of DSAN 5100 was a joint session across all individual sections, taught on Zoom.)\n\n\n\n\n\n\nToday‚Äôs Links\n\n\n\n\nWeek 1 Lecture Notes\nWeek 1 Lecture Recording"
  },
  {
    "objectID": "w02/slides.html#prof.-jeff-introduction",
    "href": "w02/slides.html#prof.-jeff-introduction",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Prof.¬†Jeff Introduction!",
    "text": "Prof.¬†Jeff Introduction!\n\nBorn and raised in NW DC ‚Üí high school in Rockville, MD\nUniversity of Maryland: Computer Science, Math, Economics (2008-2012)"
  },
  {
    "objectID": "w02/slides.html#grad-school",
    "href": "w02/slides.html#grad-school",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Grad School",
    "text": "Grad School\n\nStudied abroad in Beijing (Peking University/ÂåóÂ§ß) ‚Üí internship with Huawei in Hong Kong (HKUST)\n\n\n\n\nStanford for MS in Computer Science (2012-2014)\nResearch Economist at UC Berkeley (2014-2015)\n\n\n\n\n\n\nColumbia (NYC) for PhD[+Postdoc] in Political Science (2015-2023)"
  },
  {
    "objectID": "w02/slides.html#dissertation-political-science-history",
    "href": "w02/slides.html#dissertation-political-science-history",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Dissertation (Political Science + History)",
    "text": "Dissertation (Political Science + History)\n‚ÄúOur Word is Our Weapon‚Äù: Text-Analyzing Wars of Ideas from the French Revolution to the First Intifada"
  },
  {
    "objectID": "w02/slides.html#research-labor-economics",
    "href": "w02/slides.html#research-labor-economics",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Research (Labor Economics)",
    "text": "Research (Labor Economics)\n\n\n\n‚ÄúMonopsony in Online Labor Markets‚Äù: Machine Learning to enhance causal estimates of the effect of job description language on uptake rate\n\n\n\n‚ÄúFreedom as Non-Domination in the Labor Market‚Äù: Game-theoretic models of workers‚Äô rights (monopsony vs.¬†labor discipline)\n\n\n\n\n\n‚ÄúUnsupervised Extraction of Workplace Rights and Duties from Collective Bargaining Agreements‚Äù: Linguistic (dependency) parses of contracts ‚Üí time series of worker vs.¬†employer rights and responsibilities over time"
  },
  {
    "objectID": "w02/slides.html#deterministic-processes",
    "href": "w02/slides.html#deterministic-processes",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Deterministic Processes",
    "text": "Deterministic Processes\n\nGiven a set of inputs, we can compute the outcome exactly\nExample: Given the radius of a circle, we can compute its area without any uncertainty. \\(r \\mapsto \\pi r^2\\)\n(The fact that we can compute the outcome doesn‚Äôt mean that it‚Äôs easy to do so! See, e.g., the double pendulum)\n\n\nImage credit: Tenor.com\nThe pendulum example points to the fact that the notion of a chaotic system, one which is ‚Äúsensitive to initial conditions‚Äù, is different from that of a stochastic system."
  },
  {
    "objectID": "w02/slides.html#holy-grail-deterministic-model-newtonian-physics",
    "href": "w02/slides.html#holy-grail-deterministic-model-newtonian-physics",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "‚ÄúHoly Grail‚Äù Deterministic Model: Newtonian Physics",
    "text": "‚ÄúHoly Grail‚Äù Deterministic Model: Newtonian Physics\n\n\n\n\n\n\n\n\n\n\ngrid\n\n \n\ncluster_01\n\n ‚ÄúNature‚Äù  \n\ncluster_02\n\n ‚ÄúScience‚Äù   \n\nObs\n\n Thing(s) we can see   \n\nUnd\n\n Underlying processes   \n\nUnd-&gt;Obs\n\n    \n\nModel\n\n Model   \n\nUnd-&gt;Model\n\n    \n\nModel-&gt;Obs\n\n   \n\n\n\n\n\n\n\n\n\n\n\n\n\ngrid\n\n \n\ncluster_04\n\n  Woolsthorpe Manor    \n\ncluster_03\n\n Isaac Newton   \n\nTree\n\n  Falling Apple     \n\nPhysics\n\n  Particle Interactions     \n\nPhysics-&gt;Tree\n\n    \n\nNewton\n\n Newton‚Äôs Laws   \n\nPhysics-&gt;Newton\n\n    \n\nNewton-&gt;Tree\n\n   \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\n\\leadsto F_g = G\\frac{m_1m_2}{r^2}\n\\]\nFigure¬†1: Newton‚Äôs Law of Universal Gravitation‚Üê Dr.¬†Zirkel follows Newton‚Äôs famous steps. Coloured wood engraving. Wellcome Collection (Public Domain)"
  },
  {
    "objectID": "w02/slides.html#but-what-happens-when",
    "href": "w02/slides.html#but-what-happens-when",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "But What Happens When‚Ä¶",
    "text": "But What Happens When‚Ä¶\n\\[\n\\text{Outcome}\\left(\\text{Dice Roll}\\right) = \\; ?\\frac{?_1?_2}{?^2}\n\\]\n\n\n\nPre-Enlightenment\n\n\n\n\nHans Sebald Beham, Fortuna (1541), CC BY 4.0, via Wikimedia Commons\n\n\n\n\nPost-Enlightenment\n\n\n\n\nBlaise Pascal, Trait√© du triangle arithm√©tique (1665). Public Domain, via Internet Archive"
  },
  {
    "objectID": "w02/slides.html#random-processes",
    "href": "w02/slides.html#random-processes",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Random Processes",
    "text": "Random Processes\n\n\n\nCan‚Äôt compute the outcome exactly, but can still say something about potential outcomes!\nExample: randomly chosen radius \\(r \\in [0,1]\\), what can we say about \\(A = \\pi r^2\\)?\n\nUnif: \\([0,\\pi]\\) equally likely\nExp: closer to \\(0\\) more likely"
  },
  {
    "objectID": "w02/slides.html#data-ground-truth-noise",
    "href": "w02/slides.html#data-ground-truth-noise",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Data = Ground Truth + Noise",
    "text": "Data = Ground Truth + Noise\n\nDepressing but true origin of statistics (as opposed to probability): the Plague üò∑\n\n\n\n\n\n\n\nGround Truth: The Great Plague (Lord Have Mercy on London, Unknown Artist, circa 1665, via Wikimedia Commons)\n\n\n\n\n\n\n\nNoisy Data (Recorded amidst chaos): London Bill of Mortality, 1665 (Public Domain, Wellcome Collection)"
  },
  {
    "objectID": "w02/slides.html#random-variables",
    "href": "w02/slides.html#random-variables",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Random Variables",
    "text": "Random Variables\n\nIn algebra, to solve problems we work with variables\nIn probability theory, to solve problems we work with random variables\nRecall the difference between random and deterministic: \\(A = \\pi r^2\\) tells us that, given a value of \\(r\\), we can solve for the unique value of \\(A\\)\nIn probability theory, however, there is no one ‚Äútrue‚Äù value of a random variable \\(X\\).\nLet \\(X = f(N)\\) mean that \\(X\\) is the result of a rolled die, where the die has \\(N\\) sides.\nPlugging in \\(N = 6\\) (standard 6-sided die) still doesn‚Äôt mean we know ‚Äúthe‚Äù value of \\(X\\). However, (if the die is fair) we do know\n\n\\[\n\\Pr(X = 1) = \\Pr(X = 2) = \\cdots = \\Pr(X = 6) = \\frac{1}{6}\n\\]"
  },
  {
    "objectID": "w02/slides.html#discrete-vs.-continuous",
    "href": "w02/slides.html#discrete-vs.-continuous",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Discrete vs.¬†Continuous",
    "text": "Discrete vs.¬†Continuous\n\nMany complicated definitions, often misleading or unintuitive!\nHow I want you to remember: How many possible values between two known values?\nDiscrete: e.g., number of siblings\n\nI have 2 siblings, you have 3 siblings‚Ä¶ How many values (sibling counts) in between?\n\nContinuous: e.g., temperature\n\nIt is 27.0¬∞ C in my room, 28.0¬∞ C in your room‚Ä¶ How many values (temperatures) in between?\n\nSo, if \\(X\\) is the result of a rolled die, is \\(X\\) discrete or continuous? How many values can be rolled between 3 and 4?"
  },
  {
    "objectID": "w02/slides.html#thinking-about-independence",
    "href": "w02/slides.html#thinking-about-independence",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Thinking About Independence",
    "text": "Thinking About Independence\n\nWe‚Äôll define it formally later; for now, this is our working definition:\n\n\n\n\n\n\n\nWorking Definition: Independence\n\n\nTwo random variables \\(X\\) and \\(Y\\) are independent if learning information about \\(X\\) does not give you information about the value of \\(Y\\), or vice-versa."
  },
  {
    "objectID": "w02/slides.html#na√Øve-definition-of-probability",
    "href": "w02/slides.html#na√Øve-definition-of-probability",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Na√Øve Definition of Probability",
    "text": "Na√Øve Definition of Probability\n\nSample Space: The set of all possible outcomes of an experiment\nEvent: A subset of the sample space\n\n\n\n\n\n\n\nNa√Øve Definition of Probability\n\n\nGiven a sample space \\(S\\), and an event \\(E \\subset S\\),\n\\[\n\\Pr(\\underbrace{E}_{\\text{event}}) = \\frac{\\text{\\# Favorable Outcomes}}{\\text{\\# Possible Outcomes}} = \\frac{|E|}{|S|}\n\\]"
  },
  {
    "objectID": "w02/slides.html#example-flipping-two-coins",
    "href": "w02/slides.html#example-flipping-two-coins",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Example: Flipping Two Coins",
    "text": "Example: Flipping Two Coins\n\n\n\n\n\n\nNa√Øve Definition of Probability\n\n\nGiven a sample space \\(S\\), and an event \\(E \\subset S\\),\n\\[\n\\Pr(\\underbrace{E}_{\\text{event}}) = \\frac{\\text{\\# Favorable Outcomes}}{\\text{\\# Possible Outcomes}} = \\frac{|E|}{|S|}\n\\]\n\n\n\n\nFlipping two coins:\n\nSample space \\(S = \\{TT, TH, HT, HH\\}\\)\nEvent \\(E_1\\): Result of first flip is \\(H\\), result of second flip is \\(T\\) \\(\\implies\\) \\(E_1 = \\{HT\\}\\).\nEvent \\(E_2\\): At least one \\(H\\) \\(\\implies\\) \\(E_2 = \\{TH, HT, HH\\}\\).\n\n\n\\[\n\\begin{align*}\n\\Pr(E_1) &= \\frac{|\\{HT\\}|}{|S|} = \\frac{|\\{HT\\}|}{|\\{TT, TH, HT, HH\\}|} = \\frac{1}{4} \\\\\n\\Pr(E_2) &= \\frac{|\\{TH, HT, HH\\}|}{|S|} = \\frac{|\\{TH, HT, HH\\}|}{|\\{TT, TH, HT, HH\\}|} = \\frac{3}{4}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "w02/slides.html#events-neq-outcomes",
    "href": "w02/slides.html#events-neq-outcomes",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Events \\(\\neq\\) Outcomes!",
    "text": "Events \\(\\neq\\) Outcomes!\n\nOutcomes are things, events are sets of things\nSubtle but extremely important distinction!\nIn the coin flip example:\n\nThe event \\(E_1 = \\{HT\\}\\) can be confused with the outcome \\(HT\\).\nSo, try to remember instead the event \\(E_2 = \\{TH, HT, HH\\}\\): it is more clear, in this case, how this event does not correspond to any individual outcome"
  },
  {
    "objectID": "w02/slides.html#back-to-the-na√Øve-definition",
    "href": "w02/slides.html#back-to-the-na√Øve-definition",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Back to the Na√Øve Definition",
    "text": "Back to the Na√Øve Definition\n\n\n\n\n\n\nNa√Øve Definition of Probability\n\n\nGiven a sample space \\(S\\), and an event \\(E \\subset S\\),\n\\[\n\\Pr(\\underbrace{E}_{\\text{event}}) = \\frac{\\text{\\# Favorable Outcomes}}{\\text{\\# Possible Outcomes}} = \\frac{|E|}{|S|}\n\\]\n\n\n\n\nThe na√Øve definition tells us that probabilities are just ratios of counts:\n\nCount the number of ways the event \\(E\\) can happen, count the total number of things that can happen, and divide!\n\nThis is why we begin studying probability by studying combinatorics: the mathematics of counting"
  },
  {
    "objectID": "w02/slides.html#combinatorics-ice-cream-possibilities",
    "href": "w02/slides.html#combinatorics-ice-cream-possibilities",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Combinatorics: Ice Cream Possibilities",
    "text": "Combinatorics: Ice Cream Possibilities\n\n\n\n\n\n\n\nThe \\(6 = 2 \\cdot 3\\) possible cone+flavor combinations which can result from choosing a flavor first and a cone type second.\n\n\n\n\n\n\n\nThe \\(6 = 3 \\cdot 2\\) possible cone+flavor combinations which can result from choosing a flavor first and a cone type second."
  },
  {
    "objectID": "w02/slides.html#grouping-vs.-ordering",
    "href": "w02/slides.html#grouping-vs.-ordering",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Grouping vs.¬†Ordering",
    "text": "Grouping vs.¬†Ordering\n\nIn standard statistics/combinatorics introductions you‚Äôll learn different counting formulas for when order matters vs.¬†when order doesn‚Äôt matter\nThis is not a mathematical distinction so much as a pragmatic distinction: what are you trying to accomplish by counting?\nProblems with extremely similar descriptions can differ in small detail, so that the units you need to distinguish between in one version differ from the units you need to distinguish between in the other."
  },
  {
    "objectID": "w02/slides.html#does-order-matter",
    "href": "w02/slides.html#does-order-matter",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Does Order Matter?",
    "text": "Does Order Matter?\n\n\n\n\n\n\nExample: Student Government vs.¬†Student Sports\n\n\n\nConsider a school where students can either try out for the swim team or run for a position in the student government\nThe swim team has 4 slots, but slots aren‚Äôt differentiated: you‚Äôre either on the team (one of the 4 chosen students) or not\nThe student government also has 4 slots, but there is a difference between the slots: first slot is President, second is Vice President, third is Secretary, and fourth is Treasurer.\n\n\n\n\n\nSimple case (for intuition): the school only has 4 students. In this case, how many ways are there to form the swim team? What about the student government?\n\nSwim team: \\(1\\) way. You have only one choice, to let all 4 students onto the team\nStudent government: \\(4 \\cdot 3 \\cdot 2 \\cdot 1 = 24\\) ways. You have to let all 4 students in, but you have a choice of who is President, Vice President, Secretary, and Treasurer\n\nHow did we get \\(4 \\cdot 3 \\cdot 2 \\cdot 1\\)? (Think about the ice cream example‚Ä¶)\n\nStart by choosing the President: 4 choices\nNow choose the Vice President: only 3 students left to choose from\nNow choose the Secretary: only 2 students left to choose from\nNow choose the Treasurer: only 1 student left to choose from"
  },
  {
    "objectID": "w02/slides.html#permutations-vs.-combinations",
    "href": "w02/slides.html#permutations-vs.-combinations",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Permutations vs.¬†Combinations",
    "text": "Permutations vs.¬†Combinations\n\nPermutations: How many ways can I choose groups of size \\(k\\) out of \\(n\\) total objects, where order within groups matters: \\(P_{n,k}\\) (sometimes written \\(_nP_k\\)).\n\nIn this case, we want to count \\((a,b)\\) and \\((b,a)\\) as two separate groups\n\nCombinations: How many ways can I choose groups of size \\(k\\) out of \\(n\\) total objects, where order in the groups doesn‚Äôt matter: \\(C_{n,k}\\) (sometimes written \\(_nC_k,\\binom{n}{k}\\)).\n\nIn this case, we don‚Äôt want to count \\((a, b)\\) and \\((b, a)\\) as two separate groups‚Ä¶\n\n\n\\[\n\\begin{align*}\nP_{n,k} = \\frac{n!}{(n-k)!}, \\; C_{n,k} = \\frac{n!}{k!(n-k)!}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "w02/slides.html#no-need-to-memorize",
    "href": "w02/slides.html#no-need-to-memorize",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "No Need to Memorize!",
    "text": "No Need to Memorize!\n\n\nKey point: you don‚Äôt have to remember these as two separate formulas!\nThe number of combinations is based on the number of permutations, but corrected for double counting: e.g., corrected for the fact that \\((a,b) \\neq (b,a)\\) when counting permutations but \\((a,b) = (b,a)\\) when counting combinations.\n\n\\[\nC_{n,k} = \\frac{P_{n,k}}{k!} \\genfrac{}{}{0pt}{}{\\leftarrow \\text{Permutations}}{\\leftarrow \\text{Duplicate groups}}\n\\]\nWhere does \\(k!\\) come from? (How many different orderings can we make of the same group?)\n\n\\(k = 2\\): \\((\\underbrace{\\boxed{\\phantom{a}}}_{\\text{2 choices}},\\underbrace{\\boxed{\\phantom{a}}}_{\\text{1 remaining choice}}) \\implies 2\\)\n\\(k = 3\\): \\((\\underbrace{\\boxed{\\phantom{a}}}_{\\text{3 choices}},\\underbrace{\\boxed{\\phantom{a}}}_{\\text{2 remaining choices}}, \\underbrace{\\boxed{\\phantom{a}}}_{\\text{1 remaining choice}}) \\implies 6\\)\n\\(k = 4\\): \\((\\underbrace{\\boxed{\\phantom{a}}}_{\\text{4 choices}}, \\underbrace{\\boxed{\\phantom{a}}}_{\\text{3 remaining choices}}, \\underbrace{\\boxed{\\phantom{a}}}_{\\text{2 remaining choices}}, \\underbrace{\\boxed{\\phantom{a}}}_{\\text{1 remaining choice}}) \\implies 24\\)"
  },
  {
    "objectID": "w02/slides.html#with-or-without-replacement",
    "href": "w02/slides.html#with-or-without-replacement",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "With or Without Replacement?",
    "text": "With or Without Replacement?\n\nBoils down to: can the same object be included in my sample more than once?\n\n\n\n\nWithout Replacement\nWith Replacement\n\n\n\n\nMost statistical problems: ‚ÄúCheck off‚Äù objects as you collect data about them, so that each observation in your data is unique\nVery special (but very important!) set of statistical problems: allow objects to appear in your sample multiple times, to ‚Äúsqueeze‚Äù more information out of the sample (called Bootstrapping‚Äîmuch more on this later in the course!)"
  },
  {
    "objectID": "w02/slides.html#how-many-possible-samples",
    "href": "w02/slides.html#how-many-possible-samples",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "How Many Possible Samples?",
    "text": "How Many Possible Samples?\n\n\n\n\n\n\nExample: How Many Possible Samples?\n\n\nFrom a population of \\(N = 3\\), how many ways can we take samples of size \\(k = 2\\)?\n\n\n\n\n\n\n\nWithout Replacement\nWith Replacement\n\n\n\n\n\\(3 \\cdot 2 = 6\\) ways (3 objects to choose from for first element of sample, 2 remaining objects to choose from for second element of sample)\n\\(3\\cdot 3 = 3^2 = 9\\) ways (3 objects to choose from for first element of sample, still 3 objects to choose from for second element of sample)\n\n\n\n\n\n\n\n\n\n\n\n\nResult: How Many Possible Samples\n\n\nFrom a population of size \\(N\\), how many ways can we take samples of size \\(k\\)? (Try to extrapolate from above examples before looking at answer!)\n\n\n\n\n\n\n\nWithout Replacement\nWith Replacement\n\n\n\n\n\\(\\displaystyle \\underbrace{N \\cdot (N-1) \\cdot \\cdots \\cdot (N - k + 1)}_{k\\text{ times}} = \\frac{N!}{(N - k )!}\\)(This formula should look somewhat familiar‚Ä¶)\n\\(\\displaystyle \\underbrace{N \\cdot N \\cdot \\cdots \\cdot N}_{k\\text{ times}} = N^k\\)"
  },
  {
    "objectID": "w02/slides.html#logic-sets-and-probability",
    "href": "w02/slides.html#logic-sets-and-probability",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Logic, Sets, and Probability",
    "text": "Logic, Sets, and Probability\n\nThere is a deep connection1 between the objects and operations of logic, set theory, and probability:\n\n\n\n\n\n\n\n\n\n\n\nLogic\nSet Theory\nProbability Theory\n\n\n\n\nObjects\nPredicates\\(p, q \\in \\{T, F\\}\\)\nSets\\(S = \\{a, b, \\ldots\\}\\)\nEvents\\(E = \\{TH, HT, HH\\}\\)\n\n\nConjunction\nAnd (\\(\\wedge\\))\\(p \\wedge q\\)\nIntersection (\\(\\cap\\))\\(A \\cap B\\)\nMultiplication (\\(\\times\\)):\\(\\Pr(E_1 \\cap E_2) = \\Pr(E_1)\\times \\Pr(E_2)\\)\n\n\nDisjunction\nOr (\\(\\vee\\))\\(p \\vee q\\)\nUnion (\\(\\cup\\))\\(A \\cup B\\)\nAddition (\\(+\\)): \\(\\Pr(E_1 \\cup E_2) =\\)\\(\\Pr(E_1) + \\Pr(E_2) - \\Pr(E_1 \\wedge E_2)\\)\n\n\nNegation\nNot (\\(\\neg\\))\\(\\neg p\\)\nComplement (\\(^c\\))\\(S^c\\)\nSubtract from 1\\(\\Pr(A^c) = 1 - \\Pr(A)\\)\n\n\n\nFor math majors, you can think of it as an isomorphism between the objects and operations of the three subjects"
  },
  {
    "objectID": "w02/slides.html#example-flipping-two-coins-1",
    "href": "w02/slides.html#example-flipping-two-coins-1",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Example: Flipping Two Coins",
    "text": "Example: Flipping Two Coins\n\nLogic: We can define 4 predicates:\n\n\\(p_1\\) = ‚ÄúFirst result is \\(H\\)‚Äù, \\(q_1\\) = ‚ÄúFirst result is \\(T\\)‚Äù\n\\(p_2\\) = ‚ÄúSecond result is \\(H\\)‚Äù, \\(q_2\\) = ‚ÄúSecond result is \\(T\\)‚Äù\n\nLogical formulas:\n\n\\(f_1 = p_1 \\wedge q_2\\): ‚ÄúFirst result is \\(H\\) and second result is \\(T\\)‚Äù\n\\(f_2 = p_1 \\vee q_2\\): ‚ÄúFirst result is \\(H\\) or second result is \\(T\\)‚Äù\n\\(f_3 = \\neg p_1\\): ‚ÄúFirst result is not \\(H\\)‚Äù\n\nThe issue?: We don‚Äôt know, until after the coins have been flipped, whether these are true or false!\nBut, we should still be able to say something about their likelihood, for example, whether \\(f_1\\) or \\(f_2\\) is more likely to happen‚Ä¶ Enter probability theory!"
  },
  {
    "objectID": "w02/slides.html#logic-rightarrow-probability",
    "href": "w02/slides.html#logic-rightarrow-probability",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Logic \\(\\rightarrow\\) Probability",
    "text": "Logic \\(\\rightarrow\\) Probability\n\nProbability theory lets us reason about the uncertainty surrounding logical predicates like \\(p\\) and \\(q\\), by:\n\nencoding them as sets of possibilities \\(P\\) and \\(Q\\), and\nrepresenting the uncertainty around any given possibility using a probability measure \\(\\Pr: S \\mapsto [0,1]\\),\n\nthus allowing us to reason about\n\nthe likelihood of these set-encoded predicates on their own: \\(\\Pr(P)\\) and \\(\\Pr(Q)\\), but also\ntheir logical connections: \\(\\Pr(p \\wedge q) = \\Pr(P \\cap Q)\\), \\(\\Pr(\\neg p) = \\Pr(P^c)\\), and so on."
  },
  {
    "objectID": "w02/slides.html#flipping-two-coins-logic-rightarrow-probability",
    "href": "w02/slides.html#flipping-two-coins-logic-rightarrow-probability",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Flipping Two Coins: Logic \\(\\rightarrow\\) Probability",
    "text": "Flipping Two Coins: Logic \\(\\rightarrow\\) Probability\n\nReturning to the two coins example: we can look at the predicates and see that they exhaust all possibilities, so that we can define a sample space \\(S = \\{TT, TH, HT, HH\\}\\) of all possible outcomes of our coin-flipping experiment, noting that \\(|S| = 4\\), so there are 4 possible outcomes.\nThen we can associate each predicate with an event, a subset of the sample space, and use our na√Øve definition to compute the probability of these events:\n\n\n\n\n\n\n\n\n\nPredicate\nEvent\nProbability\n\n\n\n\n\\(p_1\\) = ‚ÄúFirst result is \\(H\\)‚Äù\n\\(P_1 = \\{HT, HH\\}\\)\n\\(\\Pr(P_1) = \\frac{|P_1|}{|S|} = \\frac{2}{4} = \\frac{1}{2}\\)\n\n\n\\(q_1\\) = ‚ÄúFirst result is \\(T\\)‚Äù\n\\(Q_1 = \\{TT, TH\\}\\)\n\\(\\Pr(Q_1) = \\frac{|Q_1|}{|S|} = \\frac{2}{4} = \\frac{1}{2}\\)\n\n\n\\(p_2\\) = ‚ÄúSecond result is \\(H\\)‚Äù\n\\(P_2 = \\{TH, HH\\}\\)\n\\(\\Pr(P_2) = \\frac{|P_2|}{|S|} = \\frac{2}{4} = \\frac{1}{2}\\)\n\n\n\\(q_2\\) = ‚ÄúSecond result is \\(T\\)‚Äù\n\\(Q_2 = \\{TT, HT\\}\\)\n\\(\\Pr(Q_2) = \\frac{|Q_2|}{|S|} = \\frac{2}{4} = \\frac{1}{2}\\)"
  },
  {
    "objectID": "w02/slides.html#moving-from-predicates-to-formulas",
    "href": "w02/slides.html#moving-from-predicates-to-formulas",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Moving from Predicates to Formulas",
    "text": "Moving from Predicates to Formulas\n\nNotice that, in the four rows of the previous table, we were only computing the probabilities of ‚Äúsimple‚Äù events: events corresponding to a single predicate\nBut we promised that probability theory lets us compute probabilities for logical formulas as well! ‚Ä¶The magic of encoding events as sets becomes clear:\n\n\n\n\n\n\n\n\n\nFormula\nEvent\nProbability\n\n\n\n\n\\(f_1 = p_1 \\wedge q_2\\)\n\\[ \\begin{align*} F_1 &= P_1 \\cap Q_2 \\\\ &= \\{HT, HH\\} \\cap \\{TT, HT\\} \\\\ &= \\{HT\\} \\end{align*} \\]\n\\[\\begin{align*} \\Pr(F_1) &= \\Pr(\\{HT\\}) \\\\ &= \\frac{|\\{HT\\}|}{|S|} = \\frac{1}{4} \\end{align*}\\]\n\n\n\\(f_2 = p_1 \\vee q_2\\)\n\\[\\begin{align*} F_2 &= P_1 \\cup Q_2 \\\\ &= \\{HT, HH\\} \\cup \\{TT, HT\\} \\\\ &= \\{TT, HT, HH\\} \\end{align*}\\]\n\\[\\begin{align*} \\Pr(F_2) &= \\Pr(\\{TT, HT, HH\\}) \\\\ &= \\frac{|\\{TT, HT, HH\\}|}{|S|} = \\frac{3}{4} \\end{align*}\\]\n\n\n\\(f_3 = \\neg p_1\\)\n\\[\\begin{align*} F_3 &= P_1^c \\\\ &= \\{HT, HH\\}^c \\\\ &= \\{TT, TH\\}\\end{align*}\\]\n\\[\\begin{align*} \\Pr(F_3) &= \\Pr(\\{TT, TH\\}) \\\\ &= \\frac{|\\{TT, TH\\}|}{|S|} = \\frac{2}{4} = \\frac{1}{2} \\end{align*}\\]"
  },
  {
    "objectID": "w02/slides.html#using-rules-of-probability",
    "href": "w02/slides.html#using-rules-of-probability",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Using ‚ÄúRules‚Äù of Probability",
    "text": "Using ‚ÄúRules‚Äù of Probability\n\nHopefully, though, you found all this churning through set theory to be a bit tedious‚Ä¶\nThis is where rules of probability come from! They simplify set-theoretic computations into simple multiplications, additions, and subtractions:\n\n\\(\\Pr(A \\cap B) = \\Pr(A) \\times \\Pr(B)\\)\n\\(\\Pr(A \\cup B) = \\Pr(A) + \\Pr(B) - \\Pr(A \\cap B)\\)\n\\(\\Pr(A^c) = 1 - \\Pr(A)\\)\n\nSince we know probabilities of the ‚Äúsimple‚Äù events \\(P_1\\), \\(Q_1\\), \\(P_2\\), \\(Q_2\\), we don‚Äôt need to ‚Äúlook inside them‚Äù! Just take the probabilities and multiply/add/subtract as needed:\n\n\n\n\n\n\n\n\n\nFormula\nEvent\nProbability\n\n\n\n\n\\(f_1 = p_1 \\wedge q_2\\)\n\\(F_1 = P_1 \\cap Q_2\\)\n\\(\\Pr(F_1) = \\Pr(P_1) \\times \\Pr(Q_2) = \\frac{1}{2}\\times \\frac{1}{2} = \\frac{1}{4}\\)\n\n\n\\(f_2 = p_1 \\vee q_2\\)\n\\(F_2 = P_1 \\cup Q_2\\)\n\\[\\textstyle{\\begin{align*} \\textstyle \\Pr(F_2) &= \\Pr(P_1) + \\Pr(Q_2) - \\Pr(P_1 \\cap Q_2) \\\\ \\textstyle &= \\frac{1}{2} + \\frac{1}{2} - \\frac{1}{4} = \\frac{3}{4} \\end{align*}}\\]\n\n\n\\(f_3 = \\neg p_1\\)\n\\(F_3 = P_1^c\\)\n\\(\\Pr(F_3) = 1 - \\Pr(P_1) = 1 - \\frac{1}{2} = \\frac{1}{2}\\)"
  },
  {
    "objectID": "w02/slides.html#importing-results-from-logic",
    "href": "w02/slides.html#importing-results-from-logic",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "‚ÄúImporting‚Äù Results from Logic",
    "text": "‚ÄúImporting‚Äù Results from Logic\n\nThis deep connection between the three fields means that, if we have some useful theorem or formula from one field, we can immediately put it to use in another!\nFor example: DeMorgan‚Äôs Laws were developed in logic (DeMorgan was a 19th-century logician), and basically just tell us how to distribute logic operators:\n\n\\[\n\\begin{align*}\n\\underbrace{\\neg(p \\wedge q)}_{\\text{``}p\\text{ and }q\\text{'' is not true}} &\\iff \\underbrace{\\neg p \\vee \\neg q}_{p\\text{ is not true or }q\\text{ is not true}} \\\\\n\\underbrace{\\neg(p \\vee q)}_{\\text{``}p\\text{ or }q\\text{'' is not true}} &\\iff \\underbrace{\\neg p \\wedge \\neg q}_{p\\text{ is not true and }q\\text{ is not true}}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "w02/slides.html#converting-to-probability-theory",
    "href": "w02/slides.html#converting-to-probability-theory",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Converting to Probability Theory",
    "text": "Converting to Probability Theory\n\nSo, using the same principles we used in our coin flipping examples, we can consider events \\(P\\) and \\(Q\\), and get the following ‚Äútranslation‚Äù of DeMorgan‚Äôs Laws:\n\n\n\n\n\n\n\n\n\nLogic\nSet Theory\nProbability Theory\n\n\n\n\n\\(\\neg(p \\wedge q) = \\neg p \\vee \\neg q\\)\n\\((P \\cap Q)^c = P^c \\cup Q^c\\)\n\\(\\Pr((P \\cap Q)^c) = \\Pr(P^c \\cup Q^c)\\)\n\n\n\\(\\neg(p \\vee q) = \\neg p \\wedge \\neg q\\)\n\\((P \\cup Q)^c = P^c \\cap Q^c\\)\n\\(\\Pr((P \\cup Q)^c) = \\Pr(P^c \\cap Q^c)\\)\n\n\n\n\nNote that, since these are isomorphic to one another, we could have derived DeMorgan‚Äôs Laws from within probability theory, rather than the other way around:\n\n\\[\n\\begin{align*}\n\\Pr((P \\cap Q)^c) &= 1 - \\Pr(P \\cap Q) = 1 - \\Pr(P)\\Pr(Q) \\\\\n&= 1 - (1-\\Pr(P^c))(1 - \\Pr(Q^c)) \\\\\n&= 1 - [1 - \\Pr(P^c) - \\Pr(Q^c) + \\Pr(P^c)\\Pr(Q^c)] \\\\\n&= \\Pr(P^c) + \\Pr(Q^c) - \\Pr(P^c)\\Pr(Q^c) \\\\\n&= \\Pr(P^c) + \\Pr(Q^c) - \\Pr(P^c \\cap Q^c) \\\\\n&= \\Pr(P^c \\cup Q^c) \\; ‚úÖ\n\\end{align*}\n\\]"
  },
  {
    "objectID": "w02/slides.html#random-variables-1",
    "href": "w02/slides.html#random-variables-1",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Random Variables",
    "text": "Random Variables\n\nRecall our discussion of random variables: used by analogy to algebra, since we can do math with them:\nJust as \\(2 \\cdot 3\\) is shorthand for \\(2 + 2 + 2\\), we can define \\(X\\) as shorthand for the possible outcomes of a random process. \\[\n\\begin{align*}\nS = \\{ &\\text{result of dice roll is 1}, \\\\\n&\\text{result of dice roll is 2}, \\\\\n&\\text{result of dice roll is 3}, \\\\\n&\\text{result of dice roll is 4}, \\\\\n&\\text{result of dice roll is 5}, \\\\\n&\\text{result of dice roll is 6}\\} \\rightsquigarrow X \\in \\{1,\\ldots,6\\}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "w02/slides.html#random-variables-as-events",
    "href": "w02/slides.html#random-variables-as-events",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Random Variables as Events",
    "text": "Random Variables as Events\n\nEach value \\(v_X\\) that a random variable \\(X\\) can take on gives rise to an event \\(X = v_X\\): the event that the random variable \\(X\\) takes on value \\(v\\).\nSince \\(X = v_X\\) is an event, we can compute its probability \\(\\Pr(X = v_X)\\)!\n\n\n\n\nEvent in words\nEvent in terms of RV\n\n\n\n\nResult of dice roll is 1\n\\(X = 1\\)\n\n\nResult of dice roll is 2\n\\(X = 2\\)\n\n\nResult of dice roll is 3\n\\(X = 3\\)\n\n\nResult of dice roll is 4\n\\(X = 4\\)\n\n\nResult of dice roll is 5\n\\(X = 5\\)\n\n\nResult of dice roll is 6\n\\(X = 6\\)"
  },
  {
    "objectID": "w02/slides.html#doing-math-with-events",
    "href": "w02/slides.html#doing-math-with-events",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Doing Math with Events",
    "text": "Doing Math with Events\n\nWe‚Äôve seen how \\(\\Pr(\\cdot)\\) can ‚Äúencode‚Äù logical expressions involving uncertain outcomes.\nEven more powerful when paired with the notion of random variables: lets us also ‚Äúencode‚Äù mathematical expressions involving uncertain quantities!\nConsider an experiment where we roll two dice. Let \\(X\\) be the RV encoding the outcome of the first roll, and \\(Y\\) be the RV encoding the outcome of the second roll.\nWe can compute probabilities involving \\(X\\) and \\(Y\\) separately, e.g., \\(\\Pr(X = 1) = \\frac{1}{6}\\), but we can also reason probabilistically about mathematical expressions involving \\(X\\) and \\(Y\\)! For example, we can reason about their sum:\n\n\\[\n\\begin{align*}\n\\Pr(\\text{rolls sum to 10}) &= \\Pr(X + Y = 10) \\\\\n&= \\Pr(Y = 10 - X)\n\\end{align*}\n\\]\n\nOr about how the outcome of one roll will relate to the outcome of the other:\n\n\\[\n\\begin{align*}\n\\Pr(\\text{first roll above mean}) &= \\Pr\\left(X &gt; \\frac{X+Y}{2}\\right) \\\\\n&= \\Pr(2X &gt; X+Y) = \\Pr(X &gt; Y)\n\\end{align*}\n\\]"
  },
  {
    "objectID": "w02/slides.html#are-random-variables-all-powerful",
    "href": "w02/slides.html#are-random-variables-all-powerful",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Are Random Variables All-Powerful??",
    "text": "Are Random Variables All-Powerful??\n\nJust remember that probability \\(P(\\cdot)\\) is always probability of an event‚Äîrandom variables are just shorthand for quantifiable events.\nNot all events can be simplified via random variables!\n\n\\(\\text{catch a fish} \\mapsto P(\\text{trout}), P(\\text{bass}), \\ldots\\)\n\nWhat types of events can be quantified like this?\n\n(Hint: It has to do with a key topic in the early weeks of both DSAN 5000 and 5100‚Ä¶)\n\n\n\nThe answer is, broadly, any situation where you‚Äôre modeling things, like dice rolls, where mathematical operations like addition, multiplication, etc. make sense. So, if we‚Äôre modeling dice, it makes sense to say e.g.¬†‚Äúresult is 6‚Äù + ‚Äúresult is 3‚Äù = ‚Äútotal is 9‚Äù. More on the next page!"
  },
  {
    "objectID": "w02/slides.html#recall-types-of-variables",
    "href": "w02/slides.html#recall-types-of-variables",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Recall: Types of Variables",
    "text": "Recall: Types of Variables\n\nCategorical\n\nNo meaningful way to order values: \\(\\{\\text{trout}, \\text{bass}, \\ldots \\}\\)\n\nOrdinal\n\nCan place in order (bigger, smaller), though gaps aren‚Äôt meaningful: \\(\\{{\\color{orange}\\text{great}},{\\color{orange}\\text{greater}},{\\color{orange}\\text{greatest}}\\}\\)\n\\({\\color{orange}\\text{greater}} \\overset{?}{=} 2\\cdot {\\color{orange}\\text{great}} - 1\\)\n\nCardinal\n\nCan place in order, and gaps are meaningful \\(\\implies\\) can do ‚Äústandard‚Äù math with them! Example: \\(\\{{\\color{blue}1},{\\color{blue}2},\\ldots,{\\color{blue}10}\\}\\)\n\\({\\color{blue}7}\\overset{\\color{green}\\unicode{x2714}}{=} 2 \\cdot {\\color{blue}4} - 1\\)\nIf events have this structure (meaningful way to define multiplication, addition, subtraction), then we can analyze them as random variables"
  },
  {
    "objectID": "w02/slides.html#visualizing-discrete-rvs",
    "href": "w02/slides.html#visualizing-discrete-rvs",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Visualizing Discrete RVs",
    "text": "Visualizing Discrete RVs\n\nUltimate Probability Pro-Tip: When you hear ‚Äúdiscrete distribution‚Äù, think of a bar graph: \\(x\\)-axis = events, bar height = probability of events\nTwo coins example: \\(X\\) = RV representing number of heads obtained in two coin flips"
  },
  {
    "objectID": "w02/slides.html#preview-visualizing-continuous-rvs",
    "href": "w02/slides.html#preview-visualizing-continuous-rvs",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "(Preview:) Visualizing Continuous RVs",
    "text": "(Preview:) Visualizing Continuous RVs\n\nThis works even for continuous distributions, if you focus on the area under the curve instead of the height:"
  },
  {
    "objectID": "w02/slides.html#probability-theory-gives-us-distributions-for-rvs-not-numbers",
    "href": "w02/slides.html#probability-theory-gives-us-distributions-for-rvs-not-numbers",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Probability Theory Gives Us Distributions for RVs, not Numbers!",
    "text": "Probability Theory Gives Us Distributions for RVs, not Numbers!\n\nWe‚Äôre going beyond ‚Äúbase‚Äù probability theory if we want to summarize these distributions\nHowever, we can understand a lot about the full distribution by looking at some basic summary statistics. Most common way to summarize:\n\n\n\n\n\n\n\n\n\n\\(\\underbrace{\\text{point estimate}}_{\\text{mean/median}}\\)\n\\(\\pm\\)\n\\(\\underbrace{\\text{uncertainty}}_{\\text{variance/standard deviation}}\\)"
  },
  {
    "objectID": "w02/slides.html#example-game-reviews",
    "href": "w02/slides.html#example-game-reviews",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Example: Game Reviews",
    "text": "Example: Game Reviews\n\n(Data from Metacritic)"
  },
  {
    "objectID": "w02/slides.html#adding-a-single-line",
    "href": "w02/slides.html#adding-a-single-line",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Adding a Single Line",
    "text": "Adding a Single Line\n\n(Data from Metacritic)"
  },
  {
    "objectID": "w02/slides.html#or-a-single-ribbon",
    "href": "w02/slides.html#or-a-single-ribbon",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Or a Single Ribbon",
    "text": "Or a Single Ribbon"
  },
  {
    "objectID": "w02/slides.html#example-the-normal-distribution",
    "href": "w02/slides.html#example-the-normal-distribution",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Example: The Normal Distribution",
    "text": "Example: The Normal Distribution\n\n\n\n(The distribution you saw a few slides ago)\n\n\n\n\n\n\n\n\n‚ÄúRV \\(X\\) is normally distributed with mean \\({\\color{purple}\\mu}\\) and standard deviation \\({\\color{purple}\\sigma}\\)‚Äù\n\nTranslates to \\(X \\sim \\mathcal{N}(\\color{purple}{\\mu},\\color{purple}{\\sigma})\\)1\n\\(\\color{purple}{\\mu}\\) and \\(\\color{purple}{\\sigma}\\) are parameters2: the ‚Äúknobs‚Äù or ‚Äúsliders‚Äù which change the location/shape of the distribution\n\n\n\n\n\nThe parameters in this case give natural summaries of the data:\n\n\\({\\color{\\purple}\\mu}\\) = center (mean), \\({\\color{purple}\\sigma}\\) = [square root of] variance around center\n\nMean can usually be interpreted intuitively; for standard deviation, we usually use the 68-95-99.7 rule, which will make more sense relative to some real-world data‚Ä¶\n\nThroughout the course, this ‚Äúcalligraphic‚Äù font \\(\\mathcal{N}\\), \\(\\mathcal{D}\\), etc., will be used to denote distributionsThroughout the course, remember, purrple is for purrameters"
  },
  {
    "objectID": "w02/slides.html#real-data-and-the-68-95-99.7-rule",
    "href": "w02/slides.html#real-data-and-the-68-95-99.7-rule",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Real Data and the 68-95-99.7 Rule",
    "text": "Real Data and the 68-95-99.7 Rule\n\n\n\n\n\n\n\nFigure¬†2: Heights (cm) for 18K professional athletes\n\n\n\n\n\n\n\nFigure¬†3: The 68-95-99.7 Rule visualized (Wikimedia Commons)\n\n\n\n\nThe point estimate \\({\\color{purple}\\mu} = 186.48\\) is straightforward: the average height of the athletes is 186.48cm. Using the 68-95-99.7 Rule to interpret the SD, \\({\\color{purple}\\sigma} = 9.7\\), we get:\n\n\n\nAbout 68% of the heights fall between\n\n\n\n\n\n\n\n\n\n[\\({\\color{purple}\\mu} - 1\\cdot {\\color{purple}\\sigma}\\)\nand\n\\({\\color{purple}\\mu} + 1\\cdot {\\color{purple}\\sigma}\\)]\n\n\n[186.48 - 1 ¬∑ 9.7\nand\n186.48 + 1 ¬∑ 9.7]\n\n\n[176.78\nand\n196.18]\n\n\n\n\n\nAbout 95% of the heights fall between\n\n\n\n\n\n\n\n\n\n[\\({\\color{purple}\\mu} - 2 \\cdot {\\color{purple}\\sigma}\\)\nand\n\\({\\color{purple}\\mu} + 2 \\cdot {\\color{purple}\\sigma}\\)]\n\n\n[186.48 - 2 ¬∑ 9.7\nand\n186.48 + 2 ¬∑ 9.7]\n\n\n[167.08\nand\n205.88]"
  },
  {
    "objectID": "w02/slides.html#boxplots-comparing-multiple-distributions",
    "href": "w02/slides.html#boxplots-comparing-multiple-distributions",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Boxplots: Comparing Multiple Distributions",
    "text": "Boxplots: Comparing Multiple Distributions\n\n\n\n\n\n\nJhguch at en.wikipedia, CC BY-SA 2.5, via Wikimedia Commons\n\n\n\n\n\n\n\nProtonk, CC BY-SA 3.0, via Wikimedia Commons"
  },
  {
    "objectID": "w02/slides.html#another-option-joyplots",
    "href": "w02/slides.html#another-option-joyplots",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Another Option: Joyplots",
    "text": "Another Option: Joyplots\n\n\n\n\n\n\nFigure¬†4: (Iconic album cover)\n\n\n\n\n\n\nFigure¬†5: (Tooting my own horn)"
  },
  {
    "objectID": "w02/slides.html#multivariate-distributions-preview",
    "href": "w02/slides.html#multivariate-distributions-preview",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Multivariate Distributions: Preview",
    "text": "Multivariate Distributions: Preview\n\nThe bivariate normal distribution represents the distribution of two normally-distributed RVs \\(\\mathbf{X} = [\\begin{smallmatrix} X_1 & X_2\\end{smallmatrix}]\\), which may or may not be correlated:\n\n\n\\[\n\\mathbf{X} = \\begin{bmatrix}X_1 \\\\ X_2\\end{bmatrix}, \\; \\boldsymbol{\\mu} =\n%\\begin{bmatrix}\\mu_1 \\\\ \\mu_2\\end{bmatrix}\n\\begin{bmatrix}\\smash{\\overbrace{\\mu_1}^{\\mathbb{E}[X_1]}} \\\\ \\smash{\\underbrace{\\mu_2}_{\\mathbb{E}[X_2]}}\\end{bmatrix}\n, \\; \\mathbf{\\Sigma} = \\begin{bmatrix}\\smash{\\overbrace{\\sigma_1^2}^{\\text{Var}[X_1]}} & \\smash{\\overbrace{\\rho\\sigma_1\\sigma_2}^{\\text{Cov}[X_1,X_2]}} \\\\ \\smash{\\underbrace{\\rho\\sigma_2\\sigma_1}_{\\text{Cov}[X_2,X_1]}} & \\smash{\\underbrace{\\sigma_2^2}_{\\text{Var}[X_2]}}\\end{bmatrix}\n% \\begin{bmatrix}\\sigma_1^2 & \\rho\\sigma_1\\sigma_2 \\\\ \\rho\\sigma_2\\sigma_1 & \\sigma_2^2 \\end{bmatrix}\n% = \\begin{bmatrix}\\text{Var}[X_1] & \\text{Cov}[X_1,X_2] \\\\ \\text{Cov}[X_2,X_1] & \\text{Var}[X_2] \\end{bmatrix}\n\\]\n\n\nBy squishing all this information intro matrices, we can specify the parameters of multivariate-normally-distributed vectors of RVs similarly to how we specify single-dimensional normally-distributed RVs:\n\n\n\\[\n\\begin{align*}\n\\overbrace{X}^{\\mathclap{\\text{scalar}}} &\\sim \\mathcal{N}\\phantom{_k}(\\overbrace{\\mu}^{\\text{scalar}}, \\overbrace{\\sigma}^{\\text{scalar}}) \\tag{Univariate} \\\\\n\\underbrace{\\mathbf{X}}_{\\text{vector}} &\\sim \\boldsymbol{\\mathcal{N}}_k(\\smash{\\underbrace{\\boldsymbol{\\mu}}_{\\text{vector}}}, \\underbrace{\\mathbf{\\Sigma}}_{\\text{matrix}}) \\tag{Multivariate}\n\\end{align*}\n\\]\n\n\nNote: In the future I‚Äôll use the notation \\(\\mathbf{X}_{[a \\times b]}\\) to denote the dimensions of the vectors/matrices, like \\(\\mathbf{X}_{[k \\times 1]} \\sim \\boldsymbol{\\mathcal{N}}_k(\\boldsymbol{\\mu}_{[k \\times 1]}, \\mathbf{\\Sigma}_{[k \\times k]})\\)"
  },
  {
    "objectID": "w02/slides.html#visualizing-3d-distributions-projection",
    "href": "w02/slides.html#visualizing-3d-distributions-projection",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Visualizing 3D Distributions: Projection",
    "text": "Visualizing 3D Distributions: Projection\n\nSince most of our intuitions about plots come from 2D plots, it is extremely useful to be able to take a 3D plot like this and imagine ‚Äúprojecting‚Äù it down into different 2D plots:\n\n\n(Adapted via LaTeX from StackExchange discussion)"
  },
  {
    "objectID": "w02/slides.html#visualizing-3d-distributions-contours",
    "href": "w02/slides.html#visualizing-3d-distributions-contours",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Visualizing 3D Distributions: Contours",
    "text": "Visualizing 3D Distributions: Contours\n\nFrom Prof.¬†Hickman‚Äôs slides!"
  },
  {
    "objectID": "w02/slides.html#visualizing-3d-distributions-contours-1",
    "href": "w02/slides.html#visualizing-3d-distributions-contours-1",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Visualizing 3D Distributions: Contours",
    "text": "Visualizing 3D Distributions: Contours\n\n\n\n\n\nAlso from Prof.¬†Hickman‚Äôs slides!"
  },
  {
    "objectID": "w02/index.html",
    "href": "w02/index.html",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "",
    "text": "Open slides in new window ‚Üí"
  },
  {
    "objectID": "w02/index.html#prof.-jeff-introduction",
    "href": "w02/index.html#prof.-jeff-introduction",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Prof.¬†Jeff Introduction!",
    "text": "Prof.¬†Jeff Introduction!\n\nBorn and raised in NW DC ‚Üí high school in Rockville, MD\nUniversity of Maryland: Computer Science, Math, Economics (2008-2012)"
  },
  {
    "objectID": "w02/index.html#grad-school",
    "href": "w02/index.html#grad-school",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Grad School",
    "text": "Grad School\n\nStudied abroad in Beijing (Peking University/ÂåóÂ§ß) ‚Üí internship with Huawei in Hong Kong (HKUST)\n\n\n\n\nStanford for MS in Computer Science (2012-2014)\nResearch Economist at UC Berkeley (2014-2015)\n\n\n\n\n\n\nColumbia (NYC) for PhD[+Postdoc] in Political Science (2015-2023)"
  },
  {
    "objectID": "w02/index.html#dissertation-political-science-history",
    "href": "w02/index.html#dissertation-political-science-history",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Dissertation (Political Science + History)",
    "text": "Dissertation (Political Science + History)\n‚ÄúOur Word is Our Weapon‚Äù: Text-Analyzing Wars of Ideas from the French Revolution to the First Intifada"
  },
  {
    "objectID": "w02/index.html#research-labor-economics",
    "href": "w02/index.html#research-labor-economics",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Research (Labor Economics)",
    "text": "Research (Labor Economics)\n\n\n\n‚ÄúMonopsony in Online Labor Markets‚Äù: Machine Learning to enhance causal estimates of the effect of job description language on uptake rate\n\n\n\n‚ÄúFreedom as Non-Domination in the Labor Market‚Äù: Game-theoretic models of workers‚Äô rights (monopsony vs.¬†labor discipline)\n\n\n\n\n\n‚ÄúUnsupervised Extraction of Workplace Rights and Duties from Collective Bargaining Agreements‚Äù: Linguistic (dependency) parses of contracts ‚Üí time series of worker vs.¬†employer rights and responsibilities over time"
  },
  {
    "objectID": "w02/index.html#deterministic-processes",
    "href": "w02/index.html#deterministic-processes",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Deterministic Processes",
    "text": "Deterministic Processes\n\nGiven a set of inputs, we can compute the outcome exactly\nExample: Given the radius of a circle, we can compute its area without any uncertainty. \\(r \\mapsto \\pi r^2\\)\n(The fact that we can compute the outcome doesn‚Äôt mean that it‚Äôs easy to do so! See, e.g., the double pendulum)\n\n\n\n\nImage credit: Tenor.com\n\n\n\nThe pendulum example points to the fact that the notion of a chaotic system, one which is ‚Äúsensitive to initial conditions‚Äù, is different from that of a stochastic system."
  },
  {
    "objectID": "w02/index.html#holy-grail-deterministic-model-newtonian-physics",
    "href": "w02/index.html#holy-grail-deterministic-model-newtonian-physics",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "‚ÄúHoly Grail‚Äù Deterministic Model: Newtonian Physics",
    "text": "‚ÄúHoly Grail‚Äù Deterministic Model: Newtonian Physics\n\n\n\n\n\n\n\n\n\n\ngrid\n\n \n\ncluster_01\n\n ‚ÄúNature‚Äù  \n\ncluster_02\n\n ‚ÄúScience‚Äù   \n\nObs\n\n Thing(s) we can see   \n\nUnd\n\n Underlying processes   \n\nUnd-&gt;Obs\n\n    \n\nModel\n\n Model   \n\nUnd-&gt;Model\n\n    \n\nModel-&gt;Obs\n\n   \n\n\n\n\n\n\n\n\n\n\n\n\n\ngrid\n\n \n\ncluster_03\n\n Isaac Newton  \n\ncluster_04\n\n  Woolsthorpe Manor     \n\nTree\n\n  Falling Apple     \n\nPhysics\n\n  Particle Interactions     \n\nPhysics-&gt;Tree\n\n    \n\nNewton\n\n Newton‚Äôs Laws   \n\nPhysics-&gt;Newton\n\n    \n\nNewton-&gt;Tree\n\n   \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\n\\leadsto F_g = G\\frac{m_1m_2}{r^2}\n\\]\nFigure¬†1: Newton‚Äôs Law of Universal Gravitation‚Üê Dr.¬†Zirkel follows Newton‚Äôs famous steps. Coloured wood engraving. Wellcome Collection (Public Domain)"
  },
  {
    "objectID": "w02/index.html#but-what-happens-when",
    "href": "w02/index.html#but-what-happens-when",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "But What Happens When‚Ä¶",
    "text": "But What Happens When‚Ä¶\n\\[\n\\text{Outcome}\\left(\\text{Dice Roll}\\right) = \\; ?\\frac{?_1?_2}{?^2}\n\\]\n\n\n\nPre-Enlightenment\n\n\n\n\nHans Sebald Beham, Fortuna (1541), CC BY 4.0, via Wikimedia Commons\n\n\n\n\nPost-Enlightenment\n\n\n\n\nBlaise Pascal, Trait√© du triangle arithm√©tique (1665). Public Domain, via Internet Archive"
  },
  {
    "objectID": "w02/index.html#random-processes",
    "href": "w02/index.html#random-processes",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Random Processes",
    "text": "Random Processes\n\n\n\nCan‚Äôt compute the outcome exactly, but can still say something about potential outcomes!\nExample: randomly chosen radius \\(r \\in [0,1]\\), what can we say about \\(A = \\pi r^2\\)?\n\nUnif: \\([0,\\pi]\\) equally likely\nExp: closer to \\(0\\) more likely\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(ggplot2)\nN &lt;- 1000\nradii &lt;- rexp(N, 4)\ntitle &lt;- paste0(N, \" Exponentially-Distributed Radii\")\nplot_circ_with_distr(N, radii, title, alpha=0.15)\n\nWarning: Removed 1456 rows containing missing values (`geom_path()`)."
  },
  {
    "objectID": "w02/index.html#data-ground-truth-noise",
    "href": "w02/index.html#data-ground-truth-noise",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Data = Ground Truth + Noise",
    "text": "Data = Ground Truth + Noise\n\nDepressing but true origin of statistics (as opposed to probability): the Plague üò∑\n\n\n\n\n\n\n\nGround Truth: The Great Plague (Lord Have Mercy on London, Unknown Artist, circa 1665, via Wikimedia Commons)\n\n\n\n\n\n\n\nNoisy Data (Recorded amidst chaos): London Bill of Mortality, 1665 (Public Domain, Wellcome Collection)"
  },
  {
    "objectID": "w02/index.html#random-variables",
    "href": "w02/index.html#random-variables",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Random Variables",
    "text": "Random Variables\n\nIn algebra, to solve problems we work with variables\nIn probability theory, to solve problems we work with random variables\nRecall the difference between random and deterministic: \\(A = \\pi r^2\\) tells us that, given a value of \\(r\\), we can solve for the unique value of \\(A\\)\nIn probability theory, however, there is no one ‚Äútrue‚Äù value of a random variable \\(X\\).\nLet \\(X = f(N)\\) mean that \\(X\\) is the result of a rolled die, where the die has \\(N\\) sides.\nPlugging in \\(N = 6\\) (standard 6-sided die) still doesn‚Äôt mean we know ‚Äúthe‚Äù value of \\(X\\). However, (if the die is fair) we do know\n\n\\[\n\\Pr(X = 1) = \\Pr(X = 2) = \\cdots = \\Pr(X = 6) = \\frac{1}{6}\n\\]"
  },
  {
    "objectID": "w02/index.html#discrete-vs.-continuous",
    "href": "w02/index.html#discrete-vs.-continuous",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Discrete vs.¬†Continuous",
    "text": "Discrete vs.¬†Continuous\n\nMany complicated definitions, often misleading or unintuitive!\nHow I want you to remember: How many possible values between two known values?\nDiscrete: e.g., number of siblings\n\nI have 2 siblings, you have 3 siblings‚Ä¶ How many values (sibling counts) in between?\n\nContinuous: e.g., temperature\n\nIt is 27.0¬∞ C in my room, 28.0¬∞ C in your room‚Ä¶ How many values (temperatures) in between?\n\nSo, if \\(X\\) is the result of a rolled die, is \\(X\\) discrete or continuous? How many values can be rolled between 3 and 4?"
  },
  {
    "objectID": "w02/index.html#thinking-about-independence",
    "href": "w02/index.html#thinking-about-independence",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Thinking About Independence",
    "text": "Thinking About Independence\n\nWe‚Äôll define it formally later; for now, this is our working definition:\n\n\n\n\n\n\n\nWorking Definition: Independence\n\n\n\nTwo random variables \\(X\\) and \\(Y\\) are independent if learning information about \\(X\\) does not give you information about the value of \\(Y\\), or vice-versa."
  },
  {
    "objectID": "w02/index.html#na√Øve-definition-of-probability",
    "href": "w02/index.html#na√Øve-definition-of-probability",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Na√Øve Definition of Probability",
    "text": "Na√Øve Definition of Probability\n\nSample Space: The set of all possible outcomes of an experiment\nEvent: A subset of the sample space\n\n\n\n\n\n\n\nNa√Øve Definition of Probability\n\n\n\nGiven a sample space \\(S\\), and an event \\(E \\subset S\\),\n\\[\n\\Pr(\\underbrace{E}_{\\text{event}}) = \\frac{\\text{\\# Favorable Outcomes}}{\\text{\\# Possible Outcomes}} = \\frac{|E|}{|S|}\n\\]"
  },
  {
    "objectID": "w02/index.html#example-flipping-two-coins",
    "href": "w02/index.html#example-flipping-two-coins",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Example: Flipping Two Coins",
    "text": "Example: Flipping Two Coins\n\n\n\n\n\n\nNa√Øve Definition of Probability\n\n\n\nGiven a sample space \\(S\\), and an event \\(E \\subset S\\),\n\\[\n\\Pr(\\underbrace{E}_{\\text{event}}) = \\frac{\\text{\\# Favorable Outcomes}}{\\text{\\# Possible Outcomes}} = \\frac{|E|}{|S|}\n\\]\n\n\n\nFlipping two coins:\n\nSample space \\(S = \\{TT, TH, HT, HH\\}\\)\nEvent \\(E_1\\): Result of first flip is \\(H\\), result of second flip is \\(T\\) \\(\\implies\\) \\(E_1 = \\{HT\\}\\).\nEvent \\(E_2\\): At least one \\(H\\) \\(\\implies\\) \\(E_2 = \\{TH, HT, HH\\}\\).\n\n\n\\[\n\\begin{align*}\n\\Pr(E_1) &= \\frac{|\\{HT\\}|}{|S|} = \\frac{|\\{HT\\}|}{|\\{TT, TH, HT, HH\\}|} = \\frac{1}{4} \\\\\n\\Pr(E_2) &= \\frac{|\\{TH, HT, HH\\}|}{|S|} = \\frac{|\\{TH, HT, HH\\}|}{|\\{TT, TH, HT, HH\\}|} = \\frac{3}{4}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "w02/index.html#events-neq-outcomes",
    "href": "w02/index.html#events-neq-outcomes",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Events \\(\\neq\\) Outcomes!",
    "text": "Events \\(\\neq\\) Outcomes!\n\nOutcomes are things, events are sets of things\nSubtle but extremely important distinction!\nIn the coin flip example:\n\nThe event \\(E_1 = \\{HT\\}\\) can be confused with the outcome \\(HT\\).\nSo, try to remember instead the event \\(E_2 = \\{TH, HT, HH\\}\\): it is more clear, in this case, how this event does not correspond to any individual outcome"
  },
  {
    "objectID": "w02/index.html#back-to-the-na√Øve-definition",
    "href": "w02/index.html#back-to-the-na√Øve-definition",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Back to the Na√Øve Definition",
    "text": "Back to the Na√Øve Definition\n\n\n\n\n\n\nNa√Øve Definition of Probability\n\n\n\nGiven a sample space \\(S\\), and an event \\(E \\subset S\\),\n\\[\n\\Pr(\\underbrace{E}_{\\text{event}}) = \\frac{\\text{\\# Favorable Outcomes}}{\\text{\\# Possible Outcomes}} = \\frac{|E|}{|S|}\n\\]\n\n\n\nThe na√Øve definition tells us that probabilities are just ratios of counts:\n\nCount the number of ways the event \\(E\\) can happen, count the total number of things that can happen, and divide!\n\nThis is why we begin studying probability by studying combinatorics: the mathematics of counting"
  },
  {
    "objectID": "w02/index.html#combinatorics-ice-cream-possibilities",
    "href": "w02/index.html#combinatorics-ice-cream-possibilities",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Combinatorics: Ice Cream Possibilities",
    "text": "Combinatorics: Ice Cream Possibilities\n\n\n\n\n\n\n\nThe \\(6 = 2 \\cdot 3\\) possible cone+flavor combinations which can result from choosing a flavor first and a cone type second.\n\n\n\n\n\n\n\nThe \\(6 = 3 \\cdot 2\\) possible cone+flavor combinations which can result from choosing a flavor first and a cone type second."
  },
  {
    "objectID": "w02/index.html#grouping-vs.-ordering",
    "href": "w02/index.html#grouping-vs.-ordering",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Grouping vs.¬†Ordering",
    "text": "Grouping vs.¬†Ordering\n\nIn standard statistics/combinatorics introductions you‚Äôll learn different counting formulas for when order matters vs.¬†when order doesn‚Äôt matter\nThis is not a mathematical distinction so much as a pragmatic distinction: what are you trying to accomplish by counting?\nProblems with extremely similar descriptions can differ in small detail, so that the units you need to distinguish between in one version differ from the units you need to distinguish between in the other."
  },
  {
    "objectID": "w02/index.html#does-order-matter",
    "href": "w02/index.html#does-order-matter",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Does Order Matter?",
    "text": "Does Order Matter?\n\n\n\n\n\n\nExample: Student Government vs.¬†Student Sports\n\n\n\n\nConsider a school where students can either try out for the swim team or run for a position in the student government\nThe swim team has 4 slots, but slots aren‚Äôt differentiated: you‚Äôre either on the team (one of the 4 chosen students) or not\nThe student government also has 4 slots, but there is a difference between the slots: first slot is President, second is Vice President, third is Secretary, and fourth is Treasurer.\n\n\n\n\nSimple case (for intuition): the school only has 4 students. In this case, how many ways are there to form the swim team? What about the student government?\n\nSwim team: \\(1\\) way. You have only one choice, to let all 4 students onto the team\nStudent government: \\(4 \\cdot 3 \\cdot 2 \\cdot 1 = 24\\) ways. You have to let all 4 students in, but you have a choice of who is President, Vice President, Secretary, and Treasurer\n\nHow did we get \\(4 \\cdot 3 \\cdot 2 \\cdot 1\\)? (Think about the ice cream example‚Ä¶)\n\nStart by choosing the President: 4 choices\nNow choose the Vice President: only 3 students left to choose from\nNow choose the Secretary: only 2 students left to choose from\nNow choose the Treasurer: only 1 student left to choose from"
  },
  {
    "objectID": "w02/index.html#permutations-vs.-combinations",
    "href": "w02/index.html#permutations-vs.-combinations",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Permutations vs.¬†Combinations",
    "text": "Permutations vs.¬†Combinations\n\nPermutations: How many ways can I choose groups of size \\(k\\) out of \\(n\\) total objects, where order within groups matters: \\(P_{n,k}\\) (sometimes written \\(_nP_k\\)).\n\nIn this case, we want to count \\((a,b)\\) and \\((b,a)\\) as two separate groups\n\nCombinations: How many ways can I choose groups of size \\(k\\) out of \\(n\\) total objects, where order in the groups doesn‚Äôt matter: \\(C_{n,k}\\) (sometimes written \\(_nC_k,\\binom{n}{k}\\)).\n\nIn this case, we don‚Äôt want to count \\((a, b)\\) and \\((b, a)\\) as two separate groups‚Ä¶\n\n\n\\[\n\\begin{align*}\nP_{n,k} = \\frac{n!}{(n-k)!}, \\; C_{n,k} = \\frac{n!}{k!(n-k)!}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "w02/index.html#no-need-to-memorize",
    "href": "w02/index.html#no-need-to-memorize",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "No Need to Memorize!",
    "text": "No Need to Memorize!\n\n\nKey point: you don‚Äôt have to remember these as two separate formulas!\nThe number of combinations is based on the number of permutations, but corrected for double counting: e.g., corrected for the fact that \\((a,b) \\neq (b,a)\\) when counting permutations but \\((a,b) = (b,a)\\) when counting combinations.\n\n\\[\nC_{n,k} = \\frac{P_{n,k}}{k!} \\genfrac{}{}{0pt}{}{\\leftarrow \\text{Permutations}}{\\leftarrow \\text{Duplicate groups}}\n\\]\nWhere does \\(k!\\) come from? (How many different orderings can we make of the same group?)\n\n\\(k = 2\\): \\((\\underbrace{\\boxed{\\phantom{a}}}_{\\text{2 choices}},\\underbrace{\\boxed{\\phantom{a}}}_{\\text{1 remaining choice}}) \\implies 2\\)\n\\(k = 3\\): \\((\\underbrace{\\boxed{\\phantom{a}}}_{\\text{3 choices}},\\underbrace{\\boxed{\\phantom{a}}}_{\\text{2 remaining choices}}, \\underbrace{\\boxed{\\phantom{a}}}_{\\text{1 remaining choice}}) \\implies 6\\)\n\\(k = 4\\): \\((\\underbrace{\\boxed{\\phantom{a}}}_{\\text{4 choices}}, \\underbrace{\\boxed{\\phantom{a}}}_{\\text{3 remaining choices}}, \\underbrace{\\boxed{\\phantom{a}}}_{\\text{2 remaining choices}}, \\underbrace{\\boxed{\\phantom{a}}}_{\\text{1 remaining choice}}) \\implies 24\\)"
  },
  {
    "objectID": "w02/index.html#with-or-without-replacement",
    "href": "w02/index.html#with-or-without-replacement",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "With or Without Replacement?",
    "text": "With or Without Replacement?\n\nBoils down to: can the same object be included in my sample more than once?\n\n\n\n\nWithout Replacement\nWith Replacement\n\n\n\n\nMost statistical problems: ‚ÄúCheck off‚Äù objects as you collect data about them, so that each observation in your data is unique\nVery special (but very important!) set of statistical problems: allow objects to appear in your sample multiple times, to ‚Äúsqueeze‚Äù more information out of the sample (called Bootstrapping‚Äîmuch more on this later in the course!)"
  },
  {
    "objectID": "w02/index.html#how-many-possible-samples",
    "href": "w02/index.html#how-many-possible-samples",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "How Many Possible Samples?",
    "text": "How Many Possible Samples?\n\n\n\n\n\n\nExample: How Many Possible Samples?\n\n\n\nFrom a population of \\(N = 3\\), how many ways can we take samples of size \\(k = 2\\)?\n\n\n\n\n\n\n\nWithout Replacement\nWith Replacement\n\n\n\n\n\\(3 \\cdot 2 = 6\\) ways (3 objects to choose from for first element of sample, 2 remaining objects to choose from for second element of sample)\n\\(3\\cdot 3 = 3^2 = 9\\) ways (3 objects to choose from for first element of sample, still 3 objects to choose from for second element of sample)\n\n\n\n\n\n\n\n\n\n\n\nResult: How Many Possible Samples\n\n\n\nFrom a population of size \\(N\\), how many ways can we take samples of size \\(k\\)? (Try to extrapolate from above examples before looking at answer!)\n\n\n\n\n\n\n\nWithout Replacement\nWith Replacement\n\n\n\n\n\\(\\displaystyle \\underbrace{N \\cdot (N-1) \\cdot \\cdots \\cdot (N - k + 1)}_{k\\text{ times}} = \\frac{N!}{(N - k )!}\\)(This formula should look somewhat familiar‚Ä¶)\n\\(\\displaystyle \\underbrace{N \\cdot N \\cdot \\cdots \\cdot N}_{k\\text{ times}} = N^k\\)"
  },
  {
    "objectID": "w02/index.html#logic-sets-and-probability",
    "href": "w02/index.html#logic-sets-and-probability",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Logic, Sets, and Probability",
    "text": "Logic, Sets, and Probability\n\nThere is a deep connection1 between the objects and operations of logic, set theory, and probability:\n\n\n\n\n\n\n\n\n\n\n\nLogic\nSet Theory\nProbability Theory\n\n\n\n\nObjects\nPredicates\\(p, q \\in \\{T, F\\}\\)\nSets\\(S = \\{a, b, \\ldots\\}\\)\nEvents\\(E = \\{TH, HT, HH\\}\\)\n\n\nConjunction\nAnd (\\(\\wedge\\))\\(p \\wedge q\\)\nIntersection (\\(\\cap\\))\\(A \\cap B\\)\nMultiplication (\\(\\times\\)):\\(\\Pr(E_1 \\cap E_2) = \\Pr(E_1)\\times \\Pr(E_2)\\)\n\n\nDisjunction\nOr (\\(\\vee\\))\\(p \\vee q\\)\nUnion (\\(\\cup\\))\\(A \\cup B\\)\nAddition (\\(+\\)): \\(\\Pr(E_1 \\cup E_2) =\\)\\(\\Pr(E_1) + \\Pr(E_2) - \\Pr(E_1 \\wedge E_2)\\)\n\n\nNegation\nNot (\\(\\neg\\))\\(\\neg p\\)\nComplement (\\(^c\\))\\(S^c\\)\nSubtract from 1\\(\\Pr(A^c) = 1 - \\Pr(A)\\)"
  },
  {
    "objectID": "w02/index.html#example-flipping-two-coins-1",
    "href": "w02/index.html#example-flipping-two-coins-1",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Example: Flipping Two Coins",
    "text": "Example: Flipping Two Coins\n\nLogic: We can define 4 predicates:\n\n\\(p_1\\) = ‚ÄúFirst result is \\(H\\)‚Äù, \\(q_1\\) = ‚ÄúFirst result is \\(T\\)‚Äù\n\\(p_2\\) = ‚ÄúSecond result is \\(H\\)‚Äù, \\(q_2\\) = ‚ÄúSecond result is \\(T\\)‚Äù\n\nLogical formulas:\n\n\\(f_1 = p_1 \\wedge q_2\\): ‚ÄúFirst result is \\(H\\) and second result is \\(T\\)‚Äù\n\\(f_2 = p_1 \\vee q_2\\): ‚ÄúFirst result is \\(H\\) or second result is \\(T\\)‚Äù\n\\(f_3 = \\neg p_1\\): ‚ÄúFirst result is not \\(H\\)‚Äù\n\nThe issue?: We don‚Äôt know, until after the coins have been flipped, whether these are true or false!\nBut, we should still be able to say something about their likelihood, for example, whether \\(f_1\\) or \\(f_2\\) is more likely to happen‚Ä¶ Enter probability theory!"
  },
  {
    "objectID": "w02/index.html#logic-rightarrow-probability",
    "href": "w02/index.html#logic-rightarrow-probability",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Logic \\(\\rightarrow\\) Probability",
    "text": "Logic \\(\\rightarrow\\) Probability\n\nProbability theory lets us reason about the uncertainty surrounding logical predicates like \\(p\\) and \\(q\\), by:\n\nencoding them as sets of possibilities \\(P\\) and \\(Q\\), and\nrepresenting the uncertainty around any given possibility using a probability measure \\(\\Pr: S \\mapsto [0,1]\\),\n\nthus allowing us to reason about\n\nthe likelihood of these set-encoded predicates on their own: \\(\\Pr(P)\\) and \\(\\Pr(Q)\\), but also\ntheir logical connections: \\(\\Pr(p \\wedge q) = \\Pr(P \\cap Q)\\), \\(\\Pr(\\neg p) = \\Pr(P^c)\\), and so on."
  },
  {
    "objectID": "w02/index.html#flipping-two-coins-logic-rightarrow-probability",
    "href": "w02/index.html#flipping-two-coins-logic-rightarrow-probability",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Flipping Two Coins: Logic \\(\\rightarrow\\) Probability",
    "text": "Flipping Two Coins: Logic \\(\\rightarrow\\) Probability\n\nReturning to the two coins example: we can look at the predicates and see that they exhaust all possibilities, so that we can define a sample space \\(S = \\{TT, TH, HT, HH\\}\\) of all possible outcomes of our coin-flipping experiment, noting that \\(|S| = 4\\), so there are 4 possible outcomes.\nThen we can associate each predicate with an event, a subset of the sample space, and use our na√Øve definition to compute the probability of these events:\n\n\n\n\n\n\n\n\n\nPredicate\nEvent\nProbability\n\n\n\n\n\\(p_1\\) = ‚ÄúFirst result is \\(H\\)‚Äù\n\\(P_1 = \\{HT, HH\\}\\)\n\\(\\Pr(P_1) = \\frac{|P_1|}{|S|} = \\frac{2}{4} = \\frac{1}{2}\\)\n\n\n\\(q_1\\) = ‚ÄúFirst result is \\(T\\)‚Äù\n\\(Q_1 = \\{TT, TH\\}\\)\n\\(\\Pr(Q_1) = \\frac{|Q_1|}{|S|} = \\frac{2}{4} = \\frac{1}{2}\\)\n\n\n\\(p_2\\) = ‚ÄúSecond result is \\(H\\)‚Äù\n\\(P_2 = \\{TH, HH\\}\\)\n\\(\\Pr(P_2) = \\frac{|P_2|}{|S|} = \\frac{2}{4} = \\frac{1}{2}\\)\n\n\n\\(q_2\\) = ‚ÄúSecond result is \\(T\\)‚Äù\n\\(Q_2 = \\{TT, HT\\}\\)\n\\(\\Pr(Q_2) = \\frac{|Q_2|}{|S|} = \\frac{2}{4} = \\frac{1}{2}\\)"
  },
  {
    "objectID": "w02/index.html#moving-from-predicates-to-formulas",
    "href": "w02/index.html#moving-from-predicates-to-formulas",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Moving from Predicates to Formulas",
    "text": "Moving from Predicates to Formulas\n\nNotice that, in the four rows of the previous table, we were only computing the probabilities of ‚Äúsimple‚Äù events: events corresponding to a single predicate\nBut we promised that probability theory lets us compute probabilities for logical formulas as well! ‚Ä¶The magic of encoding events as sets becomes clear:\n\n\n\n\n\n\n\n\n\nFormula\nEvent\nProbability\n\n\n\n\n\\(f_1 = p_1 \\wedge q_2\\)\n\\[ \\begin{align*} F_1 &= P_1 \\cap Q_2 \\\\ &= \\{HT, HH\\} \\cap \\{TT, HT\\} \\\\ &= \\{HT\\} \\end{align*} \\]\n\\[\\begin{align*} \\Pr(F_1) &= \\Pr(\\{HT\\}) \\\\ &= \\frac{|\\{HT\\}|}{|S|} = \\frac{1}{4} \\end{align*}\\]\n\n\n\\(f_2 = p_1 \\vee q_2\\)\n\\[\\begin{align*} F_2 &= P_1 \\cup Q_2 \\\\ &= \\{HT, HH\\} \\cup \\{TT, HT\\} \\\\ &= \\{TT, HT, HH\\} \\end{align*}\\]\n\\[\\begin{align*} \\Pr(F_2) &= \\Pr(\\{TT, HT, HH\\}) \\\\ &= \\frac{|\\{TT, HT, HH\\}|}{|S|} = \\frac{3}{4} \\end{align*}\\]\n\n\n\\(f_3 = \\neg p_1\\)\n\\[\\begin{align*} F_3 &= P_1^c \\\\ &= \\{HT, HH\\}^c \\\\ &= \\{TT, TH\\}\\end{align*}\\]\n\\[\\begin{align*} \\Pr(F_3) &= \\Pr(\\{TT, TH\\}) \\\\ &= \\frac{|\\{TT, TH\\}|}{|S|} = \\frac{2}{4} = \\frac{1}{2} \\end{align*}\\]"
  },
  {
    "objectID": "w02/index.html#using-rules-of-probability",
    "href": "w02/index.html#using-rules-of-probability",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Using ‚ÄúRules‚Äù of Probability",
    "text": "Using ‚ÄúRules‚Äù of Probability\n\nHopefully, though, you found all this churning through set theory to be a bit tedious‚Ä¶\nThis is where rules of probability come from! They simplify set-theoretic computations into simple multiplications, additions, and subtractions:\n\n\\(\\Pr(A \\cap B) = \\Pr(A) \\times \\Pr(B)\\)\n\\(\\Pr(A \\cup B) = \\Pr(A) + \\Pr(B) - \\Pr(A \\cap B)\\)\n\\(\\Pr(A^c) = 1 - \\Pr(A)\\)\n\nSince we know probabilities of the ‚Äúsimple‚Äù events \\(P_1\\), \\(Q_1\\), \\(P_2\\), \\(Q_2\\), we don‚Äôt need to ‚Äúlook inside them‚Äù! Just take the probabilities and multiply/add/subtract as needed:\n\n\n\n\n\n\n\n\n\nFormula\nEvent\nProbability\n\n\n\n\n\\(f_1 = p_1 \\wedge q_2\\)\n\\(F_1 = P_1 \\cap Q_2\\)\n\\(\\Pr(F_1) = \\Pr(P_1) \\times \\Pr(Q_2) = \\frac{1}{2}\\times \\frac{1}{2} = \\frac{1}{4}\\)\n\n\n\\(f_2 = p_1 \\vee q_2\\)\n\\(F_2 = P_1 \\cup Q_2\\)\n\\[\\textstyle{\\begin{align*} \\textstyle \\Pr(F_2) &= \\Pr(P_1) + \\Pr(Q_2) - \\Pr(P_1 \\cap Q_2) \\\\ \\textstyle &= \\frac{1}{2} + \\frac{1}{2} - \\frac{1}{4} = \\frac{3}{4} \\end{align*}}\\]\n\n\n\\(f_3 = \\neg p_1\\)\n\\(F_3 = P_1^c\\)\n\\(\\Pr(F_3) = 1 - \\Pr(P_1) = 1 - \\frac{1}{2} = \\frac{1}{2}\\)"
  },
  {
    "objectID": "w02/index.html#importing-results-from-logic",
    "href": "w02/index.html#importing-results-from-logic",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "‚ÄúImporting‚Äù Results from Logic",
    "text": "‚ÄúImporting‚Äù Results from Logic\n\nThis deep connection between the three fields means that, if we have some useful theorem or formula from one field, we can immediately put it to use in another!\nFor example: DeMorgan‚Äôs Laws were developed in logic (DeMorgan was a 19th-century logician), and basically just tell us how to distribute logic operators:\n\n\\[\n\\begin{align*}\n\\underbrace{\\neg(p \\wedge q)}_{\\text{``}p\\text{ and }q\\text{'' is not true}} &\\iff \\underbrace{\\neg p \\vee \\neg q}_{p\\text{ is not true or }q\\text{ is not true}} \\\\\n\\underbrace{\\neg(p \\vee q)}_{\\text{``}p\\text{ or }q\\text{'' is not true}} &\\iff \\underbrace{\\neg p \\wedge \\neg q}_{p\\text{ is not true and }q\\text{ is not true}}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "w02/index.html#converting-to-probability-theory",
    "href": "w02/index.html#converting-to-probability-theory",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Converting to Probability Theory",
    "text": "Converting to Probability Theory\n\nSo, using the same principles we used in our coin flipping examples, we can consider events \\(P\\) and \\(Q\\), and get the following ‚Äútranslation‚Äù of DeMorgan‚Äôs Laws:\n\n\n\n\n\n\n\n\n\nLogic\nSet Theory\nProbability Theory\n\n\n\n\n\\(\\neg(p \\wedge q) = \\neg p \\vee \\neg q\\)\n\\((P \\cap Q)^c = P^c \\cup Q^c\\)\n\\(\\Pr((P \\cap Q)^c) = \\Pr(P^c \\cup Q^c)\\)\n\n\n\\(\\neg(p \\vee q) = \\neg p \\wedge \\neg q\\)\n\\((P \\cup Q)^c = P^c \\cap Q^c\\)\n\\(\\Pr((P \\cup Q)^c) = \\Pr(P^c \\cap Q^c)\\)\n\n\n\n\nNote that, since these are isomorphic to one another, we could have derived DeMorgan‚Äôs Laws from within probability theory, rather than the other way around:\n\n\\[\n\\begin{align*}\n\\Pr((P \\cap Q)^c) &= 1 - \\Pr(P \\cap Q) = 1 - \\Pr(P)\\Pr(Q) \\\\\n&= 1 - (1-\\Pr(P^c))(1 - \\Pr(Q^c)) \\\\\n&= 1 - [1 - \\Pr(P^c) - \\Pr(Q^c) + \\Pr(P^c)\\Pr(Q^c)] \\\\\n&= \\Pr(P^c) + \\Pr(Q^c) - \\Pr(P^c)\\Pr(Q^c) \\\\\n&= \\Pr(P^c) + \\Pr(Q^c) - \\Pr(P^c \\cap Q^c) \\\\\n&= \\Pr(P^c \\cup Q^c) \\; ‚úÖ\n\\end{align*}\n\\]"
  },
  {
    "objectID": "w02/index.html#random-variables-1",
    "href": "w02/index.html#random-variables-1",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Random Variables",
    "text": "Random Variables\n\nRecall our discussion of random variables: used by analogy to algebra, since we can do math with them:\nJust as \\(2 \\cdot 3\\) is shorthand for \\(2 + 2 + 2\\), we can define \\(X\\) as shorthand for the possible outcomes of a random process. \\[\n\\begin{align*}\nS = \\{ &\\text{result of dice roll is 1}, \\\\\n&\\text{result of dice roll is 2}, \\\\\n&\\text{result of dice roll is 3}, \\\\\n&\\text{result of dice roll is 4}, \\\\\n&\\text{result of dice roll is 5}, \\\\\n&\\text{result of dice roll is 6}\\} \\rightsquigarrow X \\in \\{1,\\ldots,6\\}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "w02/index.html#random-variables-as-events",
    "href": "w02/index.html#random-variables-as-events",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Random Variables as Events",
    "text": "Random Variables as Events\n\nEach value \\(v_X\\) that a random variable \\(X\\) can take on gives rise to an event \\(X = v_X\\): the event that the random variable \\(X\\) takes on value \\(v\\).\nSince \\(X = v_X\\) is an event, we can compute its probability \\(\\Pr(X = v_X)\\)!\n\n\n\n\nEvent in words\nEvent in terms of RV\n\n\n\n\nResult of dice roll is 1\n\\(X = 1\\)\n\n\nResult of dice roll is 2\n\\(X = 2\\)\n\n\nResult of dice roll is 3\n\\(X = 3\\)\n\n\nResult of dice roll is 4\n\\(X = 4\\)\n\n\nResult of dice roll is 5\n\\(X = 5\\)\n\n\nResult of dice roll is 6\n\\(X = 6\\)"
  },
  {
    "objectID": "w02/index.html#doing-math-with-events",
    "href": "w02/index.html#doing-math-with-events",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Doing Math with Events",
    "text": "Doing Math with Events\n\nWe‚Äôve seen how \\(\\Pr(\\cdot)\\) can ‚Äúencode‚Äù logical expressions involving uncertain outcomes.\nEven more powerful when paired with the notion of random variables: lets us also ‚Äúencode‚Äù mathematical expressions involving uncertain quantities!\nConsider an experiment where we roll two dice. Let \\(X\\) be the RV encoding the outcome of the first roll, and \\(Y\\) be the RV encoding the outcome of the second roll.\nWe can compute probabilities involving \\(X\\) and \\(Y\\) separately, e.g., \\(\\Pr(X = 1) = \\frac{1}{6}\\), but we can also reason probabilistically about mathematical expressions involving \\(X\\) and \\(Y\\)! For example, we can reason about their sum:\n\n\\[\n\\begin{align*}\n\\Pr(\\text{rolls sum to 10}) &= \\Pr(X + Y = 10) \\\\\n&= \\Pr(Y = 10 - X)\n\\end{align*}\n\\]\n\nOr about how the outcome of one roll will relate to the outcome of the other:\n\n\\[\n\\begin{align*}\n\\Pr(\\text{first roll above mean}) &= \\Pr\\left(X &gt; \\frac{X+Y}{2}\\right) \\\\\n&= \\Pr(2X &gt; X+Y) = \\Pr(X &gt; Y)\n\\end{align*}\n\\]"
  },
  {
    "objectID": "w02/index.html#are-random-variables-all-powerful",
    "href": "w02/index.html#are-random-variables-all-powerful",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Are Random Variables All-Powerful??",
    "text": "Are Random Variables All-Powerful??\n\nJust remember that probability \\(P(\\cdot)\\) is always probability of an event‚Äîrandom variables are just shorthand for quantifiable events.\nNot all events can be simplified via random variables!\n\n\\(\\text{catch a fish} \\mapsto P(\\text{trout}), P(\\text{bass}), \\ldots\\)\n\nWhat types of events can be quantified like this?\n\n(Hint: It has to do with a key topic in the early weeks of both DSAN 5000 and 5100‚Ä¶)\n\n\n\nThe answer is, broadly, any situation where you‚Äôre modeling things, like dice rolls, where mathematical operations like addition, multiplication, etc. make sense. So, if we‚Äôre modeling dice, it makes sense to say e.g.¬†‚Äúresult is 6‚Äù + ‚Äúresult is 3‚Äù = ‚Äútotal is 9‚Äù. More on the next page!"
  },
  {
    "objectID": "w02/index.html#recall-types-of-variables",
    "href": "w02/index.html#recall-types-of-variables",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Recall: Types of Variables",
    "text": "Recall: Types of Variables\n\nCategorical\n\nNo meaningful way to order values: \\(\\{\\text{trout}, \\text{bass}, \\ldots \\}\\)\n\nOrdinal\n\nCan place in order (bigger, smaller), though gaps aren‚Äôt meaningful: \\(\\{{\\color{orange}\\text{great}},{\\color{orange}\\text{greater}},{\\color{orange}\\text{greatest}}\\}\\)\n\\({\\color{orange}\\text{greater}} \\overset{?}{=} 2\\cdot {\\color{orange}\\text{great}} - 1\\)\n\nCardinal\n\nCan place in order, and gaps are meaningful \\(\\implies\\) can do ‚Äústandard‚Äù math with them! Example: \\(\\{{\\color{blue}1},{\\color{blue}2},\\ldots,{\\color{blue}10}\\}\\)\n\\({\\color{blue}7}\\overset{\\color{green}\\unicode{x2714}}{=} 2 \\cdot {\\color{blue}4} - 1\\)\nIf events have this structure (meaningful way to define multiplication, addition, subtraction), then we can analyze them as random variables"
  },
  {
    "objectID": "w02/index.html#visualizing-discrete-rvs",
    "href": "w02/index.html#visualizing-discrete-rvs",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Visualizing Discrete RVs",
    "text": "Visualizing Discrete RVs\n\nUltimate Probability Pro-Tip: When you hear ‚Äúdiscrete distribution‚Äù, think of a bar graph: \\(x\\)-axis = events, bar height = probability of events\nTwo coins example: \\(X\\) = RV representing number of heads obtained in two coin flips\n\n\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.2     ‚úî readr     2.1.4\n‚úî forcats   1.0.0     ‚úî stringr   1.5.0\n‚úî lubridate 1.9.2     ‚úî tibble    3.2.1\n‚úî purrr     1.0.2     ‚úî tidyr     1.3.0\n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors"
  },
  {
    "objectID": "w02/index.html#preview-visualizing-continuous-rvs",
    "href": "w02/index.html#preview-visualizing-continuous-rvs",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "(Preview:) Visualizing Continuous RVs",
    "text": "(Preview:) Visualizing Continuous RVs\n\nThis works even for continuous distributions, if you focus on the area under the curve instead of the height:\n\n\nfuncShaded &lt;- function(x, lower_bound, upper_bound) {\n    y &lt;- dnorm(x)\n    y[x &lt; lower_bound | x &gt; upper_bound] &lt;- NA\n    return(y)\n}\nfuncShadedBound1 &lt;- function(x) funcShaded(x, -Inf, 0)\nfuncShadedBound2 &lt;- function(x) funcShaded(x, 0.2, 1.8)\nfuncShadedBound3 &lt;- function(x) funcShaded(x, 2, Inf)\n\nnorm_plot &lt;- ggplot(data.frame(x=c(-3,3)), aes(x = x)) +\n    stat_function(fun = dnorm) +\n    labs(\n      title=\"Probability Density, X Normally Distributed\",\n      x=\"Possible Values of X\",\n      y=\"Probability Density\"\n    ) +\n    dsan_theme(\"half\") +\n    theme(legend.position = \"none\") +\n    coord_cartesian(clip = \"off\")\nlabel_df &lt;- tribble(\n  ~x, ~y, ~label,\n  -0.8, 0.1, \"Pr(X &lt; 0) = 0.5\",\n  1.0, 0.05, \"Pr(0.2 &lt; X &lt; 1.8)\\n= 0.385\",\n  2.5,0.1,\"Pr(X &gt; 1.96)\\n= 0.025\"\n)\nshaded_plot &lt;- norm_plot +\n  stat_function(fun = funcShadedBound1, geom = \"area\", fill=cbPalette[1], alpha = 0.5) +\n  stat_function(fun = funcShadedBound2, geom = \"area\", fill=cbPalette[2], alpha = 0.5) +\n  stat_function(fun = funcShadedBound3, geom = \"area\", fill=cbPalette[3], alpha = 0.5) +\n  geom_text(label_df, mapping=aes(x = x, y = y, label = label), size=6)\nshaded_plot"
  },
  {
    "objectID": "w02/index.html#probability-theory-gives-us-distributions-for-rvs-not-numbers",
    "href": "w02/index.html#probability-theory-gives-us-distributions-for-rvs-not-numbers",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Probability Theory Gives Us Distributions for RVs, not Numbers!",
    "text": "Probability Theory Gives Us Distributions for RVs, not Numbers!\n\nWe‚Äôre going beyond ‚Äúbase‚Äù probability theory if we want to summarize these distributions\nHowever, we can understand a lot about the full distribution by looking at some basic summary statistics. Most common way to summarize:\n\n\n\n\n\n\n\n\n\n\\(\\underbrace{\\text{point estimate}}_{\\text{mean/median}}\\)\n\\(\\pm\\)\n\\(\\underbrace{\\text{uncertainty}}_{\\text{variance/standard deviation}}\\)"
  },
  {
    "objectID": "w02/index.html#example-game-reviews",
    "href": "w02/index.html#example-game-reviews",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Example: Game Reviews",
    "text": "Example: Game Reviews\n\nlibrary(readr)\nfig_title &lt;- \"Review for a Popular Nintendo Switch Game\"\nfig_subtitle &lt;- \"(That I definitely didn't play for &gt;400 hours this summer...)\"\n#score_df &lt;- read_csv(\"https://gist.githubusercontent.com/jpowerj/8b2b6a50cef5a682db640e874a14646b/raw/bbe07891a90874d1fe624224c1b82212b1ac8378/totk_scores.csv\")\nscore_df &lt;- read_csv(\"https://gist.githubusercontent.com/jpowerj/8b2b6a50cef5a682db640e874a14646b/raw/e3c2b9d258380e817289fbb64f91ba9ed4357d62/totk_scores.csv\")\n\nRows: 145 Columns: 1\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\ndbl (1): score\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nmean_score &lt;- mean(score_df$score)\nlibrary(ggplot2)\nggplot(score_df, aes(x=score)) +\n  geom_histogram() +\n  #geom_vline(xintercept=mean_score) +\n  labs(\n    title=fig_title,\n    subtitle=fig_subtitle,\n    x=\"Review Score\",\n    y=\"Number of Reviews\"\n  ) +\n  dsan_theme(\"full\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n(Data from Metacritic)"
  },
  {
    "objectID": "w02/index.html#adding-a-single-line",
    "href": "w02/index.html#adding-a-single-line",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Adding a Single Line",
    "text": "Adding a Single Line\n\nlibrary(readr)\nmean_score &lt;- mean(score_df$score)\nmean_score_label &lt;- sprintf(\"%0.2f\", mean_score)\nlibrary(ggplot2)\nggplot(score_df, aes(x=score)) +\n  geom_histogram() +\n  geom_vline(aes(xintercept=mean_score, linetype=\"dashed\"), color=\"purple\", size=1) +\n  scale_linetype_manual(\"\", values=c(\"dashed\"=\"dashed\"), labels=c(\"dashed\"=\"Mean Score\")) +\n  # Add single additional tick\n  scale_x_continuous(breaks=c(60, 70, 80, 90, mean_score, 100), labels=c(\"60\",\"70\",\"80\",\"90\",mean_score_label,\"100\")) +\n  labs(\n    title=fig_title,\n    subtitle=fig_subtitle,\n    x=\"Review Score\",\n    y=\"Number of Reviews\"\n  ) +\n  dsan_theme(\"full\") +\n  theme(\n    legend.title = element_blank(),\n    legend.spacing.y = unit(0, \"mm\")\n  ) +\n  theme(axis.text.x = element_text(colour = c('black', 'black','black', 'black', 'purple', 'black')))\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `linewidth` instead.\n\n\nWarning: Vectorized input to `element_text()` is not officially supported.\n‚Ñπ Results may be unexpected or may change in future versions of ggplot2.\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n(Data from Metacritic)"
  },
  {
    "objectID": "w02/index.html#or-a-single-ribbon",
    "href": "w02/index.html#or-a-single-ribbon",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Or a Single Ribbon",
    "text": "Or a Single Ribbon\n\n\n\nlibrary(tibble)\nN &lt;- 10\n# Each x value gets 10 y values\nx &lt;- sort(rep(seq(1,10),10))\ny &lt;- x + rnorm(length(x), 0, 5)\ndf &lt;- tibble(x=x,y=y)\ntotal_N &lt;- nrow(df)\nggplot(df, aes(x=x,y=y)) +\n  geom_point(size=g_pointsize) +\n  dsan_theme(\"column\") +\n  labs(\n    title=paste0(\"N=\",total_N,\" Randomly-Generated Points\")\n  )\n\n\n\n\n\n# This time, just the means\nlibrary(dplyr)\nmean_df &lt;- df %&gt;% group_by(x) %&gt;% summarize(mean=mean(y), min=min(y), max=max(y))\nggplot(mean_df, aes(x=x, y=mean)) +\n  geom_ribbon(aes(ymin=min, ymax=max, fill=\"ribbon\"), alpha=0.5) +\n  geom_point(aes(color=\"mean\"), size=g_pointsize) +\n  geom_line(size=g_linesize) +\n  dsan_theme(\"half\") +\n  scale_color_manual(\"\", values=c(\"mean\"=\"black\"), labels=c(\"mean\"=\"Mean\")) +\n  scale_fill_manual(\"\", values=c(\"ribbon\"=cbPalette[1]), labels=c(\"ribbon\"=\"Range\")) +\n  remove_legend_title() +\n  labs(\n    title=paste0(\"Means of N=\",total_N,\" Randomly-Generated Points\")\n  )\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\nlibrary(tibble)\nN &lt;- 100\n# Each x value gets 10 y values\nx &lt;- sort(rep(seq(1,10),10))\ny &lt;- x + rnorm(length(x), 0, 1)\ndf &lt;- tibble(x=x,y=y)\ntotal_N &lt;- nrow(df)\nggplot(df, aes(x=x,y=y)) +\n  geom_point(size=g_pointsize) +\n  dsan_theme(\"column\") +\n  labs(\n    title=paste0(\"N=\",total_N,\" Randomly-Generated Points\")\n  )\n\n\n\n\n\n# This time, just the means\nlibrary(dplyr)\nmean_df &lt;- df %&gt;% group_by(x) %&gt;% summarize(mean=mean(y), min=min(y), max=max(y))\nggplot(mean_df, aes(x=x, y=mean)) +\n  geom_ribbon(aes(ymin=min, ymax=max, fill=\"ribbon\"), alpha=0.5) +\n  geom_point(aes(color=\"mean\"), size=g_pointsize) +\n  geom_line(size=g_linesize) +\n  dsan_theme(\"half\") +\n  scale_color_manual(\"\", values=c(\"mean\"=\"black\"), labels=c(\"mean\"=\"Mean\")) +\n  scale_fill_manual(\"\", values=c(\"ribbon\"=cbPalette[1]), labels=c(\"ribbon\"=\"Range\")) +\n  remove_legend_title() +\n  labs(\n    title=paste0(\"Means of N=\",total_N,\" Randomly-Generated Points\")\n  )"
  },
  {
    "objectID": "w02/index.html#example-the-normal-distribution",
    "href": "w02/index.html#example-the-normal-distribution",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Example: The Normal Distribution",
    "text": "Example: The Normal Distribution\n\n\n\n(The distribution you saw a few slides ago)\n\n\nvlines_std_normal &lt;- tibble::tribble(\n  ~x, ~xend, ~y, ~yend, ~Params,\n  0, 0, 0, dnorm(0), \"Mean\",\n  -2, -2, 0, dnorm(-2), \"SD\",\n  -1, -1, 0, dnorm(-1), \"SD\",\n  1, 1, 0, dnorm(1), \"SD\",\n  2, 2, 0, dnorm(2), \"SD\"\n)\nggplot(data.frame(x = c(-3, 3)), aes(x = x)) +\n    stat_function(fun = dnorm, linewidth = g_linewidth) +\n    geom_segment(data=vlines_std_normal, aes(x=x, xend=xend, y=y, yend=yend, linetype = Params), linewidth = g_linewidth, color=\"purple\") +\n    geom_area(stat = \"function\", fun = dnorm, fill = cbPalette[1], xlim = c(-3, 3), alpha=0.2) +\n    #geom_area(stat = \"function\", fun = dnorm, fill = \"blue\", xlim = c(0, 2))\n    dsan_theme(\"quarter\") +\n    labs(\n      x = \"v\",\n      y = \"Density f(v)\"\n    )\n\n\n\n\n\n\n‚ÄúRV \\(X\\) is normally distributed with mean \\({\\color{purple}\\mu}\\) and standard deviation \\({\\color{purple}\\sigma}\\)‚Äù\n\nTranslates to \\(X \\sim \\mathcal{N}(\\color{purple}{\\mu},\\color{purple}{\\sigma})\\)2\n\\(\\color{purple}{\\mu}\\) and \\(\\color{purple}{\\sigma}\\) are parameters3: the ‚Äúknobs‚Äù or ‚Äúsliders‚Äù which change the location/shape of the distribution\n\n\n\n\n\nThe parameters in this case give natural summaries of the data:\n\n\\({\\color{\\purple}\\mu}\\) = center (mean), \\({\\color{purple}\\sigma}\\) = [square root of] variance around center\n\nMean can usually be interpreted intuitively; for standard deviation, we usually use the 68-95-99.7 rule, which will make more sense relative to some real-world data‚Ä¶"
  },
  {
    "objectID": "w02/index.html#real-data-and-the-68-95-99.7-rule",
    "href": "w02/index.html#real-data-and-the-68-95-99.7-rule",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Real Data and the 68-95-99.7 Rule",
    "text": "Real Data and the 68-95-99.7 Rule\n\n\n\n\n\n\n\nFigure¬†2: Heights (cm) for 18K professional athletes\n\n\n\n\n\n\n\nFigure¬†3: The 68-95-99.7 Rule visualized (Wikimedia Commons)\n\n\n\n\nThe point estimate \\({\\color{purple}\\mu} = 186.48\\) is straightforward: the average height of the athletes is 186.48cm. Using the 68-95-99.7 Rule to interpret the SD, \\({\\color{purple}\\sigma} = 9.7\\), we get:\n\n\n\nAbout 68% of the heights fall between\n\n\n\n\n\n\n\n\n\n[\\({\\color{purple}\\mu} - 1\\cdot {\\color{purple}\\sigma}\\)\nand\n\\({\\color{purple}\\mu} + 1\\cdot {\\color{purple}\\sigma}\\)]\n\n\n[186.48 - 1 ¬∑ 9.7\nand\n186.48 + 1 ¬∑ 9.7]\n\n\n[176.78\nand\n196.18]\n\n\n\n\n\nAbout 95% of the heights fall between\n\n\n\n\n\n\n\n\n\n[\\({\\color{purple}\\mu} - 2 \\cdot {\\color{purple}\\sigma}\\)\nand\n\\({\\color{purple}\\mu} + 2 \\cdot {\\color{purple}\\sigma}\\)]\n\n\n[186.48 - 2 ¬∑ 9.7\nand\n186.48 + 2 ¬∑ 9.7]\n\n\n[167.08\nand\n205.88]"
  },
  {
    "objectID": "w02/index.html#boxplots-comparing-multiple-distributions",
    "href": "w02/index.html#boxplots-comparing-multiple-distributions",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Boxplots: Comparing Multiple Distributions",
    "text": "Boxplots: Comparing Multiple Distributions\n\n\n\n\n\n\nJhguch at en.wikipedia, CC BY-SA 2.5, via Wikimedia Commons\n\n\n\n\n\n\n\nProtonk, CC BY-SA 3.0, via Wikimedia Commons"
  },
  {
    "objectID": "w02/index.html#another-option-joyplots",
    "href": "w02/index.html#another-option-joyplots",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Another Option: Joyplots",
    "text": "Another Option: Joyplots\n\n\n\n\n\n\nFigure¬†4: (Iconic album cover)\n\n\n\n\n\n\nFigure¬†5: (Tooting my own horn)"
  },
  {
    "objectID": "w02/index.html#multivariate-distributions-preview",
    "href": "w02/index.html#multivariate-distributions-preview",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Multivariate Distributions: Preview",
    "text": "Multivariate Distributions: Preview\n\nThe bivariate normal distribution represents the distribution of two normally-distributed RVs \\(\\mathbf{X} = [\\begin{smallmatrix} X_1 & X_2\\end{smallmatrix}]\\), which may or may not be correlated:\n\n\n\\[\n\\mathbf{X} = \\begin{bmatrix}X_1 \\\\ X_2\\end{bmatrix}, \\; \\boldsymbol{\\mu} =\n%\\begin{bmatrix}\\mu_1 \\\\ \\mu_2\\end{bmatrix}\n\\begin{bmatrix}\\smash{\\overbrace{\\mu_1}^{\\mathbb{E}[X_1]}} \\\\ \\smash{\\underbrace{\\mu_2}_{\\mathbb{E}[X_2]}}\\end{bmatrix}\n, \\; \\mathbf{\\Sigma} = \\begin{bmatrix}\\smash{\\overbrace{\\sigma_1^2}^{\\text{Var}[X_1]}} & \\smash{\\overbrace{\\rho\\sigma_1\\sigma_2}^{\\text{Cov}[X_1,X_2]}} \\\\ \\smash{\\underbrace{\\rho\\sigma_2\\sigma_1}_{\\text{Cov}[X_2,X_1]}} & \\smash{\\underbrace{\\sigma_2^2}_{\\text{Var}[X_2]}}\\end{bmatrix}\n% \\begin{bmatrix}\\sigma_1^2 & \\rho\\sigma_1\\sigma_2 \\\\ \\rho\\sigma_2\\sigma_1 & \\sigma_2^2 \\end{bmatrix}\n% = \\begin{bmatrix}\\text{Var}[X_1] & \\text{Cov}[X_1,X_2] \\\\ \\text{Cov}[X_2,X_1] & \\text{Var}[X_2] \\end{bmatrix}\n\\]\n\n\nBy squishing all this information intro matrices, we can specify the parameters of multivariate-normally-distributed vectors of RVs similarly to how we specify single-dimensional normally-distributed RVs:\n\n\n\\[\n\\begin{align*}\n\\overbrace{X}^{\\mathclap{\\text{scalar}}} &\\sim \\mathcal{N}\\phantom{_k}(\\overbrace{\\mu}^{\\text{scalar}}, \\overbrace{\\sigma}^{\\text{scalar}}) \\tag{Univariate} \\\\\n\\underbrace{\\mathbf{X}}_{\\text{vector}} &\\sim \\boldsymbol{\\mathcal{N}}_k(\\smash{\\underbrace{\\boldsymbol{\\mu}}_{\\text{vector}}}, \\underbrace{\\mathbf{\\Sigma}}_{\\text{matrix}}) \\tag{Multivariate}\n\\end{align*}\n\\]\n\n\nNote: In the future I‚Äôll use the notation \\(\\mathbf{X}_{[a \\times b]}\\) to denote the dimensions of the vectors/matrices, like \\(\\mathbf{X}_{[k \\times 1]} \\sim \\boldsymbol{\\mathcal{N}}_k(\\boldsymbol{\\mu}_{[k \\times 1]}, \\mathbf{\\Sigma}_{[k \\times k]})\\)"
  },
  {
    "objectID": "w02/index.html#visualizing-3d-distributions-projection",
    "href": "w02/index.html#visualizing-3d-distributions-projection",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Visualizing 3D Distributions: Projection",
    "text": "Visualizing 3D Distributions: Projection\n\nSince most of our intuitions about plots come from 2D plots, it is extremely useful to be able to take a 3D plot like this and imagine ‚Äúprojecting‚Äù it down into different 2D plots:\n\n\n\n\n(Adapted via LaTeX from StackExchange discussion)"
  },
  {
    "objectID": "w02/index.html#visualizing-3d-distributions-contours",
    "href": "w02/index.html#visualizing-3d-distributions-contours",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Visualizing 3D Distributions: Contours",
    "text": "Visualizing 3D Distributions: Contours\n\n\n\nFrom Prof.¬†Hickman‚Äôs slides!"
  },
  {
    "objectID": "w02/index.html#visualizing-3d-distributions-contours-1",
    "href": "w02/index.html#visualizing-3d-distributions-contours-1",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Visualizing 3D Distributions: Contours",
    "text": "Visualizing 3D Distributions: Contours\n\n\n\nAlso from Prof.¬†Hickman‚Äôs slides!"
  },
  {
    "objectID": "w02/index.html#footnotes",
    "href": "w02/index.html#footnotes",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor math majors, you can think of it as an isomorphism between the objects and operations of the three subjects‚Ü©Ô∏é\nThroughout the course, this ‚Äúcalligraphic‚Äù font \\(\\mathcal{N}\\), \\(\\mathcal{D}\\), etc., will be used to denote distributions‚Ü©Ô∏é\nThroughout the course, remember, purrple is for purrameters‚Ü©Ô∏é"
  },
  {
    "objectID": "w03/index.html",
    "href": "w03/index.html",
    "title": "Week 3: Conditional Probability",
    "section": "",
    "text": "Open slides in new window ‚Üí"
  },
  {
    "objectID": "w03/index.html#recap-1",
    "href": "w03/index.html#recap-1",
    "title": "Week 3: Conditional Probability",
    "section": "Recap",
    "text": "Recap\n\nLogic \\(\\rightarrow\\) Set Theory \\(\\rightarrow\\) Probability Theory\nEntirety of probability theory can be derived from two axioms:\n\n\n\n\n\n\n\nThe Entirety of Probability Theory Follows From‚Ä¶\n\n\n\nAxiom 1 (Unitarity): \\(\\Pr(\\Omega) = 1\\) (The probability that something happens is 1)\nAxiom 2 (\\(\\sigma\\)-additivity): For mutually-exclusive events \\(E_1, E_2, \\ldots\\),\n\\[\n\\underbrace{\\Pr\\left(\\bigcup_{i=1}^{\\infty}E_i\\right)}_{\\Pr(E_1\\text{ occurs }\\vee E_2\\text{ occurs } \\vee \\cdots)} = \\underbrace{\\sum_{i=1}^{\\infty}\\Pr(E_i)}_{\\Pr(E_1\\text{ occurs}) + \\Pr(E_2\\text{ occurs}) + \\cdots}\n\\]\n\n\n\nBut what does ‚Äúmutually exclusive‚Äù mean‚Ä¶?"
  },
  {
    "objectID": "w03/index.html#venn-diagrams-sets",
    "href": "w03/index.html#venn-diagrams-sets",
    "title": "Week 3: Conditional Probability",
    "section": "Venn Diagrams: Sets",
    "text": "Venn Diagrams: Sets\n\n\n\n\n\n\n\n\n\\[\n\\begin{align*}\n&A = \\{0, 1, 2\\}, \\; B = \\{4, 5, 6\\} \\\\\n&\\implies A \\cap B = \\varnothing\n\\end{align*}\n\\]\nFigure¬†1: Mutually-exclusive (disjoint) sets\n\n\n\n\n\n\n\n\\[\n\\begin{align*}\n&A = \\{1, 2, 3\\}, \\; B = \\{3, 4, 5\\} \\\\\n&\\implies A \\cap B = \\{3\\}\n\\end{align*}\n\\]\nFigure¬†2: Non-mutually-exclusive sets"
  },
  {
    "objectID": "w03/index.html#venn-diagrams-events-dice",
    "href": "w03/index.html#venn-diagrams-events-dice",
    "title": "Week 3: Conditional Probability",
    "section": "Venn Diagrams: Events (Dice)",
    "text": "Venn Diagrams: Events (Dice)\n\\[\n\\begin{align*}\nA &= \\{\\text{Roll is even}\\} = \\{2, 4, 6\\} \\\\\nB &= \\{\\text{Roll is odd}\\} = \\{1, 3, 5\\} \\\\\nC &= \\{\\text{Roll is in Fibonnaci sequence}\\} = \\{1, 2, 3, 5\\}\n\\end{align*}\n\\]\n\n\n\n\n\n\n\n\n\n\nSet 1\nSet 2\nIntersection\nMutually Exclusive?\nCan Happen Simultaneously?\n\n\n\n\n\\(A\\)\n\\(B\\)\n\\(A \\cap B = \\varnothing\\)\nYes\nNo\n\n\n\\(A\\)\n\\(C\\)\n\\(A \\cap C = \\{2\\}\\)\nNo\nYes\n\n\n\\(B\\)\n\\(C\\)\n\\(B \\cap C = \\{1, 3, 5\\}\\)\nNo\nYes"
  },
  {
    "objectID": "w03/index.html#rules-of-probability",
    "href": "w03/index.html#rules-of-probability",
    "title": "Week 3: Conditional Probability",
    "section": "‚ÄúRules‚Äù of Probability",
    "text": "‚ÄúRules‚Äù of Probability\n\n(Remember: not ‚Äúrules‚Äù but ‚Äúfacts resulting from the logic \\(\\leftrightarrow\\) probability connection‚Äù)\n\n\n\n\n\n\n\n‚ÄúRules‚Äù of Probability\n\n\n\nFor logical predicates \\(p, q \\in \\{T, F\\}\\), events \\(P, Q\\) defined so \\(P\\) = event that \\(p\\) becomes true, \\(Q\\) = event that \\(q\\) becomes true,\n\nLogical AND = Probabilistic Multiplication\n\n\\[\n\\Pr(p \\wedge q) = \\Pr(P \\cap Q) = \\Pr(P) \\cdot \\Pr(Q)\n\\]\n\nLogical OR = Probabilistic Addition\n\n\\[\n\\Pr(p \\vee q) = \\Pr(P \\cup Q) = \\Pr(P) + \\Pr(Q) - \\underbrace{\\Pr(P \\cap Q)}_{\\text{(see rule 1)}}\n\\]\n\nLogical NOT = Probabilistic Complement\n\n\\[\n\\Pr(\\neg p) = \\Pr(P^c) = 1 - \\Pr(P)\n\\]"
  },
  {
    "objectID": "w03/index.html#conditional-probability",
    "href": "w03/index.html#conditional-probability",
    "title": "Week 3: Conditional Probability",
    "section": "Conditional Probability",
    "text": "Conditional Probability\n\nUsually if someone asks you probabilistic questions, like\n\n‚ÄúWhat is the likelihood that [our team] wins?‚Äù\n‚ÄúDo you think it will rain tomorrow?‚Äù and so on\n\nYou don‚Äôt guess a random number, you consider and incorporate evidence.\nExample: \\(\\Pr(\\text{rain})\\) on its own, without any other info? A tough question‚Ä¶ maybe \\(0.5\\)?\nIn reality, we would think about\n\n\\(\\Pr(\\text{rain} \\mid \\text{month of the year})\\)\n\\(\\Pr(\\text{rain} \\mid \\text{where we live})\\)\n\\(\\Pr(\\text{rain} \\mid \\text{did it rain yesterday?})\\)\n\nPsychologically, breaks down into two steps: (1) Think of a baseline probability, (2) Update baseline probability to incorporate relevant evidence (more on this in a bit‚Ä¶)\nAlso recall from last week: all probability is conditional probability, even if just conditioned on ‚Äúsomething happened‚Äù (\\(\\Omega\\), the thing defined so \\(\\Pr(\\Omega) = 1\\))"
  },
  {
    "objectID": "w03/index.html#na√Øve-definition-2.0",
    "href": "w03/index.html#na√Øve-definition-2.0",
    "title": "Week 3: Conditional Probability",
    "section": "Na√Øve Definition 2.0",
    "text": "Na√Øve Definition 2.0\n\n\n\n\n\n\n[Slightly Less] Na√Øve Definition of Probability\n\n\n\n\\[\n\\Pr(A \\mid B) = \\frac{\\text{\\# of Desired Outcomes in world where }B\\text{ happened}}{\\text{\\# Total outcomes in world where }B\\text{ happened}} = \\frac{|B \\cap A|}{|B|}\n\\]\n\n\n\n\n\n\n\n\n\n\nWorld Name\nWeather in World\nLikelihood of Rain Today\n\n\n\n\n\\(R\\)\nRained for the past 5 days\n\\(\\Pr(\\text{rain} \\mid R) &gt; 0.5\\)\n\n\n\\(M\\)\nMix of rain and non-rain over past 5 days\n\\(\\Pr(\\text{rain} \\mid M) \\approx 0.5\\)\n\n\n\\(S\\)\nSunny for the past 5 days\n\\(\\Pr(\\text{rain} \\mid S) &lt; 0.5\\)"
  },
  {
    "objectID": "w03/index.html#law-of-total-probability",
    "href": "w03/index.html#law-of-total-probability",
    "title": "Week 3: Conditional Probability",
    "section": "Law of Total Probability",
    "text": "Law of Total Probability\n\nSuppose the events \\(B_1, \\ldots, B_k\\) form a partition of the space \\(S\\) and \\(\\Pr(B_j) &gt; 0 \\forall j\\).\nThen, for every event \\(A\\) in \\(S\\),\n\n\\[\n\\Pr(A) = \\sum_{i=1}^k \\Pr(B_j)\\Pr(A \\mid B_j)\n\\]\n\nProbability of an event is the sum of its conditional probabilities across all conditions.\nIn other words: \\(A\\) is some event, \\(B_1, \\ldots, B_n\\) are mutually exclusive events filling entire sample-space, then\n\n\\[\n\\Pr(A) = \\Pr(A \\mid B_1)\\Pr(B_1) + \\Pr(A \\mid B_2)\\Pr(B_2) + \\cdots + \\Pr(A \\mid B_n)\\Pr(B_n)\n\\]\ni.e.¬†Compute the probability by summing over all possible cases."
  },
  {
    "objectID": "w03/index.html#example",
    "href": "w03/index.html#example",
    "title": "Week 3: Conditional Probability",
    "section": "Example",
    "text": "Example\n\nProbabilities of completing a job on time, with and without rain, are 0.42 and 0.90 respectively.\nProbability it will rain is 0.45. What is the probability the job will be completed on time?\n\\(A\\) = job will be completed on time, \\(B\\) = rain\n\n\\[\n\\Pr(B) = 0.45 \\implies \\Pr(B^c) = 1 - \\Pr(B) = 0.55.\n\\]\n\nNote: Events \\(B\\) and \\(B^c\\) are exclusive and form partitions of the sample space \\(S\\)\nWe know \\(\\Pr(A \\mid B) = 0.24\\), \\(\\Pr(A \\mid B^c) = 0.9\\).\nBy the Law of Total Probability, we have\n\n\\[\n\\begin{align*}\n\\Pr(A) &= \\Pr(B)\\Pr(A \\mid B) + \\Pr(B^c)\\Pr(A \\mid B^c) \\\\\n&= 0.45(0.42) + 0.55(0.9) = 0.189 + 0.495 = 0684.\n\\end{align*}\n\\]\nSo, the probability that the job will be completed on time is 0.684. (source)"
  },
  {
    "objectID": "w03/index.html#deriving-bayes-theorem",
    "href": "w03/index.html#deriving-bayes-theorem",
    "title": "Week 3: Conditional Probability",
    "section": "Deriving Bayes‚Äô Theorem",
    "text": "Deriving Bayes‚Äô Theorem\n\nLiterally just a re-writing of the conditional probability definition (don‚Äôt be scared)!\n\n\n\n\nFor two events \\(A\\) and \\(B\\), definition of conditional probability says that\n\n\\[\n\\begin{align*}\n\\Pr(A \\mid B) &= \\frac{\\Pr(A \\cap B)}{\\Pr(B)} \\tag{1} \\\\\n\\Pr(B \\mid A) &= \\frac{\\Pr(B \\cap A)}{\\Pr(A)} \\tag{2}\n\\end{align*}\n\\]\n\nMultiply to get rid of fractions\n\n\\[\n\\begin{align*}\n\\Pr(A \\mid B)\\Pr(B) &= \\Pr(A \\cap B) \\tag{1*} \\\\\n\\Pr(B \\mid A)\\Pr(A) &= \\Pr(B \\cap A) \\tag{2*}\n\\end{align*}\n\\]\n\n\nBut set intersection is associative (just like multiplication‚Ä¶), \\(A \\cap B = B \\cap A\\)! So, we know LHS of \\((\\text{1*})\\) = LHS of \\((\\text{2*})\\):\n\n\\[\n\\Pr(A \\mid B)\\Pr(B) = \\Pr(B \\mid A)\\Pr(A)\n\\]\n\nDivide both sides by \\(\\Pr(B)\\) to get a new definition of \\(\\Pr(A \\mid B)\\), Bayes‚Äô Theorem!\n\n\n\n\\[\n\\boxed{\\Pr(A \\mid B) = \\frac{\\Pr(B \\mid A)\\Pr(A)}{\\Pr(B)}}\n\\]\nFigure¬†3: Bayes‚Äô Theorem"
  },
  {
    "objectID": "w03/index.html#why-is-this-helpful",
    "href": "w03/index.html#why-is-this-helpful",
    "title": "Week 3: Conditional Probability",
    "section": "Why Is This Helpful?",
    "text": "Why Is This Helpful?\n\n\n\n\n\n\nBayes‚Äô Theorem\n\n\n\nFor any two events \\(A\\) and \\(B\\), \\[\n\\Pr(A \\mid B) = \\frac{\\Pr(B \\mid A)\\Pr(A)}{\\Pr(B)}\n\\]\n\n\n\nIn words (as exciting as I can make it, for now): Bayes‚Äô Theorem allows us to take information about \\(B \\mid A\\) and use it to infer information about \\(A \\mid B\\)\nIt isn‚Äôt until you work through some examples that this becomes mind-blowing, the most powerful equation we have for inferring unknowns from knowns‚Ä¶\nConsider \\(A = \\{\\text{person has disease}\\}\\), \\(B = \\{\\text{person tests positive for disease}\\}\\)\n\nIs \\(A\\) observable on its own? No, but‚Ä¶\n\nIs \\(B\\) observable on its own? Yes, and\nCan we infer information about \\(A\\) from knowing \\(B\\)? Also Yes, thanks to Bayes!\n\nTherefore, we can use \\(B\\) to infer information about \\(A\\), i.e., calculate \\(\\Pr(A \\mid B)\\)‚Ä¶"
  },
  {
    "objectID": "w03/index.html#why-is-this-helpful-for-data-science",
    "href": "w03/index.html#why-is-this-helpful-for-data-science",
    "title": "Week 3: Conditional Probability",
    "section": "Why Is This Helpful for Data Science?",
    "text": "Why Is This Helpful for Data Science?\n\nIt merges probability theory and hypothesis testing into a single framework:\n\n\\[\nP(\\text{hypothesis} \\mid \\text{data}) = \\frac{P(\\text{data} \\mid \\text{hypothesis})}{P(\\text{data})}\n\\]"
  },
  {
    "objectID": "w03/index.html#probability-forwards-and-backwards",
    "href": "w03/index.html#probability-forwards-and-backwards",
    "title": "Week 3: Conditional Probability",
    "section": "Probability Forwards and Backwards",
    "text": "Probability Forwards and Backwards\n\nTwo discrete RVs:\n\nWeather on a given day, \\(W \\in \\{\\textsf{Rain},\\textsf{Sun}\\}\\)\nAction that day, \\(A \\in \\{\\textsf{Go}, \\textsf{Stay}\\}\\): go to party or stay in and watch movie\n\nData-generating process: if \\(\\textsf{Sun}\\), rolls a die \\(R\\) and goes out unless \\(R = 6\\). If \\(\\textsf{Rain}\\), flips a coin and goes out if \\(\\textsf{H}\\).\nProbabilistic Graphical Model (PGM):"
  },
  {
    "objectID": "w03/index.html#section",
    "href": "w03/index.html#section",
    "title": "Week 3: Conditional Probability",
    "section": "",
    "text": "So, if we know \\(W = \\textsf{Sun}\\), what is \\(P(A = \\textsf{Go})\\)? \\[\n\\begin{align*}\nP(A = \\textsf{Go} \\mid W) &= 1 - P(R = 6) \\\\\n&= 1 - \\frac{1}{6} = \\frac{5}{6}\n\\end{align*}\n\\]\nConditional probability lets us go forwards (left to right):\n\n\n\n\n\n\n\nBut what if we want to perform inference going backwards?"
  },
  {
    "objectID": "w03/index.html#section-1",
    "href": "w03/index.html#section-1",
    "title": "Week 3: Conditional Probability",
    "section": "",
    "text": "If we see Ana at the party, we know \\(A = \\textsf{Go}\\)\nWhat does this tell us about the weather?\nIntuitively, we should increase our degree of belief that \\(W = \\textsf{Sun}\\). But, by how much?\nWe don‚Äôt know \\(P(W \\mid A)\\), only \\(P(A \\mid W)\\)‚Ä¶"
  },
  {
    "objectID": "w03/index.html#section-2",
    "href": "w03/index.html#section-2",
    "title": "Week 3: Conditional Probability",
    "section": "",
    "text": "\\[\nP(W = \\textsf{Sun} \\mid A = \\textsf{Go}) = \\frac{\\overbrace{P(A = \\textsf{Go} \\mid W = \\textsf{Sun})}^{5/6~ ‚úÖ}\\overbrace{P(W = \\textsf{Sun})}^{‚ùì}}{\\underbrace{P(A = \\textsf{Go})}_{‚ùì}}\n\\]\n\nWe‚Äôve seen \\(P(W = \\textsf{Sun})\\) before, it‚Äôs our prior: the probability without having any additional relevant knowledge. So, let‚Äôs say 50/50. \\(P(W = \\textsf{Sun}) = \\frac{1}{2}\\)\nIf we lived in Seattle, we could pick \\(P(W = \\textsf{Sun}) = \\frac{1}{4}\\)"
  },
  {
    "objectID": "w03/index.html#section-3",
    "href": "w03/index.html#section-3",
    "title": "Week 3: Conditional Probability",
    "section": "",
    "text": "\\[\nP(W = \\textsf{Sun} \\mid A = \\textsf{Go}) = \\frac{\\overbrace{P(A = \\textsf{Go} \\mid W = \\textsf{Sunny})}^{5/6~ ‚úÖ}\\overbrace{P(W = \\textsf{Sun})}^{1/2~ ‚úÖ}}{\\underbrace{P(A = \\textsf{Go})}_{‚ùì}}\n\\]\n\n\\(P(A = \\textsf{Go})\\) is trickier: the probability that Ana goes out regardless of what the weather is. But there are only two possible weather outcomes! So we just compute\n\n\\[\n\\begin{align*}\n&P(A = \\textsf{Go}) = \\sum_{\\omega \\in S(W)}P(A = \\textsf{Go}, \\omega) = \\sum_{\\omega \\in S(W)}P(A = \\textsf{Go} \\mid \\omega)P(\\omega) \\\\\n&= P(A = \\textsf{Go} \\mid W = \\textsf{Rain})P(W = \\textsf{Rain}) + P(A = \\textsf{Go} \\mid W = \\textsf{Sun})P(W = \\textsf{Sun}) \\\\\n&= \\left( \\frac{1}{2} \\right)\\left( \\frac{1}{2} \\right) + \\left( \\frac{5}{6} \\right)\\left( \\frac{1}{2} \\right) = \\frac{1}{4} + \\frac{5}{12} = \\frac{2}{3}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "w03/index.html#putting-it-all-together",
    "href": "w03/index.html#putting-it-all-together",
    "title": "Week 3: Conditional Probability",
    "section": "Putting it All Together",
    "text": "Putting it All Together\n\\[\n\\begin{align*}\nP(W = \\textsf{Sun} \\mid A = \\textsf{Go}) &= \\frac{\\overbrace{P(A = \\textsf{Go} \\mid W = \\textsf{Sunny})}^{3/4~ ‚úÖ}\\overbrace{P(W = \\textsf{Sun})}^{1/2~ ‚úÖ}}{\\underbrace{P(A = \\textsf{Go})}_{1/2~ ‚úÖ}} \\\\\n&= \\frac{\\left(\\frac{3}{4}\\right)\\left(\\frac{1}{2}\\right)}{\\frac{1}{2}} = \\frac{\\frac{3}{8}}{\\frac{1}{2}} = \\frac{3}{4}.\n\\end{align*}\n\\]\n\nGiven that we see Ana at the party, we should update our beliefs, so that \\(P(W = \\textsf{Sun}) = \\frac{3}{4}, P(W = \\textsf{Rain}) = \\frac{1}{4}\\)."
  },
  {
    "objectID": "w03/index.html#a-scarier-example",
    "href": "w03/index.html#a-scarier-example",
    "title": "Week 3: Conditional Probability",
    "section": "A Scarier Example",
    "text": "A Scarier Example\n\nBo worries he has a rare disease. He takes a test with 99% accuracy and tests positive. What‚Äôs the probability Bo has the disease? (Intuition: 99%? ‚Ä¶Let‚Äôs do the math!)\n\n\n\n\n\\(H \\in \\{\\textsf{sick}, \\textsf{healthy}\\}, T \\in \\{\\textsf{T}^+, \\textsf{T}^-\\}\\)\nThe test: 99% accurate. \\(P(T = \\textsf{T}^+ \\mid H = \\textsf{sick}) = 0.99\\), \\(P(T = \\textsf{T}^- \\mid H = \\textsf{healthy}) = 0.99\\).\nThe disease: 1 in 10K. \\(P(H = \\textsf{sick}) = \\frac{1}{10000}\\)\nWhat do we want to know? \\(P(H = \\textsf{sick} \\mid T = \\textsf{T}^+)\\)\nHow do we get there?\n\n\n\n\n\nThis photo, originally thought to be of Thomas Bayes, turns out to be probably someone else‚Ä¶ \\(P(\\textsf{Bayes})\\)?\n\n\n\n\n\n\\(H\\) for health, \\(T\\) for test result\nPhoto credit: https://thedatascientist.com/wp-content/uploads/2019/04/reverend-thomas-bayes.jpg"
  },
  {
    "objectID": "w03/index.html#section-4",
    "href": "w03/index.html#section-4",
    "title": "Week 3: Conditional Probability",
    "section": "",
    "text": "\\[\n\\begin{align*}\nP(H = \\textsf{sick} \\mid T = \\textsf{T}^+) &= \\frac{P(T = \\textsf{T}^+ \\mid H = \\textsf{sick})P(H = \\textsf{sick})}{P(T = \\textsf{T}^+)} \\\\\n&= \\frac{(0.99)\\left(\\frac{1}{10000}\\right)}{(0.99)\\left( \\frac{1}{10000} \\right) + (0.01)\\left( \\frac{9999}{10000} \\right)}\n\\end{align*}\n\\]\n\np_sick &lt;- 1 / 10000\np_healthy &lt;- 1 - p_sick\np_pos_given_sick &lt;- 0.99\np_neg_given_sick &lt;- 1 - p_pos_given_sick\np_neg_given_healthy &lt;- 0.99\np_pos_given_healthy &lt;- 1 - p_neg_given_healthy\nnumer &lt;- p_pos_given_sick * p_sick\ndenom1 &lt;- numer\ndenom2 &lt;- p_pos_given_healthy * p_healthy\nfinal_prob &lt;- numer / (denom1 + denom2)\nfinal_prob\n\n[1] 0.009803922\n\n\n\n‚Ä¶ Less than 1% üò±"
  },
  {
    "objectID": "w03/index.html#proof-in-the-pudding",
    "href": "w03/index.html#proof-in-the-pudding",
    "title": "Week 3: Conditional Probability",
    "section": "Proof in the Pudding",
    "text": "Proof in the Pudding\n\nlibrary(tibble)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n# Disease rarity\np_disease &lt;- 1 / 10000\n# 1K people\nnum_people &lt;- 1000\n# Give them ids\nppl_df &lt;- tibble(id=seq(1,num_people))\n# Whether they have the disease or not\nhas_disease &lt;- rbinom(num_people, 2, p_disease)\nppl_df &lt;- ppl_df %&gt;% mutate(has_disease=has_disease)\ndisp(ppl_df %&gt;% head(3))\n\n\n\n\n\n\n\n\nCode\nsum(ppl_df$has_disease) / num_people\n\n\n[1] 0.001\n\n\n\n(Foreshadowing Monte Carlo methods)"
  },
  {
    "objectID": "w03/index.html#section-5",
    "href": "w03/index.html#section-5",
    "title": "Week 3: Conditional Probability",
    "section": "",
    "text": "library(dplyr)\n# Data Generating Process\ntake_test &lt;- function(is_sick) {\n  if (is_sick) {\n    return(rbinom(1,2,p_neg_given_sick))\n  } else {\n    return(rbinom(1,2,p_pos_given_healthy))\n  }\n}\nppl_df['test_result'] &lt;- unlist(lapply(ppl_df$has_disease, take_test))\n#ppl_df %&gt;% head(5)\nnum_positive &lt;- sum(ppl_df$test_result)\np_positive &lt;- num_positive / num_people\nwriteLines(paste0(num_positive,\" positive tests / \",num_people,\" total = \",p_positive))\n\n14 positive tests / 1000 total = 0.014\n\n\n\nBo is one of those 14 people!"
  },
  {
    "objectID": "w03/index.html#section-6",
    "href": "w03/index.html#section-6",
    "title": "Week 3: Conditional Probability",
    "section": "",
    "text": "pos_ppl &lt;- ppl_df %&gt;% filter(test_result == 1)\ndisp(pos_ppl, obs_per_page = 10)\n\n\n\n\n\n\n\n\nBo doesn‚Äôt have it, and neither does anyone else who tested positive!\nBut, in the real world, we only observe \\(T\\)"
  },
  {
    "objectID": "w03/index.html#bayes-takeaway",
    "href": "w03/index.html#bayes-takeaway",
    "title": "Week 3: Conditional Probability",
    "section": "Bayes: Takeaway",
    "text": "Bayes: Takeaway\n\nBayesian approach allows new evidence to be weighed against existing evidence, with statistically principled way to derive these weights:\n\n\\[\n\\begin{array}{ccccc}\n\\Pr_{\\text{post}}(\\mathcal{H}) &\\hspace{-6mm}\\propto &\\hspace{-6mm} \\Pr(X \\mid \\mathcal{H}) &\\hspace{-6mm} \\times &\\hspace{-6mm} \\Pr_{\\text{pre}}(\\mathcal{H}) \\\\\n\\text{Posterior} &\\hspace{-6mm}\\propto &\\hspace{-6mm}\\text{Evidence} &\\hspace{-6mm} \\times &\\hspace{-6mm} \\text{Prior}\n\\end{array}\n\\]"
  },
  {
    "objectID": "w03/index.html#monte-carlo-methods-overview",
    "href": "w03/index.html#monte-carlo-methods-overview",
    "title": "Week 3: Conditional Probability",
    "section": "Monte Carlo Methods: Overview",
    "text": "Monte Carlo Methods: Overview\n\nYou already saw an example, in our rare disease simulation!\nGenerally, using computers (rather than math, ‚Äúby hand‚Äù) to estimate probabilistic quantities\n\n\n\nPros:\n\nMost real-world processes have no analytic solution\nStep-by-step breakdown of complex processes\n\n\nCons:\n\nCan require immense computing power\n‚ö†Ô∏è Can generate incorrect answers ‚ö†Ô∏è\n\n\n\n\nBy step-by-step I mean, a lot of the time you are just walking through, generating the next column using previously-generated columns. Like we did in the example above, generating test_result based on has_disease."
  },
  {
    "objectID": "w03/index.html#birthday-problem",
    "href": "w03/index.html#birthday-problem",
    "title": "Week 3: Conditional Probability",
    "section": "Birthday Problem",
    "text": "Birthday Problem\n\n\n\n30 people gather in a room together. What is the probability that two of them share the same birthday?\nAnalytic solution is fun, but requires some thought‚Ä¶ Monte Carlo it!\n\n\n\n\nCode\ngen_bday_room &lt;- function(room_num=NULL) {\n  num_people &lt;- 30\n  num_days &lt;- 366\n  ppl_df &lt;- tibble(id=seq(1,num_people))\nbirthdays &lt;- sample(1:num_days, num_people,replace = T)\n  ppl_df['birthday'] &lt;- birthdays\n  if (!is.null(room_num)) {\n    ppl_df &lt;- ppl_df %&gt;% mutate(room_num=room_num) %&gt;% relocate(room_num)\n  }\n  return(ppl_df)\n}\nppl_df &lt;- gen_bday_room(1)\ndisp(ppl_df %&gt;% head()) #, obs_per_page = 3)"
  },
  {
    "objectID": "w03/index.html#section-7",
    "href": "w03/index.html#section-7",
    "title": "Week 3: Conditional Probability",
    "section": "",
    "text": "# Inefficient version (return_num=FALSE) is for: if you want tibbles of *all* shared bdays for each room\nget_shared_bdays &lt;- function(df, is_grouped=NULL, return_num=FALSE, return_bool=FALSE) {\n  bday_pairs &lt;- tibble()\n  for (i in 1:(nrow(df)-1)) {\n    i_data &lt;- df[i,]\n    i_bday &lt;- i_data$birthday\n    for (j in (i+1):nrow(df)) {\n      j_data &lt;- df[j,]\n      j_bday &lt;- j_data$birthday\n      # Check if they're the same\n      same_bday &lt;- i_bday == j_bday\n      if (same_bday) {\n        if (return_bool) {\n          return(1)\n        }\n        pair_data &lt;- tibble(i=i,j=j,bday=i_bday)\n        if (!is.null(is_grouped)) {\n          i_room &lt;- i_data$room_num\n          pair_data['room'] &lt;- i_room\n        }\n        bday_pairs &lt;- bind_rows(bday_pairs, pair_data)\n      }\n    }\n  }\n  if (return_bool) {\n    return(0)\n  }\n  if (return_num) {\n    return(nrow(bday_pairs))\n  }\n  return(bday_pairs)\n}\n#get_shared_bdays(ppl_df)\nget_shared_bdays(ppl_df)\n\n# A tibble: 0 √ó 0"
  },
  {
    "objectID": "w03/index.html#section-8",
    "href": "w03/index.html#section-8",
    "title": "Week 3: Conditional Probability",
    "section": "",
    "text": "Let‚Äôs try more rooms‚Ä¶\n\n\n\n# Get tibbles for each room\nlibrary(purrr)\ngen_bday_rooms &lt;- function(num_rooms) {\n  rooms_df &lt;- tibble()\n  for (r in seq(1, num_rooms)) {\n      cur_room &lt;- gen_bday_room(r)\n      rooms_df &lt;- bind_rows(rooms_df, cur_room)\n  }\n  return(rooms_df)\n}\nnum_rooms &lt;- 10\nrooms_df &lt;- gen_bday_rooms(num_rooms)\nrooms_df %&gt;% group_by(room_num) %&gt;% group_map(~ get_shared_bdays(.x, is_grouped=TRUE))\n\nWarning: Unknown or uninitialised column: `room_num`.\nUnknown or uninitialised column: `room_num`.\nUnknown or uninitialised column: `room_num`.\nUnknown or uninitialised column: `room_num`.\nUnknown or uninitialised column: `room_num`.\nUnknown or uninitialised column: `room_num`.\nUnknown or uninitialised column: `room_num`.\nUnknown or uninitialised column: `room_num`.\nUnknown or uninitialised column: `room_num`.\nUnknown or uninitialised column: `room_num`.\nUnknown or uninitialised column: `room_num`.\nUnknown or uninitialised column: `room_num`.\nUnknown or uninitialised column: `room_num`.\n\n\n[[1]]\n# A tibble: 3 √ó 3\n      i     j  bday\n  &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1     3     6   116\n2     7    12   287\n3    19    30   267\n\n[[2]]\n# A tibble: 0 √ó 0\n\n[[3]]\n# A tibble: 1 √ó 3\n      i     j  bday\n  &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1     7    23   138\n\n[[4]]\n# A tibble: 1 √ó 3\n      i     j  bday\n  &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1     6    18    72\n\n[[5]]\n# A tibble: 2 √ó 3\n      i     j  bday\n  &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1     8    10   255\n2    16    30    66\n\n[[6]]\n# A tibble: 0 √ó 0\n\n[[7]]\n# A tibble: 1 √ó 3\n      i     j  bday\n  &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1    11    23   333\n\n[[8]]\n# A tibble: 1 √ó 3\n      i     j  bday\n  &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1     2    17   328\n\n[[9]]\n# A tibble: 2 √ó 3\n      i     j  bday\n  &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1     2     4   283\n2    13    21    28\n\n[[10]]\n# A tibble: 2 √ó 3\n      i     j  bday\n  &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1     8    23   135\n2    11    12   204\n\n\n\nNumber of shared birthdays per room:\n\n# Now just get the # shared bdays\nshared_per_room &lt;- rooms_df %&gt;%\n    group_by(room_num) %&gt;%\n    group_map(~ get_shared_bdays(.x, is_grouped = TRUE, return_num=TRUE))\n\nWarning: Unknown or uninitialised column: `room_num`.\nUnknown or uninitialised column: `room_num`.\nUnknown or uninitialised column: `room_num`.\nUnknown or uninitialised column: `room_num`.\nUnknown or uninitialised column: `room_num`.\nUnknown or uninitialised column: `room_num`.\nUnknown or uninitialised column: `room_num`.\nUnknown or uninitialised column: `room_num`.\nUnknown or uninitialised column: `room_num`.\nUnknown or uninitialised column: `room_num`.\nUnknown or uninitialised column: `room_num`.\nUnknown or uninitialised column: `room_num`.\nUnknown or uninitialised column: `room_num`.\n\nshared_per_room &lt;- unlist(shared_per_room)\nshared_per_room\n\n [1] 3 0 1 1 2 0 1 1 2 2\n\n\n\n\\(\\widehat{\\Pr}(\\text{shared})\\)\n\n\nsum(shared_per_room &gt; 0) / num_rooms\n\n[1] 0.8"
  },
  {
    "objectID": "w03/index.html#section-9",
    "href": "w03/index.html#section-9",
    "title": "Week 3: Conditional Probability",
    "section": "",
    "text": "How about A THOUSAND ROOMS?\n\n\nnum_rooms_many &lt;- 100\nmany_rooms_df &lt;- gen_bday_rooms(num_rooms_many)\nanyshared_per_room &lt;- many_rooms_df %&gt;%\n    group_by(room_num) %&gt;%\n    group_map(~ get_shared_bdays(.x, is_grouped = TRUE, return_bool = TRUE))\nanyshared_per_room &lt;- unlist(anyshared_per_room)\nanyshared_per_room\n\n  [1] 1 0 0 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 0 1 1\n [38] 1 0 1 0 1 1 1 0 0 1 0 0 0 1 1 1 0 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1\n [75] 1 0 1 0 1 0 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 1 0 1\n\n\n\n\\(\\widehat{\\Pr}(\\text{shared bday})\\)?\n\n\n# And now the probability estimate\nsum(anyshared_per_room &gt; 0) / num_rooms_many\n\n[1] 0.72\n\n\n\nThe analytic solution: \\(\\Pr(\\text{shared} \\mid k\\text{ people in room}) = 1 - \\frac{366!}{366^{k}(366-k)!}\\)\nIn our case: \\(1 - \\frac{366!}{366^{30}(366-30)!} = 1 - \\frac{366!}{366^{30}336!} = 1 - \\frac{\\prod_{i=337}^{366}i}{366^{30}}\\)\nR can juust barely handle these numbers:\n\n\n(exact_solution &lt;- 1 - (prod(seq(337,366))) / (366^30))\n\n[1] 0.7053034"
  },
  {
    "objectID": "w03/index.html#wrapping-up",
    "href": "w03/index.html#wrapping-up",
    "title": "Week 3: Conditional Probability",
    "section": "Wrapping Up",
    "text": "Wrapping Up\n\nlibrary(ggplot2)\noptions(ggplot2.discrete.colour = cbPalette)\nglobal_theme &lt;- ggplot2::theme_classic() + ggplot2::theme(\n    plot.title = element_text(hjust = 0.5, size = 18),\n    axis.title = element_text(size = 16),\n    axis.text = element_text(size = 14),\n    legend.title = element_text(size = 16, hjust = 0.5),\n    legend.text = element_text(size = 14),\n    legend.box.background = element_rect(colour = \"black\")\n)\nknitr::opts_chunk$set(fig.align = \"center\")\ng_pointsize &lt;- 6\n# Bday problem\ntrials_per_roomsize &lt;- 3\nbday_est_lbounds &lt;- c()\nbday_est_means &lt;- c()\nbday_est_ubounds &lt;- c()\nsample_sizes &lt;- c()\nfor (num_rooms_many in c(10,50,100,500, 1000)) {\n  cur_size_ests &lt;- c()\n  for (trial_num in seq(1,trials_per_roomsize)) {\n    many_rooms_df &lt;- gen_bday_rooms(num_rooms_many)\n    anyshared_per_room &lt;- many_rooms_df %&gt;%\n        group_by(room_num) %&gt;%\n        group_map(~ get_shared_bdays(.x, is_grouped = TRUE, return_bool = TRUE))\n    anyshared_per_room &lt;- unlist(anyshared_per_room)\n    cur_est &lt;- sum(anyshared_per_room &gt; 0) / num_rooms_many\n    cur_size_ests &lt;- c(cur_size_ests, cur_est)\n  }\n  bday_est_lbounds &lt;- c(bday_est_lbounds, min(cur_size_ests))\n  bday_est_ubounds &lt;- c(bday_est_ubounds, max(cur_size_ests))\n  bday_est_means &lt;- c(bday_est_means, mean(cur_size_ests))\n  sample_sizes &lt;- c(sample_sizes, num_rooms_many)\n}\nresult_df &lt;- tibble(n=sample_sizes,est=bday_est_means, lbound=bday_est_lbounds, ubound=bday_est_ubounds)\nbase_plot &lt;- ggplot(result_df, aes(x=n, y=est)) +\n  geom_point(aes(color=\"black\")) +\n  geom_line(color=\"black\") +\n  geom_ribbon(aes(ymin = lbound, ymax = ubound, fill = cbPalette[1]), alpha = 0.3) +\n      geom_hline(aes(yintercept = exact_solution, linetype = \"dashed\"), color = \"purple\") +\n      scale_color_manual(\"\", values = c(\"black\", \"purple\"), labels = c(\"Sample Mean X\", \"True Mean mu\")) +\n      scale_linetype_manual(\"\", values = \"dashed\", labels = \"True Mean mu\") +\n      scale_fill_manual(\"\", values = cbPalette[1], labels = \"95% CI\") +\n      global_theme +\n      theme(\n          legend.title = element_blank(),\n          legend.spacing.y = unit(0, \"mm\")\n      ) +\n      labs(\n          title = \"Monte Carlo Estimates of Birthday Problem Solution\",\n          x = \"n (Sample Size)\",\n          y = \"Estimate\"\n      )\nlog_plot &lt;- base_plot + scale_x_log10(breaks=c(10,100,1000,10000,100000), labels=c(\"10\",\"100\",\"1000\",\"10000\",\"100000\"))\nlog_plot"
  },
  {
    "objectID": "w03/index.html#final-note-functions-of-random-variables",
    "href": "w03/index.html#final-note-functions-of-random-variables",
    "title": "Week 3: Conditional Probability",
    "section": "Final Note: Functions of Random Variables",
    "text": "Final Note: Functions of Random Variables\n\n\\(X \\sim U[0,1], Y \\sim U[0,1]\\).\n\\(P(Y &lt; X^2)\\)?\nThe hard way: solve analytically\nThe easy way: simulate!"
  },
  {
    "objectID": "w03/index.html#lab-2-demonstrations",
    "href": "w03/index.html#lab-2-demonstrations",
    "title": "Week 3: Conditional Probability",
    "section": "Lab 2 Demonstrations",
    "text": "Lab 2 Demonstrations\nLab 2 Demonstrations"
  },
  {
    "objectID": "w03/index.html#lab-2-assignment-overview",
    "href": "w03/index.html#lab-2-assignment-overview",
    "title": "Week 3: Conditional Probability",
    "section": "Lab 2 Assignment Overview",
    "text": "Lab 2 Assignment Overview\nLab 2 Assignment"
  },
  {
    "objectID": "w03/slides.html#recap-1",
    "href": "w03/slides.html#recap-1",
    "title": "Week 3: Conditional Probability",
    "section": "Recap",
    "text": "Recap\n\nLogic \\(\\rightarrow\\) Set Theory \\(\\rightarrow\\) Probability Theory\nEntirety of probability theory can be derived from two axioms:\n\n\n\n\nThe Entirety of Probability Theory Follows From‚Ä¶\n\n\nAxiom 1 (Unitarity): \\(\\Pr(\\Omega) = 1\\) (The probability that something happens is 1)\nAxiom 2 (\\(\\sigma\\)-additivity): For mutually-exclusive events \\(E_1, E_2, \\ldots\\),\n\\[\n\\underbrace{\\Pr\\left(\\bigcup_{i=1}^{\\infty}E_i\\right)}_{\\Pr(E_1\\text{ occurs }\\vee E_2\\text{ occurs } \\vee \\cdots)} = \\underbrace{\\sum_{i=1}^{\\infty}\\Pr(E_i)}_{\\Pr(E_1\\text{ occurs}) + \\Pr(E_2\\text{ occurs}) + \\cdots}\n\\]\n\n\n\n\nBut what does ‚Äúmutually exclusive‚Äù mean‚Ä¶?"
  },
  {
    "objectID": "w03/slides.html#venn-diagrams-sets",
    "href": "w03/slides.html#venn-diagrams-sets",
    "title": "Week 3: Conditional Probability",
    "section": "Venn Diagrams: Sets",
    "text": "Venn Diagrams: Sets\n\n\n\n\n\n\n\n\n\\[\n\\begin{align*}\n&A = \\{0, 1, 2\\}, \\; B = \\{4, 5, 6\\} \\\\\n&\\implies A \\cap B = \\varnothing\n\\end{align*}\n\\]\nFigure¬†1: Mutually-exclusive (disjoint) sets\n\n\n\n\n\n\n\n\\[\n\\begin{align*}\n&A = \\{1, 2, 3\\}, \\; B = \\{3, 4, 5\\} \\\\\n&\\implies A \\cap B = \\{3\\}\n\\end{align*}\n\\]\nFigure¬†2: Non-mutually-exclusive sets"
  },
  {
    "objectID": "w03/slides.html#venn-diagrams-events-dice",
    "href": "w03/slides.html#venn-diagrams-events-dice",
    "title": "Week 3: Conditional Probability",
    "section": "Venn Diagrams: Events (Dice)",
    "text": "Venn Diagrams: Events (Dice)\n\\[\n\\begin{align*}\nA &= \\{\\text{Roll is even}\\} = \\{2, 4, 6\\} \\\\\nB &= \\{\\text{Roll is odd}\\} = \\{1, 3, 5\\} \\\\\nC &= \\{\\text{Roll is in Fibonnaci sequence}\\} = \\{1, 2, 3, 5\\}\n\\end{align*}\n\\]\n\n\n\n\n\n\n\n\n\n\nSet 1\nSet 2\nIntersection\nMutually Exclusive?\nCan Happen Simultaneously?\n\n\n\n\n\\(A\\)\n\\(B\\)\n\\(A \\cap B = \\varnothing\\)\nYes\nNo\n\n\n\\(A\\)\n\\(C\\)\n\\(A \\cap C = \\{2\\}\\)\nNo\nYes\n\n\n\\(B\\)\n\\(C\\)\n\\(B \\cap C = \\{1, 3, 5\\}\\)\nNo\nYes"
  },
  {
    "objectID": "w03/slides.html#rules-of-probability",
    "href": "w03/slides.html#rules-of-probability",
    "title": "Week 3: Conditional Probability",
    "section": "‚ÄúRules‚Äù of Probability",
    "text": "‚ÄúRules‚Äù of Probability\n\n(Remember: not ‚Äúrules‚Äù but ‚Äúfacts resulting from the logic \\(\\leftrightarrow\\) probability connection‚Äù)\n\n\n\n\n‚ÄúRules‚Äù of Probability\n\n\nFor logical predicates \\(p, q \\in \\{T, F\\}\\), events \\(P, Q\\) defined so \\(P\\) = event that \\(p\\) becomes true, \\(Q\\) = event that \\(q\\) becomes true,\n\nLogical AND = Probabilistic Multiplication\n\n\\[\n\\Pr(p \\wedge q) = \\Pr(P \\cap Q) = \\Pr(P) \\cdot \\Pr(Q)\n\\]\n\nLogical OR = Probabilistic Addition\n\n\\[\n\\Pr(p \\vee q) = \\Pr(P \\cup Q) = \\Pr(P) + \\Pr(Q) - \\underbrace{\\Pr(P \\cap Q)}_{\\text{(see rule 1)}}\n\\]\n\nLogical NOT = Probabilistic Complement\n\n\\[\n\\Pr(\\neg p) = \\Pr(P^c) = 1 - \\Pr(P)\n\\]"
  },
  {
    "objectID": "w03/slides.html#conditional-probability",
    "href": "w03/slides.html#conditional-probability",
    "title": "Week 3: Conditional Probability",
    "section": "Conditional Probability",
    "text": "Conditional Probability\n\nUsually if someone asks you probabilistic questions, like\n\n‚ÄúWhat is the likelihood that [our team] wins?‚Äù\n‚ÄúDo you think it will rain tomorrow?‚Äù and so on\n\nYou don‚Äôt guess a random number, you consider and incorporate evidence.\nExample: \\(\\Pr(\\text{rain})\\) on its own, without any other info? A tough question‚Ä¶ maybe \\(0.5\\)?\nIn reality, we would think about\n\n\\(\\Pr(\\text{rain} \\mid \\text{month of the year})\\)\n\\(\\Pr(\\text{rain} \\mid \\text{where we live})\\)\n\\(\\Pr(\\text{rain} \\mid \\text{did it rain yesterday?})\\)\n\nPsychologically, breaks down into two steps: (1) Think of a baseline probability, (2) Update baseline probability to incorporate relevant evidence (more on this in a bit‚Ä¶)\nAlso recall from last week: all probability is conditional probability, even if just conditioned on ‚Äúsomething happened‚Äù (\\(\\Omega\\), the thing defined so \\(\\Pr(\\Omega) = 1\\))"
  },
  {
    "objectID": "w03/slides.html#na√Øve-definition-2.0",
    "href": "w03/slides.html#na√Øve-definition-2.0",
    "title": "Week 3: Conditional Probability",
    "section": "Na√Øve Definition 2.0",
    "text": "Na√Øve Definition 2.0\n\n\n\n[Slightly Less] Na√Øve Definition of Probability\n\n\n\\[\n\\Pr(A \\mid B) = \\frac{\\text{\\# of Desired Outcomes in world where }B\\text{ happened}}{\\text{\\# Total outcomes in world where }B\\text{ happened}} = \\frac{|B \\cap A|}{|B|}\n\\]\n\n\n\n\n\n\n\n\n\n\n\nWorld Name\nWeather in World\nLikelihood of Rain Today\n\n\n\n\n\\(R\\)\nRained for the past 5 days\n\\(\\Pr(\\text{rain} \\mid R) &gt; 0.5\\)\n\n\n\\(M\\)\nMix of rain and non-rain over past 5 days\n\\(\\Pr(\\text{rain} \\mid M) \\approx 0.5\\)\n\n\n\\(S\\)\nSunny for the past 5 days\n\\(\\Pr(\\text{rain} \\mid S) &lt; 0.5\\)"
  },
  {
    "objectID": "w03/slides.html#law-of-total-probability",
    "href": "w03/slides.html#law-of-total-probability",
    "title": "Week 3: Conditional Probability",
    "section": "Law of Total Probability",
    "text": "Law of Total Probability\n\nSuppose the events \\(B_1, \\ldots, B_k\\) form a partition of the space \\(S\\) and \\(\\Pr(B_j) &gt; 0 \\forall j\\).\nThen, for every event \\(A\\) in \\(S\\),\n\n\\[\n\\Pr(A) = \\sum_{i=1}^k \\Pr(B_j)\\Pr(A \\mid B_j)\n\\]\n\nProbability of an event is the sum of its conditional probabilities across all conditions.\nIn other words: \\(A\\) is some event, \\(B_1, \\ldots, B_n\\) are mutually exclusive events filling entire sample-space, then\n\n\\[\n\\Pr(A) = \\Pr(A \\mid B_1)\\Pr(B_1) + \\Pr(A \\mid B_2)\\Pr(B_2) + \\cdots + \\Pr(A \\mid B_n)\\Pr(B_n)\n\\]\ni.e.¬†Compute the probability by summing over all possible cases."
  },
  {
    "objectID": "w03/slides.html#example",
    "href": "w03/slides.html#example",
    "title": "Week 3: Conditional Probability",
    "section": "Example",
    "text": "Example\n\nProbabilities of completing a job on time, with and without rain, are 0.42 and 0.90 respectively.\nProbability it will rain is 0.45. What is the probability the job will be completed on time?\n\\(A\\) = job will be completed on time, \\(B\\) = rain\n\n\\[\n\\Pr(B) = 0.45 \\implies \\Pr(B^c) = 1 - \\Pr(B) = 0.55.\n\\]\n\nNote: Events \\(B\\) and \\(B^c\\) are exclusive and form partitions of the sample space \\(S\\)\nWe know \\(\\Pr(A \\mid B) = 0.24\\), \\(\\Pr(A \\mid B^c) = 0.9\\).\nBy the Law of Total Probability, we have\n\n\\[\n\\begin{align*}\n\\Pr(A) &= \\Pr(B)\\Pr(A \\mid B) + \\Pr(B^c)\\Pr(A \\mid B^c) \\\\\n&= 0.45(0.42) + 0.55(0.9) = 0.189 + 0.495 = 0684.\n\\end{align*}\n\\]\nSo, the probability that the job will be completed on time is 0.684. (source)"
  },
  {
    "objectID": "w03/slides.html#deriving-bayes-theorem",
    "href": "w03/slides.html#deriving-bayes-theorem",
    "title": "Week 3: Conditional Probability",
    "section": "Deriving Bayes‚Äô Theorem",
    "text": "Deriving Bayes‚Äô Theorem\n\nLiterally just a re-writing of the conditional probability definition (don‚Äôt be scared)!\n\n\n\n\nFor two events \\(A\\) and \\(B\\), definition of conditional probability says that\n\n\\[\n\\begin{align*}\n\\Pr(A \\mid B) &= \\frac{\\Pr(A \\cap B)}{\\Pr(B)} \\tag{1} \\\\\n\\Pr(B \\mid A) &= \\frac{\\Pr(B \\cap A)}{\\Pr(A)} \\tag{2}\n\\end{align*}\n\\]\n\nMultiply to get rid of fractions\n\n\\[\n\\begin{align*}\n\\Pr(A \\mid B)\\Pr(B) &= \\Pr(A \\cap B) \\tag{1*} \\\\\n\\Pr(B \\mid A)\\Pr(A) &= \\Pr(B \\cap A) \\tag{2*}\n\\end{align*}\n\\]\n\n\nBut set intersection is associative (just like multiplication‚Ä¶), \\(A \\cap B = B \\cap A\\)! So, we know LHS of \\((\\text{1*})\\) = LHS of \\((\\text{2*})\\):\n\n\\[\n\\Pr(A \\mid B)\\Pr(B) = \\Pr(B \\mid A)\\Pr(A)\n\\]\n\nDivide both sides by \\(\\Pr(B)\\) to get a new definition of \\(\\Pr(A \\mid B)\\), Bayes‚Äô Theorem!\n\n\n\n\\[\n\\boxed{\\Pr(A \\mid B) = \\frac{\\Pr(B \\mid A)\\Pr(A)}{\\Pr(B)}}\n\\]\nFigure¬†3: Bayes‚Äô Theorem"
  },
  {
    "objectID": "w03/slides.html#why-is-this-helpful",
    "href": "w03/slides.html#why-is-this-helpful",
    "title": "Week 3: Conditional Probability",
    "section": "Why Is This Helpful?",
    "text": "Why Is This Helpful?\n\n\n\nBayes‚Äô Theorem\n\n\nFor any two events \\(A\\) and \\(B\\), \\[\n\\Pr(A \\mid B) = \\frac{\\Pr(B \\mid A)\\Pr(A)}{\\Pr(B)}\n\\]\n\n\n\n\nIn words (as exciting as I can make it, for now): Bayes‚Äô Theorem allows us to take information about \\(B \\mid A\\) and use it to infer information about \\(A \\mid B\\)\nIt isn‚Äôt until you work through some examples that this becomes mind-blowing, the most powerful equation we have for inferring unknowns from knowns‚Ä¶\nConsider \\(A = \\{\\text{person has disease}\\}\\), \\(B = \\{\\text{person tests positive for disease}\\}\\)\n\nIs \\(A\\) observable on its own? No, but‚Ä¶\n\nIs \\(B\\) observable on its own? Yes, and\nCan we infer information about \\(A\\) from knowing \\(B\\)? Also Yes, thanks to Bayes!\n\nTherefore, we can use \\(B\\) to infer information about \\(A\\), i.e., calculate \\(\\Pr(A \\mid B)\\)‚Ä¶"
  },
  {
    "objectID": "w03/slides.html#why-is-this-helpful-for-data-science",
    "href": "w03/slides.html#why-is-this-helpful-for-data-science",
    "title": "Week 3: Conditional Probability",
    "section": "Why Is This Helpful for Data Science?",
    "text": "Why Is This Helpful for Data Science?\n\nIt merges probability theory and hypothesis testing into a single framework:\n\n\\[\nP(\\text{hypothesis} \\mid \\text{data}) = \\frac{P(\\text{data} \\mid \\text{hypothesis})}{P(\\text{data})}\n\\]"
  },
  {
    "objectID": "w03/slides.html#probability-forwards-and-backwards",
    "href": "w03/slides.html#probability-forwards-and-backwards",
    "title": "Week 3: Conditional Probability",
    "section": "Probability Forwards and Backwards",
    "text": "Probability Forwards and Backwards\n\nTwo discrete RVs:\n\nWeather on a given day, \\(W \\in \\{\\textsf{Rain},\\textsf{Sun}\\}\\)\nAction that day, \\(A \\in \\{\\textsf{Go}, \\textsf{Stay}\\}\\): go to party or stay in and watch movie\n\nData-generating process: if \\(\\textsf{Sun}\\), rolls a die \\(R\\) and goes out unless \\(R = 6\\). If \\(\\textsf{Rain}\\), flips a coin and goes out if \\(\\textsf{H}\\).\nProbabilistic Graphical Model (PGM):"
  },
  {
    "objectID": "w03/slides.html#section",
    "href": "w03/slides.html#section",
    "title": "Week 3: Conditional Probability",
    "section": "",
    "text": "So, if we know \\(W = \\textsf{Sun}\\), what is \\(P(A = \\textsf{Go})\\)? \\[\n\\begin{align*}\nP(A = \\textsf{Go} \\mid W) &= 1 - P(R = 6) \\\\\n&= 1 - \\frac{1}{6} = \\frac{5}{6}\n\\end{align*}\n\\]\nConditional probability lets us go forwards (left to right):\n\n\n\n\n\n\n\nBut what if we want to perform inference going backwards?"
  },
  {
    "objectID": "w03/slides.html#section-1",
    "href": "w03/slides.html#section-1",
    "title": "Week 3: Conditional Probability",
    "section": "",
    "text": "If we see Ana at the party, we know \\(A = \\textsf{Go}\\)\nWhat does this tell us about the weather?\nIntuitively, we should increase our degree of belief that \\(W = \\textsf{Sun}\\). But, by how much?\nWe don‚Äôt know \\(P(W \\mid A)\\), only \\(P(A \\mid W)\\)‚Ä¶"
  },
  {
    "objectID": "w03/slides.html#section-2",
    "href": "w03/slides.html#section-2",
    "title": "Week 3: Conditional Probability",
    "section": "",
    "text": "\\[\nP(W = \\textsf{Sun} \\mid A = \\textsf{Go}) = \\frac{\\overbrace{P(A = \\textsf{Go} \\mid W = \\textsf{Sun})}^{5/6~ ‚úÖ}\\overbrace{P(W = \\textsf{Sun})}^{‚ùì}}{\\underbrace{P(A = \\textsf{Go})}_{‚ùì}}\n\\]\n\nWe‚Äôve seen \\(P(W = \\textsf{Sun})\\) before, it‚Äôs our prior: the probability without having any additional relevant knowledge. So, let‚Äôs say 50/50. \\(P(W = \\textsf{Sun}) = \\frac{1}{2}\\)\nIf we lived in Seattle, we could pick \\(P(W = \\textsf{Sun}) = \\frac{1}{4}\\)"
  },
  {
    "objectID": "w03/slides.html#section-3",
    "href": "w03/slides.html#section-3",
    "title": "Week 3: Conditional Probability",
    "section": "",
    "text": "\\[\nP(W = \\textsf{Sun} \\mid A = \\textsf{Go}) = \\frac{\\overbrace{P(A = \\textsf{Go} \\mid W = \\textsf{Sunny})}^{5/6~ ‚úÖ}\\overbrace{P(W = \\textsf{Sun})}^{1/2~ ‚úÖ}}{\\underbrace{P(A = \\textsf{Go})}_{‚ùì}}\n\\]\n\n\\(P(A = \\textsf{Go})\\) is trickier: the probability that Ana goes out regardless of what the weather is. But there are only two possible weather outcomes! So we just compute\n\n\\[\n\\begin{align*}\n&P(A = \\textsf{Go}) = \\sum_{\\omega \\in S(W)}P(A = \\textsf{Go}, \\omega) = \\sum_{\\omega \\in S(W)}P(A = \\textsf{Go} \\mid \\omega)P(\\omega) \\\\\n&= P(A = \\textsf{Go} \\mid W = \\textsf{Rain})P(W = \\textsf{Rain}) + P(A = \\textsf{Go} \\mid W = \\textsf{Sun})P(W = \\textsf{Sun}) \\\\\n&= \\left( \\frac{1}{2} \\right)\\left( \\frac{1}{2} \\right) + \\left( \\frac{5}{6} \\right)\\left( \\frac{1}{2} \\right) = \\frac{1}{4} + \\frac{5}{12} = \\frac{2}{3}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "w03/slides.html#putting-it-all-together",
    "href": "w03/slides.html#putting-it-all-together",
    "title": "Week 3: Conditional Probability",
    "section": "Putting it All Together",
    "text": "Putting it All Together\n\\[\n\\begin{align*}\nP(W = \\textsf{Sun} \\mid A = \\textsf{Go}) &= \\frac{\\overbrace{P(A = \\textsf{Go} \\mid W = \\textsf{Sunny})}^{3/4~ ‚úÖ}\\overbrace{P(W = \\textsf{Sun})}^{1/2~ ‚úÖ}}{\\underbrace{P(A = \\textsf{Go})}_{1/2~ ‚úÖ}} \\\\\n&= \\frac{\\left(\\frac{3}{4}\\right)\\left(\\frac{1}{2}\\right)}{\\frac{1}{2}} = \\frac{\\frac{3}{8}}{\\frac{1}{2}} = \\frac{3}{4}.\n\\end{align*}\n\\]\n\nGiven that we see Ana at the party, we should update our beliefs, so that \\(P(W = \\textsf{Sun}) = \\frac{3}{4}, P(W = \\textsf{Rain}) = \\frac{1}{4}\\)."
  },
  {
    "objectID": "w03/slides.html#a-scarier-example",
    "href": "w03/slides.html#a-scarier-example",
    "title": "Week 3: Conditional Probability",
    "section": "A Scarier Example",
    "text": "A Scarier Example\n\nBo worries he has a rare disease. He takes a test with 99% accuracy and tests positive. What‚Äôs the probability Bo has the disease? (Intuition: 99%? ‚Ä¶Let‚Äôs do the math!)\n\n\n\n\n\\(H \\in \\{\\textsf{sick}, \\textsf{healthy}\\}, T \\in \\{\\textsf{T}^+, \\textsf{T}^-\\}\\)\nThe test: 99% accurate. \\(P(T = \\textsf{T}^+ \\mid H = \\textsf{sick}) = 0.99\\), \\(P(T = \\textsf{T}^- \\mid H = \\textsf{healthy}) = 0.99\\).\nThe disease: 1 in 10K. \\(P(H = \\textsf{sick}) = \\frac{1}{10000}\\)\nWhat do we want to know? \\(P(H = \\textsf{sick} \\mid T = \\textsf{T}^+)\\)\nHow do we get there?\n\n\n\n\n\nThis photo, originally thought to be of Thomas Bayes, turns out to be probably someone else‚Ä¶ \\(P(\\textsf{Bayes})\\)?\n\n\n\n\n\n\\(H\\) for health, \\(T\\) for test result\nPhoto credit: https://thedatascientist.com/wp-content/uploads/2019/04/reverend-thomas-bayes.jpg"
  },
  {
    "objectID": "w03/slides.html#section-4",
    "href": "w03/slides.html#section-4",
    "title": "Week 3: Conditional Probability",
    "section": "",
    "text": "\\[\n\\begin{align*}\nP(H = \\textsf{sick} \\mid T = \\textsf{T}^+) &= \\frac{P(T = \\textsf{T}^+ \\mid H = \\textsf{sick})P(H = \\textsf{sick})}{P(T = \\textsf{T}^+)} \\\\\n&= \\frac{(0.99)\\left(\\frac{1}{10000}\\right)}{(0.99)\\left( \\frac{1}{10000} \\right) + (0.01)\\left( \\frac{9999}{10000} \\right)}\n\\end{align*}\n\\]\n\np_sick &lt;- 1 / 10000\np_healthy &lt;- 1 - p_sick\np_pos_given_sick &lt;- 0.99\np_neg_given_sick &lt;- 1 - p_pos_given_sick\np_neg_given_healthy &lt;- 0.99\np_pos_given_healthy &lt;- 1 - p_neg_given_healthy\nnumer &lt;- p_pos_given_sick * p_sick\ndenom1 &lt;- numer\ndenom2 &lt;- p_pos_given_healthy * p_healthy\nfinal_prob &lt;- numer / (denom1 + denom2)\nfinal_prob\n\n[1] 0.009803922\n\n\n\n‚Ä¶ Less than 1% üò±"
  },
  {
    "objectID": "w03/slides.html#proof-in-the-pudding",
    "href": "w03/slides.html#proof-in-the-pudding",
    "title": "Week 3: Conditional Probability",
    "section": "Proof in the Pudding",
    "text": "Proof in the Pudding\n\nlibrary(tibble)\nlibrary(dplyr)\n# Disease rarity\np_disease &lt;- 1 / 10000\n# 1K people\nnum_people &lt;- 1000\n# Give them ids\nppl_df &lt;- tibble(id=seq(1,num_people))\n# Whether they have the disease or not\nhas_disease &lt;- rbinom(num_people, 2, p_disease)\nppl_df &lt;- ppl_df %&gt;% mutate(has_disease=has_disease)\ndisp(ppl_df %&gt;% head(3))\n\n\n\n\n\n\n\n\nCode\nsum(ppl_df$has_disease) / num_people\n\n\n[1] 0.001\n\n\n\n(Foreshadowing Monte Carlo methods)"
  },
  {
    "objectID": "w03/slides.html#section-5",
    "href": "w03/slides.html#section-5",
    "title": "Week 3: Conditional Probability",
    "section": "",
    "text": "14 positive tests / 1000 total = 0.014\n\n\n\nBo is one of those 14 people!"
  },
  {
    "objectID": "w03/slides.html#section-6",
    "href": "w03/slides.html#section-6",
    "title": "Week 3: Conditional Probability",
    "section": "",
    "text": "Bo doesn‚Äôt have it, and neither does anyone else who tested positive!\nBut, in the real world, we only observe \\(T\\)"
  },
  {
    "objectID": "w03/slides.html#bayes-takeaway",
    "href": "w03/slides.html#bayes-takeaway",
    "title": "Week 3: Conditional Probability",
    "section": "Bayes: Takeaway",
    "text": "Bayes: Takeaway\n\nBayesian approach allows new evidence to be weighed against existing evidence, with statistically principled way to derive these weights:\n\n\\[\n\\begin{array}{ccccc}\n\\Pr_{\\text{post}}(\\mathcal{H}) &\\hspace{-6mm}\\propto &\\hspace{-6mm} \\Pr(X \\mid \\mathcal{H}) &\\hspace{-6mm} \\times &\\hspace{-6mm} \\Pr_{\\text{pre}}(\\mathcal{H}) \\\\\n\\text{Posterior} &\\hspace{-6mm}\\propto &\\hspace{-6mm}\\text{Evidence} &\\hspace{-6mm} \\times &\\hspace{-6mm} \\text{Prior}\n\\end{array}\n\\]"
  },
  {
    "objectID": "w03/slides.html#monte-carlo-methods-overview",
    "href": "w03/slides.html#monte-carlo-methods-overview",
    "title": "Week 3: Conditional Probability",
    "section": "Monte Carlo Methods: Overview",
    "text": "Monte Carlo Methods: Overview\n\nYou already saw an example, in our rare disease simulation!\nGenerally, using computers (rather than math, ‚Äúby hand‚Äù) to estimate probabilistic quantities\n\n\n\nPros:\n\nMost real-world processes have no analytic solution\nStep-by-step breakdown of complex processes\n\n\nCons:\n\nCan require immense computing power\n‚ö†Ô∏è Can generate incorrect answers ‚ö†Ô∏è\n\n\n\n\nBy step-by-step I mean, a lot of the time you are just walking through, generating the next column using previously-generated columns. Like we did in the example above, generating test_result based on has_disease."
  },
  {
    "objectID": "w03/slides.html#birthday-problem",
    "href": "w03/slides.html#birthday-problem",
    "title": "Week 3: Conditional Probability",
    "section": "Birthday Problem",
    "text": "Birthday Problem\n\n\n\n30 people gather in a room together. What is the probability that two of them share the same birthday?\nAnalytic solution is fun, but requires some thought‚Ä¶ Monte Carlo it!\n\n\n\n\nCode\ngen_bday_room &lt;- function(room_num=NULL) {\n  num_people &lt;- 30\n  num_days &lt;- 366\n  ppl_df &lt;- tibble(id=seq(1,num_people))\nbirthdays &lt;- sample(1:num_days, num_people,replace = T)\n  ppl_df['birthday'] &lt;- birthdays\n  if (!is.null(room_num)) {\n    ppl_df &lt;- ppl_df %&gt;% mutate(room_num=room_num) %&gt;% relocate(room_num)\n  }\n  return(ppl_df)\n}\nppl_df &lt;- gen_bday_room(1)\ndisp(ppl_df %&gt;% head()) #, obs_per_page = 3)"
  },
  {
    "objectID": "w03/slides.html#section-7",
    "href": "w03/slides.html#section-7",
    "title": "Week 3: Conditional Probability",
    "section": "",
    "text": "# A tibble: 0 √ó 0"
  },
  {
    "objectID": "w03/slides.html#section-8",
    "href": "w03/slides.html#section-8",
    "title": "Week 3: Conditional Probability",
    "section": "",
    "text": "Let‚Äôs try more rooms‚Ä¶\n\n\n\n# Get tibbles for each room\nlibrary(purrr)\ngen_bday_rooms &lt;- function(num_rooms) {\n  rooms_df &lt;- tibble()\n  for (r in seq(1, num_rooms)) {\n      cur_room &lt;- gen_bday_room(r)\n      rooms_df &lt;- bind_rows(rooms_df, cur_room)\n  }\n  return(rooms_df)\n}\nnum_rooms &lt;- 10\nrooms_df &lt;- gen_bday_rooms(num_rooms)\nrooms_df %&gt;% group_by(room_num) %&gt;% group_map(~ get_shared_bdays(.x, is_grouped=TRUE))\n\n[[1]]\n# A tibble: 3 √ó 3\n      i     j  bday\n  &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1     3     6   116\n2     7    12   287\n3    19    30   267\n\n[[2]]\n# A tibble: 0 √ó 0\n\n[[3]]\n# A tibble: 1 √ó 3\n      i     j  bday\n  &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1     7    23   138\n\n[[4]]\n# A tibble: 1 √ó 3\n      i     j  bday\n  &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1     6    18    72\n\n[[5]]\n# A tibble: 2 √ó 3\n      i     j  bday\n  &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1     8    10   255\n2    16    30    66\n\n[[6]]\n# A tibble: 0 √ó 0\n\n[[7]]\n# A tibble: 1 √ó 3\n      i     j  bday\n  &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1    11    23   333\n\n[[8]]\n# A tibble: 1 √ó 3\n      i     j  bday\n  &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1     2    17   328\n\n[[9]]\n# A tibble: 2 √ó 3\n      i     j  bday\n  &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1     2     4   283\n2    13    21    28\n\n[[10]]\n# A tibble: 2 √ó 3\n      i     j  bday\n  &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1     8    23   135\n2    11    12   204\n\n\n\nNumber of shared birthdays per room:\n\n# Now just get the # shared bdays\nshared_per_room &lt;- rooms_df %&gt;%\n    group_by(room_num) %&gt;%\n    group_map(~ get_shared_bdays(.x, is_grouped = TRUE, return_num=TRUE))\nshared_per_room &lt;- unlist(shared_per_room)\nshared_per_room\n\n [1] 3 0 1 1 2 0 1 1 2 2\n\n\n\n\\(\\widehat{\\Pr}(\\text{shared})\\)\n\n\n\n[1] 0.8"
  },
  {
    "objectID": "w03/slides.html#section-9",
    "href": "w03/slides.html#section-9",
    "title": "Week 3: Conditional Probability",
    "section": "",
    "text": "How about A THOUSAND ROOMS?\n\n\n\n  [1] 1 0 0 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 0 1 1\n [38] 1 0 1 0 1 1 1 0 0 1 0 0 0 1 1 1 0 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1\n [75] 1 0 1 0 1 0 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 1 0 1\n\n\n\n\\(\\widehat{\\Pr}(\\text{shared bday})\\)?\n\n\n\n[1] 0.72\n\n\n\nThe analytic solution: \\(\\Pr(\\text{shared} \\mid k\\text{ people in room}) = 1 - \\frac{366!}{366^{k}(366-k)!}\\)\nIn our case: \\(1 - \\frac{366!}{366^{30}(366-30)!} = 1 - \\frac{366!}{366^{30}336!} = 1 - \\frac{\\prod_{i=337}^{366}i}{366^{30}}\\)\nR can juust barely handle these numbers:\n\n\n\n[1] 0.7053034"
  },
  {
    "objectID": "w03/slides.html#wrapping-up",
    "href": "w03/slides.html#wrapping-up",
    "title": "Week 3: Conditional Probability",
    "section": "Wrapping Up",
    "text": "Wrapping Up"
  },
  {
    "objectID": "w03/slides.html#final-note-functions-of-random-variables",
    "href": "w03/slides.html#final-note-functions-of-random-variables",
    "title": "Week 3: Conditional Probability",
    "section": "Final Note: Functions of Random Variables",
    "text": "Final Note: Functions of Random Variables\n\n\\(X \\sim U[0,1], Y \\sim U[0,1]\\).\n\\(P(Y &lt; X^2)\\)?\nThe hard way: solve analytically\nThe easy way: simulate!"
  },
  {
    "objectID": "w03/slides.html#lab-2-demonstrations",
    "href": "w03/slides.html#lab-2-demonstrations",
    "title": "Week 3: Conditional Probability",
    "section": "Lab 2 Demonstrations",
    "text": "Lab 2 Demonstrations\nLab 2 Demonstrations"
  },
  {
    "objectID": "w03/slides.html#lab-2-assignment-overview",
    "href": "w03/slides.html#lab-2-assignment-overview",
    "title": "Week 3: Conditional Probability",
    "section": "Lab 2 Assignment Overview",
    "text": "Lab 2 Assignment Overview\nLab 2 Assignment"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DSAN 5100, Section 03 (Thursdays)",
    "section": "",
    "text": "This is a ‚Äúhub‚Äù collecting relevant links for each week, for students in Prof.¬†Jeff‚Äôs Thursday section (Section 03) of DSAN 5100: Probabilistic Modeling and Statistical Computing, Fall 2023 at Georgetown University. Sections take place in Car Barn room 201 on Thursdays from 12:30pm to 3:30pm.\nThis page is not a replacement for the Main Course Page or the course‚Äôs Canvas Page, which are shared across all sections!\nUse the menu on the left, or the table below, to view the resources for a specific week.\n\n\n\n\n\n\n\n\n\n\nTitle\n\n\nDate\n\n\n\n\n\n\nWeek 1: Welcome to DSAN 5100!\n\n\nThursday, August 24, 2023\n\n\n\n\nWeek 2: Introduction to Probabilistic Modeling\n\n\nWednesday, September 6, 2023\n\n\n\n\nWeek 3: Conditional Probability\n\n\nThursday, September 7, 2023\n\n\n\n\n\n\nNo matching items"
  }
]