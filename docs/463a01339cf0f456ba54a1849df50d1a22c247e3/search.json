[
  {
    "objectID": "cheatsheet-math.html",
    "href": "cheatsheet-math.html",
    "title": "DSAN 5100 Math Cheatsheet",
    "section": "",
    "text": "Proposition \\(p\\): A true-false statement like ‚Äú1 + 1 = 2‚Äù or ‚Äú\\(x\\) is greater than 5‚Äù. The latter would be written as \\(p(x)\\), since its true/false value depends on a given value of \\(x\\).\n\\(\\wedge\\): Logical ‚Äúand‚Äù. \\(p \\wedge q\\) is true only if both \\(p\\) and \\(q\\) are true.\n\\(\\vee\\): Logical ‚Äúor‚Äù. \\(p \\vee q\\) is true if \\(p\\) is true or \\(q\\) is true, or both are true.\n\\(\\neg\\): Logical negation: If \\(p\\) is true, \\(\\neg p\\) is false. If \\(p\\) is false, \\(\\neg p\\) is true.\nDeMorgan‚Äôs Laws: Logical identities which illustrate how the negation operator gets ‚Äúdistributed‚Äù in a logical statement, allowing conversion of ‚Äúor‚Äù statements into ‚Äúand‚Äù statements, and vice-versa: \\(\\neg(p \\wedge q) = \\neg(\\neg p \\vee \\neg q)\\)\n\\(\\implies\\): ‚ÄúImplies‚Äù. \\(a \\implies b\\) is true if, whenever \\(a\\) is true, \\(b\\) is also true.\n\\(\\iff\\): ‚ÄúIf and only if‚Äù. \\(a \\iff b\\) is true if, \\(a\\) is only true when \\(b\\) is true, and \\(a\\) is only false when \\(b\\) is false.\n\\(\\exists\\): ‚ÄúThere exists‚Äù. In the course this will be written as \\(\\exists x \\in S [p(x)]\\), which means that there exists some element \\(x\\) in the set \\(S\\) such that \\(p(x)\\) is true. Also called the ‚Äúexistential quantifier‚Äù.\n\\(\\forall\\): ‚ÄúFor all‚Äù. In the course this will be written as \\(\\forall x \\in S [p(x)]\\), which means that for every element in a set \\(S\\) (with \\(x\\) representing some element arbitrarily taken from \\(S\\)), the proposition \\(p(x)\\) is true. Also called the ‚Äúuniversal quantifier‚Äù.\n\nNote that the universal and existential quantifiers are closely related by a negation law (just like \\(\\wedge\\) and \\(\\vee\\) and their connection via DeMorgan‚Äôs Laws): \\(\\neg \\left( \\forall x \\in S[p(x)] \\right) \\iff \\exists x \\in S [\\neg p(x)]\\). The negation on the outside of the quantified proposition has moved inside of it, with the quantifier flipped.\nThe same holds true in reverse: \\(\\neg \\left( \\exists x \\in S [p(x)] \\right) \\iff \\forall x \\in S [\\neg p(x)]\\)\n\n\n\n\n\n\nA set \\(S\\) (denoted by a capital letter when possible) is a collection of elements (denoted by a lowercase letter when possible).\n\nFor example, if \\(S = \\{0,1,2,3\\}\\), then \\(0\\) is an element of \\(S\\).\n\n\\(a \\in S\\): The proposition that \\(a\\) is an element of the set \\(S\\).\n\nIf \\(S = \\{0,1,2,3\\}\\), then \\(2 \\in S\\) but \\(5 \\notin S\\).\n\n\\(A \\subseteq S\\): The proposition that \\(A\\) is a subset of the set \\(S\\).\n\nIf \\(S = \\{0, 1, 2, 3\\}\\), then \\(\\{1,3\\} \\subseteq S\\) but \\(\\{1,4\\} \\not\\subseteq S\\). Note that sets are defined to be subsets of themselves, so that \\(S \\subseteq S\\).\n\n\\(A \\subset S\\): The proposition that \\(A\\) is a proper subset of \\(S\\), meaning that \\(A \\subseteq S\\) but \\(A \\neq S\\).\n\nWhile sets are subsets of themselves, sets are not proper subsets of themselves, so that if \\(S = \\{0, 1, 2, 3\\}\\), then \\(\\{1,2,3\\} \\subset S\\) but \\(\\{0, 1, 2, 3\\} \\not\\subset S\\).\n\n\\(|S|\\): The cardinality of, or number of elements in, a set \\(S\\).\n\nIf \\(S = \\{1, 2, 3\\}\\), \\(|S| = 3\\).\nIf the set \\(S\\) has infinite cardinality, we can distinguish between two cases:\n\nIf elements of \\(S\\) can be put into one-to-one correspondence with the natural numbers \\(\\mathbb{N}\\), we say that \\(S\\) is countably infinite and has cardinality \\(\\aleph_0\\) (pronounced ‚Äúaleph-null‚Äù): \\(|S| = \\aleph_0\\).\nIf elements of \\(S\\) cannot be put into one-to-one correspondence with the natural numbers \\(\\mathbb{N}\\), we say that \\(S\\) is uncountably infinite and has cardinality greater than \\(\\aleph_0\\): \\(|S| &gt; \\aleph_0\\).\n\n\n\\(\\mathcal{P}(S)\\): The power set of a set \\(S\\), which is the set of all possible subsets of \\(S\\).\n\nFor example, if \\(S = \\{1, 2, 3\\}\\), \\(\\mathcal{P}(S) = \\{\\varnothing, \\{1\\}, \\{2\\}, \\{3\\}, \\{1,2\\}, \\{1,3\\}, \\{2,3\\}, \\{1,2,3\\}\\}\\). Notice that \\(|\\mathcal{P}(S)| = 2^{|S|}\\), which is always true of the power set.\n\n\n\n\n\n\n\\(\\mathbb{N}\\): The set of all natural numbers, sometimes called the ‚Äúcounting numbers‚Äù: \\(\\{0, 1, 2, 3, \\ldots \\}\\) (This set is countably infinite).\n\\(\\mathbb{Z}\\): The set of all integers, which includes all of the natural numbers along with their negatives: \\(\\{\\ldots, -2, -1, 0, 1, 2, \\ldots\\}\\) (This set is also countably infinite)\n\\(\\mathbb{Q}\\): The set of all rational numbers, i.e., well-defined ratios of two integers. \\(x \\in \\mathbb{Q} \\iff x = \\frac{p}{q}\\) for two integers \\(p\\) and \\(q\\), and \\(q \\neq 0\\). (This set is, surprisingly, also countably infinite)\n\\(\\mathbb{R}\\): The set of all real numbers, which includes all integers as well as numbers such as \\(\\pi\\), \\(2.356\\), or \\(\\sqrt{2}\\) (This set is uncountably infinite).\nScalar: A single number from some set of numbers, like \\(-2.1 \\in \\mathbb{R}\\)\nVector: A \\(d\\)-dimensional vector \\(\\mathbf{v}\\) is a collection of \\(d\\) scalars in \\(\\mathbb{R}\\), \\(\\mathbf{v} = (v_1, v_2, \\ldots, v_d)^\\top\\), which we can interpret as an arrow pointing \\(v_i\\) units in each dimension \\(i\\). For example, if \\(\\mathbf{v} = (3,5)\\), we can interpret \\(\\mathbf{v}\\) as representing an arrow pointing \\(3\\) units in the \\(x\\) direction and \\(5\\) units in the \\(y\\) direction.\n\nAs indicated above, however, we will assume that vectors are column vectors unless otherwise specified, though they will be written row-wise with a transpose symbol at the end. This means that, although it will be written like \\(\\mathbf{v} = (v_1, v_2, \\ldots, v_n)^\\top\\), you should apply the transpose in your head: \\[\n\\mathbf{v} = (v_1, v_2, \\ldots, v_n)^\\top = \\begin{bmatrix}v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n\\end{bmatrix}\n\\]\n\nMatrix: An \\(m \\times n\\) matrix \\(\\mathbf{M}_{[m \\times n]}\\) is an \\(m\\)-by-\\(n\\) grid of scalars, where the scalar in the \\(i\\)th row and \\(j\\)th column is denoted \\(m_{i,j}\\). In the class, we will be careful to add a subscript like \\(\\mathbf{M}_{[m \\times n]}\\) to indicate the number of rows (\\(m\\)) and columns (\\(n\\)) in the matrix, since operations like multiplication are only defined for matrices with particular dimensions.\nMatrix multiplication: For two matrices \\(\\mathbf{X}_{[a \\times b]}\\) and \\(\\mathbf{Y}_{[c \\times d]}\\), matrix multiplication is defined if \\(b = c\\), and produces the following \\(a \\times d\\) matrix \\(\\mathbf{XY}_{[a \\times d]}\\):\n\\[\n  \\mathbf{XY}_{[a \\times d]} = \\begin{bmatrix}\n  x_{1,1}y_{1,1} & \\cdots & x_{m,1}y_{1,n} \\\\\n  \\vdots & \\ddots & \\vdots \\\\\n  x_{1,n}y_{m,1} & \\cdots & x_{m,n}y_{m,n}\n  \\end{bmatrix}\n  \\]\n\\(\\sum_{i=1}^n f(i)\\): \\(\\Sigma\\) is the capitalized Greek letter ‚ÄúSigma‚Äù, and stands for ‚ÄúSum‚Äù in this case. This notation means: ‚Äúthe sum of \\(f(i)\\) from \\(i = 1\\) to \\(i = n\\)‚Äù. For example, \\(\\sum_{i=1}^3 i^2 = 1^2 + 2^2 + 3^2 = 14\\).\n\\(\\prod_{i=1}^n f(i)\\): \\(\\Pi\\) is the capitalized Greek letter ‚ÄúPi‚Äù, and stands for ‚ÄúProduct‚Äù in this case. This notation means: ‚Äúthe product of \\(f(i)\\) from \\(i = 1\\) to \\(i = n\\)‚Äù. For example, \\(\\prod_{i=1}^3 i^2 = 1^2 \\cdot 2^2 \\cdot 3^2 = 36\\).\n\n\n\n\n\n\\([a,b] \\in \\mathbb{R}\\): The closed interval between \\(a\\) and \\(b\\). That is, the subset of \\(\\mathbb{R}\\) containing all numbers between \\(a\\) and \\(b\\), including \\(a\\) and \\(b\\) themselves.\n\\((a,b) \\in \\mathbb{R}\\): The open interval between \\(a\\) and \\(b\\). That is, the subset of \\(\\mathbb{R}\\) containing all numbers between \\(a\\) and \\(b\\), excluding \\(a\\) and \\(b\\) themselves.\n\\([a, b)\\), \\((a, b]\\): The half-open interval between \\(a\\) and \\(b\\). In the first case, we include \\(a\\) but exclude \\(b\\), while in the second we exclude \\(a\\) but include \\(b\\).\n\\(\\int_{a}^b f(x)dx\\): The integral of the function \\(f(x)\\) between points \\(a\\) and \\(b\\). In this course, you just need to remember that this produces the area under the curve of \\(f(x)\\) between these points.\n\\(\\frac{d}{dx} \\left[ f(x) \\right]\\): The derivative of the function \\(f(x)\\). For this class, you just need to remember that the derivative is what transforms a Cumulative Density Function (CDF) into a Probability Density Function (PDF): if \\(F_X(v)\\) is the CDF of a random variable \\(X\\), then \\(\\frac{d}{dx}\\left[ F_X(v) \\right] = f_X(v)\\), the PDF of \\(X\\).\n\nFor functions of multiple variables, like the joint pdf \\(f_{X,Y}(v_X, v_Y)\\), we use the \\(\\partial\\) symbol instead of \\(d\\) to denote partial derivatives: for example, the change in \\(F_{X,Y}(v_X, v_Y)\\) as \\(v_X\\) changes is denoted \\(\\frac{\\partial}{\\partial v_X}\\left[ F_{X,Y}(v_X, v_Y) \\right]\\), while the change in \\(F_{X,Y}(v_X, v_Y)\\) as \\(v_Y\\) changes is denoted \\(\\frac{\\partial}{\\partial v_Y}\\left[ F_{X,Y}(v_X, v_Y) \\right]\\)."
  },
  {
    "objectID": "cheatsheet-math.html#math-preliminaries",
    "href": "cheatsheet-math.html#math-preliminaries",
    "title": "DSAN 5100 Math Cheatsheet",
    "section": "",
    "text": "Proposition \\(p\\): A true-false statement like ‚Äú1 + 1 = 2‚Äù or ‚Äú\\(x\\) is greater than 5‚Äù. The latter would be written as \\(p(x)\\), since its true/false value depends on a given value of \\(x\\).\n\\(\\wedge\\): Logical ‚Äúand‚Äù. \\(p \\wedge q\\) is true only if both \\(p\\) and \\(q\\) are true.\n\\(\\vee\\): Logical ‚Äúor‚Äù. \\(p \\vee q\\) is true if \\(p\\) is true or \\(q\\) is true, or both are true.\n\\(\\neg\\): Logical negation: If \\(p\\) is true, \\(\\neg p\\) is false. If \\(p\\) is false, \\(\\neg p\\) is true.\nDeMorgan‚Äôs Laws: Logical identities which illustrate how the negation operator gets ‚Äúdistributed‚Äù in a logical statement, allowing conversion of ‚Äúor‚Äù statements into ‚Äúand‚Äù statements, and vice-versa: \\(\\neg(p \\wedge q) = \\neg(\\neg p \\vee \\neg q)\\)\n\\(\\implies\\): ‚ÄúImplies‚Äù. \\(a \\implies b\\) is true if, whenever \\(a\\) is true, \\(b\\) is also true.\n\\(\\iff\\): ‚ÄúIf and only if‚Äù. \\(a \\iff b\\) is true if, \\(a\\) is only true when \\(b\\) is true, and \\(a\\) is only false when \\(b\\) is false.\n\\(\\exists\\): ‚ÄúThere exists‚Äù. In the course this will be written as \\(\\exists x \\in S [p(x)]\\), which means that there exists some element \\(x\\) in the set \\(S\\) such that \\(p(x)\\) is true. Also called the ‚Äúexistential quantifier‚Äù.\n\\(\\forall\\): ‚ÄúFor all‚Äù. In the course this will be written as \\(\\forall x \\in S [p(x)]\\), which means that for every element in a set \\(S\\) (with \\(x\\) representing some element arbitrarily taken from \\(S\\)), the proposition \\(p(x)\\) is true. Also called the ‚Äúuniversal quantifier‚Äù.\n\nNote that the universal and existential quantifiers are closely related by a negation law (just like \\(\\wedge\\) and \\(\\vee\\) and their connection via DeMorgan‚Äôs Laws): \\(\\neg \\left( \\forall x \\in S[p(x)] \\right) \\iff \\exists x \\in S [\\neg p(x)]\\). The negation on the outside of the quantified proposition has moved inside of it, with the quantifier flipped.\nThe same holds true in reverse: \\(\\neg \\left( \\exists x \\in S [p(x)] \\right) \\iff \\forall x \\in S [\\neg p(x)]\\)\n\n\n\n\n\n\nA set \\(S\\) (denoted by a capital letter when possible) is a collection of elements (denoted by a lowercase letter when possible).\n\nFor example, if \\(S = \\{0,1,2,3\\}\\), then \\(0\\) is an element of \\(S\\).\n\n\\(a \\in S\\): The proposition that \\(a\\) is an element of the set \\(S\\).\n\nIf \\(S = \\{0,1,2,3\\}\\), then \\(2 \\in S\\) but \\(5 \\notin S\\).\n\n\\(A \\subseteq S\\): The proposition that \\(A\\) is a subset of the set \\(S\\).\n\nIf \\(S = \\{0, 1, 2, 3\\}\\), then \\(\\{1,3\\} \\subseteq S\\) but \\(\\{1,4\\} \\not\\subseteq S\\). Note that sets are defined to be subsets of themselves, so that \\(S \\subseteq S\\).\n\n\\(A \\subset S\\): The proposition that \\(A\\) is a proper subset of \\(S\\), meaning that \\(A \\subseteq S\\) but \\(A \\neq S\\).\n\nWhile sets are subsets of themselves, sets are not proper subsets of themselves, so that if \\(S = \\{0, 1, 2, 3\\}\\), then \\(\\{1,2,3\\} \\subset S\\) but \\(\\{0, 1, 2, 3\\} \\not\\subset S\\).\n\n\\(|S|\\): The cardinality of, or number of elements in, a set \\(S\\).\n\nIf \\(S = \\{1, 2, 3\\}\\), \\(|S| = 3\\).\nIf the set \\(S\\) has infinite cardinality, we can distinguish between two cases:\n\nIf elements of \\(S\\) can be put into one-to-one correspondence with the natural numbers \\(\\mathbb{N}\\), we say that \\(S\\) is countably infinite and has cardinality \\(\\aleph_0\\) (pronounced ‚Äúaleph-null‚Äù): \\(|S| = \\aleph_0\\).\nIf elements of \\(S\\) cannot be put into one-to-one correspondence with the natural numbers \\(\\mathbb{N}\\), we say that \\(S\\) is uncountably infinite and has cardinality greater than \\(\\aleph_0\\): \\(|S| &gt; \\aleph_0\\).\n\n\n\\(\\mathcal{P}(S)\\): The power set of a set \\(S\\), which is the set of all possible subsets of \\(S\\).\n\nFor example, if \\(S = \\{1, 2, 3\\}\\), \\(\\mathcal{P}(S) = \\{\\varnothing, \\{1\\}, \\{2\\}, \\{3\\}, \\{1,2\\}, \\{1,3\\}, \\{2,3\\}, \\{1,2,3\\}\\}\\). Notice that \\(|\\mathcal{P}(S)| = 2^{|S|}\\), which is always true of the power set.\n\n\n\n\n\n\n\\(\\mathbb{N}\\): The set of all natural numbers, sometimes called the ‚Äúcounting numbers‚Äù: \\(\\{0, 1, 2, 3, \\ldots \\}\\) (This set is countably infinite).\n\\(\\mathbb{Z}\\): The set of all integers, which includes all of the natural numbers along with their negatives: \\(\\{\\ldots, -2, -1, 0, 1, 2, \\ldots\\}\\) (This set is also countably infinite)\n\\(\\mathbb{Q}\\): The set of all rational numbers, i.e., well-defined ratios of two integers. \\(x \\in \\mathbb{Q} \\iff x = \\frac{p}{q}\\) for two integers \\(p\\) and \\(q\\), and \\(q \\neq 0\\). (This set is, surprisingly, also countably infinite)\n\\(\\mathbb{R}\\): The set of all real numbers, which includes all integers as well as numbers such as \\(\\pi\\), \\(2.356\\), or \\(\\sqrt{2}\\) (This set is uncountably infinite).\nScalar: A single number from some set of numbers, like \\(-2.1 \\in \\mathbb{R}\\)\nVector: A \\(d\\)-dimensional vector \\(\\mathbf{v}\\) is a collection of \\(d\\) scalars in \\(\\mathbb{R}\\), \\(\\mathbf{v} = (v_1, v_2, \\ldots, v_d)^\\top\\), which we can interpret as an arrow pointing \\(v_i\\) units in each dimension \\(i\\). For example, if \\(\\mathbf{v} = (3,5)\\), we can interpret \\(\\mathbf{v}\\) as representing an arrow pointing \\(3\\) units in the \\(x\\) direction and \\(5\\) units in the \\(y\\) direction.\n\nAs indicated above, however, we will assume that vectors are column vectors unless otherwise specified, though they will be written row-wise with a transpose symbol at the end. This means that, although it will be written like \\(\\mathbf{v} = (v_1, v_2, \\ldots, v_n)^\\top\\), you should apply the transpose in your head: \\[\n\\mathbf{v} = (v_1, v_2, \\ldots, v_n)^\\top = \\begin{bmatrix}v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n\\end{bmatrix}\n\\]\n\nMatrix: An \\(m \\times n\\) matrix \\(\\mathbf{M}_{[m \\times n]}\\) is an \\(m\\)-by-\\(n\\) grid of scalars, where the scalar in the \\(i\\)th row and \\(j\\)th column is denoted \\(m_{i,j}\\). In the class, we will be careful to add a subscript like \\(\\mathbf{M}_{[m \\times n]}\\) to indicate the number of rows (\\(m\\)) and columns (\\(n\\)) in the matrix, since operations like multiplication are only defined for matrices with particular dimensions.\nMatrix multiplication: For two matrices \\(\\mathbf{X}_{[a \\times b]}\\) and \\(\\mathbf{Y}_{[c \\times d]}\\), matrix multiplication is defined if \\(b = c\\), and produces the following \\(a \\times d\\) matrix \\(\\mathbf{XY}_{[a \\times d]}\\):\n\\[\n  \\mathbf{XY}_{[a \\times d]} = \\begin{bmatrix}\n  x_{1,1}y_{1,1} & \\cdots & x_{m,1}y_{1,n} \\\\\n  \\vdots & \\ddots & \\vdots \\\\\n  x_{1,n}y_{m,1} & \\cdots & x_{m,n}y_{m,n}\n  \\end{bmatrix}\n  \\]\n\\(\\sum_{i=1}^n f(i)\\): \\(\\Sigma\\) is the capitalized Greek letter ‚ÄúSigma‚Äù, and stands for ‚ÄúSum‚Äù in this case. This notation means: ‚Äúthe sum of \\(f(i)\\) from \\(i = 1\\) to \\(i = n\\)‚Äù. For example, \\(\\sum_{i=1}^3 i^2 = 1^2 + 2^2 + 3^2 = 14\\).\n\\(\\prod_{i=1}^n f(i)\\): \\(\\Pi\\) is the capitalized Greek letter ‚ÄúPi‚Äù, and stands for ‚ÄúProduct‚Äù in this case. This notation means: ‚Äúthe product of \\(f(i)\\) from \\(i = 1\\) to \\(i = n\\)‚Äù. For example, \\(\\prod_{i=1}^3 i^2 = 1^2 \\cdot 2^2 \\cdot 3^2 = 36\\).\n\n\n\n\n\n\\([a,b] \\in \\mathbb{R}\\): The closed interval between \\(a\\) and \\(b\\). That is, the subset of \\(\\mathbb{R}\\) containing all numbers between \\(a\\) and \\(b\\), including \\(a\\) and \\(b\\) themselves.\n\\((a,b) \\in \\mathbb{R}\\): The open interval between \\(a\\) and \\(b\\). That is, the subset of \\(\\mathbb{R}\\) containing all numbers between \\(a\\) and \\(b\\), excluding \\(a\\) and \\(b\\) themselves.\n\\([a, b)\\), \\((a, b]\\): The half-open interval between \\(a\\) and \\(b\\). In the first case, we include \\(a\\) but exclude \\(b\\), while in the second we exclude \\(a\\) but include \\(b\\).\n\\(\\int_{a}^b f(x)dx\\): The integral of the function \\(f(x)\\) between points \\(a\\) and \\(b\\). In this course, you just need to remember that this produces the area under the curve of \\(f(x)\\) between these points.\n\\(\\frac{d}{dx} \\left[ f(x) \\right]\\): The derivative of the function \\(f(x)\\). For this class, you just need to remember that the derivative is what transforms a Cumulative Density Function (CDF) into a Probability Density Function (PDF): if \\(F_X(v)\\) is the CDF of a random variable \\(X\\), then \\(\\frac{d}{dx}\\left[ F_X(v) \\right] = f_X(v)\\), the PDF of \\(X\\).\n\nFor functions of multiple variables, like the joint pdf \\(f_{X,Y}(v_X, v_Y)\\), we use the \\(\\partial\\) symbol instead of \\(d\\) to denote partial derivatives: for example, the change in \\(F_{X,Y}(v_X, v_Y)\\) as \\(v_X\\) changes is denoted \\(\\frac{\\partial}{\\partial v_X}\\left[ F_{X,Y}(v_X, v_Y) \\right]\\), while the change in \\(F_{X,Y}(v_X, v_Y)\\) as \\(v_Y\\) changes is denoted \\(\\frac{\\partial}{\\partial v_Y}\\left[ F_{X,Y}(v_X, v_Y) \\right]\\)."
  },
  {
    "objectID": "cheatsheet-math.html#week-03-conditional-probability",
    "href": "cheatsheet-math.html#week-03-conditional-probability",
    "title": "DSAN 5100 Math Cheatsheet",
    "section": "Week 03: Conditional Probability",
    "text": "Week 03: Conditional Probability\n\nProbability Fundamentals\n\n\\(\\Omega\\): The set of all possible outcomes in a probability setting\n\\(\\mathcal{P}(\\Omega) = \\{ e \\mid e \\subseteq \\Omega\\}\\): The set of all subsets of \\(\\Omega\\), each of which is an event\n\n\n\nRandom Variables\n\n\\(X\\) is a random variable if it maps each element of \\(\\Omega\\) to a real number \\(o \\in \\mathbb{R}\\).\n\nFor example, if we‚Äôre rolling a die, we can create a random variable \\(X\\) which maps \\(\\text{roll a one}\\) to \\(1\\), \\(\\text{roll a two}\\) to \\(2\\), and so on. (Allows us to do math with probability spaces!)\n\n\\(X\\) is a discrete random variable if it maps outcomes to countable set of numbers, whether this means a finite set like \\(\\{1,2,3\\}\\) or a countably infinite set like \\(\\mathbb{N}\\).\n\\(X\\) is a continuous random variable if it maps outcomes to a non-countable set of numbers, typically \\(\\mathbb{R}\\).\n\\(\\mathcal{R}_X\\), the support of a random variable \\(X\\), is the set of all possible values that the random variable can map onto. For example, if \\(X\\) represents a dice roll, then \\(\\mathcal{R}_X = \\{1, 2, 3, 4, 5, 6\\}\\).\nCumulative Density Function (CDF): Given a random variable \\(X\\) (whether discrete or continuous), \\(F_X(v) = P(X \\leq v)\\) is its cumulative density function, which tells us the probability that \\(X\\) is realized as a number less than or equal to some value \\(v\\).\nProbability Mass Function (PMF): Given a discrete random variable \\(X\\), \\(p_X(v) = P(X = v)\\) is its probability mass function, which tells us the probability that \\(X\\) is realized as the value \\(v\\).\nProbability Density Function (PDF): Given a continuous random variable \\(X\\), \\(f_X(v)\\) is the unique function which allows us to determine, using integration, the probability that \\(X\\) is in some range \\([a,b]\\). That is, it is the unique function satisfying \\(P(X \\in [a,b]) = \\int_a^b f_X(x)dx\\).\n\nRemember: unlike in the discrete case where \\(p_X(v) = P(X = v)\\), \\(f_X(v)\\) is not the probability that \\(X\\) is realized as the value \\(v\\). \\(f_X(v) \\neq P(X = v)\\)."
  },
  {
    "objectID": "cheatsheet-math.html#week-06-expectation-variance-moments",
    "href": "cheatsheet-math.html#week-06-expectation-variance-moments",
    "title": "DSAN 5100 Math Cheatsheet",
    "section": "Week 06: Expectation, Variance, Moments",
    "text": "Week 06: Expectation, Variance, Moments\n\n\\(M_1(V)\\): The (‚Äúregular‚Äù) arithmetic mean of a set of values \\(V = \\{v_1, v_2, \\ldots, v_n\\}\\): \\(M_1(V) = (v_1 + v_2 + \\cdots + v_n)\\frac{1}{n} = \\left( \\sum_{i=1}^n v_i \\right)\\frac{1}{n}\\).\n\\(M_0(V)\\): The geometric mean of a set of values \\(V = \\{v_1, v_2, \\ldots, v_n\\}\\): \\(M_0(V) = (v_1\\cdot v_2 \\cdot \\cdots \\cdot v_n)^{\\frac{1}{n}} = \\left( \\prod_{i=1}^n v_i \\right)^{\\frac{1}{n}}\\)\n\\(M_{-1}(V)\\): The harmonic mean of a set of values \\(V = \\{v_1, v_2, \\ldots, v_n\\}\\): \\(M_{-1}(V) = \\frac{n}{\\frac{1}{v_1} + \\frac{1}{v_2} + \\cdots + \\frac{1}{v_n}} = \\left( \\frac{\\sum_{i=1}^n v_i^{-1}}{n} \\right)^{-1}\\)\n\\(\\odot\\): The Hadamard product of matrices. For two matrices \\(\\mathbf{X}_{[m \\times n]}\\) and \\(\\mathbf{Y}_{[m \\times n]}\\) with equal dimensions:\n\\[\n  X_{[m \\times n]} = \\begin{bmatrix}\n      x_{1,1} & \\cdots & x_{1,n} \\\\\n      \\vdots & \\ddots & \\vdots \\\\\n      x_{m,1} & \\cdots & x_{m,n}\n  \\end{bmatrix}, Y_{[m \\times n]} = \\begin{bmatrix}\n      y_{1,1} & \\cdots & y_{1,n} \\\\\n      \\vdots & \\ddots & \\vdots \\\\\n      y_{m,1} & \\cdots & y_{m,n}\n  \\end{bmatrix}\n  \\]\nTheir Hadamard product \\(\\mathbf{X} \\odot \\mathbf{Y}\\) is another \\(m \\times n\\) matrix\n\\[\n  (\\mathbf{X} \\odot \\mathbf{Y})_{[m \\times n]} = \\begin{bmatrix}\n      x_{1,1}y_{1,1} & \\cdots & x_{1,n}y_{1,n} \\\\\n      \\vdots & \\ddots & \\vdots \\\\\n      x_{m,1}y_{m,1} & \\cdots & x_{m,n}y_{m,n}\n  \\end{bmatrix}\n  \\]"
  },
  {
    "objectID": "extra-videos/index.html",
    "href": "extra-videos/index.html",
    "title": "Extra Videos",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Last Updated - Oldest\n        \n         \n          Last Updated - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\nLast Updated\n\n\n\n\n\n\nCorrected Bayes‚Äô Theorem Example (W03.1)\n\n\nFriday Sep 8, 2023\n\n\n\n\nStatistics Fundamentals (W02.5)\n\n\nTuesday Sep 5, 2023\n\n\n\n\nProbability Fundamentals (W02.4)\n\n\nMonday Sep 4, 2023\n\n\n\n\nCombinatorics (W02.3)\n\n\nSunday Sep 3, 2023\n\n\n\n\nReview (W02.2)\n\n\nSaturday Sep 2, 2023\n\n\n\n\nAbout Me (W02.1)\n\n\nFriday Sep 1, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "writeups/bayes-disease-tests/index.html",
    "href": "writeups/bayes-disease-tests/index.html",
    "title": "Bayes‚Äô Theorem: Disease Probabilities",
    "section": "",
    "text": "\\(D \\in \\{0, 1\\}\\) = has disease\n\\(T \\in \\{0, 1\\}\\) = test result\n(In the real world, however, we cannot observe \\(D\\), only \\(T\\))\n99% accuracy:\n\n\\(\\Pr(T = 1 \\mid D = 1) = 0.99\\)\n\\(\\Pr(T = 0 \\mid D = 0) = 0.99\\)\n\nRare disease: 1 in 10000 people has it\n\n\\(\\Pr(D = 1) = \\frac{1}{10000}\\)"
  },
  {
    "objectID": "writeups/bayes-disease-tests/index.html#what-were-given",
    "href": "writeups/bayes-disease-tests/index.html#what-were-given",
    "title": "Bayes‚Äô Theorem: Disease Probabilities",
    "section": "",
    "text": "\\(D \\in \\{0, 1\\}\\) = has disease\n\\(T \\in \\{0, 1\\}\\) = test result\n(In the real world, however, we cannot observe \\(D\\), only \\(T\\))\n99% accuracy:\n\n\\(\\Pr(T = 1 \\mid D = 1) = 0.99\\)\n\\(\\Pr(T = 0 \\mid D = 0) = 0.99\\)\n\nRare disease: 1 in 10000 people has it\n\n\\(\\Pr(D = 1) = \\frac{1}{10000}\\)"
  },
  {
    "objectID": "writeups/bayes-disease-tests/index.html#basic-bayes-probability-of-disease-given-positive-test",
    "href": "writeups/bayes-disease-tests/index.html#basic-bayes-probability-of-disease-given-positive-test",
    "title": "Bayes‚Äô Theorem: Disease Probabilities",
    "section": "Basic Bayes: Probability of Disease Given Positive Test",
    "text": "Basic Bayes: Probability of Disease Given Positive Test\nWe want\n\\[\n\\Pr(D = 1 \\mid T = 1)\n\\]\nWhich we can compute, using the given information, via Bayes‚Äô rule (where the second line is how I like to write it out, a bit more cluttered but makes it clear how to compute the denominator)\n\\[\n\\begin{align*}\n\\Pr(D = 1 \\mid T = 1) &= \\frac{\\Pr(T = 1 \\mid D = 1)\\Pr(D = 1)}{\\Pr(T = 1)} \\\\\n&= \\frac{\\Pr(T = 1 \\mid D = 1)\\Pr(D = 1)}{\\Pr(T = 1 \\mid D = 1)\\Pr(D = 1) + \\Pr(T = 1 \\mid D = 0)\\Pr(D = 0)}\n\\end{align*}\n\\]\nNumerically:\n\\[\n\\Pr(D = 1 \\mid T = 1) = \\frac{(0.99)(1/10000)}{(0.99)(1/10000) + (0.01)(9999/10000)} \\approx 0.0098\n\\]\n(Less than 1%)"
  },
  {
    "objectID": "writeups/bayes-disease-tests/index.html#deeper-dive",
    "href": "writeups/bayes-disease-tests/index.html#deeper-dive",
    "title": "Bayes‚Äô Theorem: Disease Probabilities",
    "section": "Deeper Dive",
    "text": "Deeper Dive\nBut now let‚Äôs think about what‚Äôs behind this‚Ä¶ It‚Äôs a dangerous, highly contagious disease, meaning that (societally) false negatives are much, much worse than false positives:\n\nA false negative, in this case, means that someone is walking around thinking they don‚Äôt have the disease (because they tested negative), when they actually do. This means that they are not quarantining, they are going out to parties and events and etc., spreading the disease.\nA false positive, on the other hand, means someone who panics unnecessarily: maybe it means, they go to the hospital, the hospital performs additional tests, and successfully discovers that the person, despite their positive test result, doesn‚Äôt have the disease.\nSo, consequence-wise, a false negative potentially means a new outbreak of the disease in the society, while a false positive means a quick (scary, but hopefully quick) trip to the hospital\n\n\nThe Catastrophic Case\nSo, let‚Äôs focus on the disastrous first case: what‚Äôs the probability of a false negative? First, we can compute the conditional probability of a negative test, conditional on someone having the disease? Here we just use our complement rule of probability: that \\(\\Pr(E^c) = 1 - \\Pr(E)\\) for any event \\(E\\):\n\\[\n\\Pr(T = 0 \\mid D = 1) = 1 - \\Pr(T = 1 \\mid D = 1) = 0.01\n\\]\nNow that we know this, let‚Äôs incorporate the base rate information‚Äîthat is, the information we have about the likelihood of having the disease (the thing we conditioned on above):\n\\[\n\\Pr(D = 1) = \\frac{1}{10000}\n\\]\nSo, given these two pieces of information, we can compute the probability of a person in the population being a false negative case: having the disease, but not being detected by the test.\n\\[\n\\begin{align*}\n\\Pr(T = 0 \\cap D = 1) &= \\Pr(T = 0 \\mid D = 1)\\Pr(D = 1) \\\\\n&= (0.01)(1/10000) = \\frac{1}{1000000},\n\\end{align*}\n\\]\ni.e., one in a million.\n\n\nThe Bad (But Not Catastrophic) Case\nNow let‚Äôs turn to the second, bad but not catastrophic, case: the probability of a false positive. As before, we start by computing the conditional probability of a positive test result for someone who in fact does not have the disease:\n\\[\n\\Pr(T = 1 \\mid D = 0) = 1 - \\Pr(T = 0 \\mid D = 0) = 0.01\n\\]\nThis time, however, we‚Äôll see that the base rate will make a big difference. The base rate in this case‚Äîthe probability of someone not having the disease‚Äîis:\n\\[\n\\Pr(D = 0) = 1 - \\Pr(D = 1) = \\frac{9999}{10000}\n\\]\nSo, incorporating these two pieces of information, we can compute the likelihood of a false positive case: someone in the population who doesn‚Äôt have the disease but does test positive:\n\\[\n\\begin{align*}\n\\Pr(T = 1 \\cap D = 0) &= \\Pr(T = 1 \\mid D = 0)\\Pr(D = 0) \\\\\n&= (0.01)(9999/10000) = \\frac{9999}{1000000}\n\\end{align*}\n\\]\nIn words: for every million people in the population, 9999 of them will have a false positive panic: they won‚Äôt have the disease, but they will think they have the disease because of their positive test."
  },
  {
    "objectID": "writeups/bayes-disease-tests/index.html#putting-it-together",
    "href": "writeups/bayes-disease-tests/index.html#putting-it-together",
    "title": "Bayes‚Äô Theorem: Disease Probabilities",
    "section": "Putting It Together:",
    "text": "Putting It Together:\nAt first, this example is depressing: ‚ÄúOh no, that‚Äôs terrible! We‚Äôre forcing thousands of people to panic, thinking that they have the disease, when they really don‚Äôt!‚Äù\nBut, walking through it with this false negative / false positive paradigm, we see the real takeaway: that there is always a tradeoff between false positives and false negatives. In this case, from a public health perspective for example, it‚Äôs actually somewhat of a good situation: at the ‚Äúcost‚Äù of having several thousand people panic unnecessarily, we achieve the benefit of making it very, very unlikely (one in a million, literally) that someone goes undetected in the population with this dangerous, contagious disease.\n\n\n\n\n\n\n\n\n\n\\(N = 1000000\\)\nTrue State of the World\n\n\n\\(D = 0\\)\n(Doesn't have disease)\n\\(D = 1\\)\n(Has disease)\n\n\n\n\nPrediction\n\\(T = 0\\)\n(Tested negative)\nTrue Negative\nSuccessfully detected no disease üëç\n989,901 People\nFalse Negative\nFailed to detect disease üö®üòµüö®\n1 Person\n\n\n\\(T = 1\\)\n(Tested positive)\nFalse Positive\nFalse alarm üò¨ sorry!\n9999 People\nTrue Positive\nSuccessfully detected disease üòÆ‚Äçüí®\n99 People\n\n\n\n\n(Compuation of the remaining two cells:)\nTrue Positive:\n\\[\n\\begin{align*}\n\\Pr(T = 1 \\cap D = 1) &= \\Pr(T = 1 \\mid D = 1)\\Pr(D = 1) \\\\\n&= (0.99)\\frac{1}{10000} = \\frac{99}{1000000}\n\\end{align*}\n\\]\nTrue Negative:\n\\[\n\\begin{align*}\n\\Pr(T = 0 \\cap D = 0) &= \\Pr(T = 0 \\mid D = 0)\\Pr(D = 0) \\\\\n&= (0.99)\\frac{9999}{10000} = \\frac{989901}{1000000}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "writeups/index.html",
    "href": "writeups/index.html",
    "title": "Extra Writeups",
    "section": "",
    "text": "Order By\n       Default\n         \n          Last Updated - Oldest\n        \n         \n          Last Updated - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\nLast Updated\n\n\nCategory\n\n\n\n\n\n\nTwo Ways of Sampling from a Data Frame\n\n\nMonday, September 18, 2023\n\n\nExtra Writeups\n\n\n\n\nTwo Ways to Compute Probabilities in R/Python\n\n\nSaturday, September 16, 2023\n\n\nExtra Writeups\n\n\n\n\nQuiz 1 Clarifications\n\n\nWednesday, September 13, 2023\n\n\nClarifications\n\n\n\n\nBayes‚Äô Theorem: Disease Probabilities\n\n\nWednesday, September 13, 2023\n\n\nExtra Writeups\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "writeups/quiz-1-clarifications/index.html",
    "href": "writeups/quiz-1-clarifications/index.html",
    "title": "Quiz 1 Clarifications",
    "section": "",
    "text": "Quiz 1 Links\n\n\n\n\nQuiz 1"
  },
  {
    "objectID": "writeups/quiz-1-clarifications/index.html#starting-off-generating-the-new-york-data",
    "href": "writeups/quiz-1-clarifications/index.html#starting-off-generating-the-new-york-data",
    "title": "Quiz 1 Clarifications",
    "section": "Starting Off: Generating the New York Data",
    "text": "Starting Off: Generating the New York Data"
  },
  {
    "objectID": "w02/slides.html#prof.-jeff-introduction",
    "href": "w02/slides.html#prof.-jeff-introduction",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Prof.¬†Jeff Introduction!",
    "text": "Prof.¬†Jeff Introduction!\n\nBorn and raised in NW DC ‚Üí high school in Rockville, MD\nUniversity of Maryland: Computer Science, Math, Economics (2008-2012)"
  },
  {
    "objectID": "w02/slides.html#grad-school",
    "href": "w02/slides.html#grad-school",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Grad School",
    "text": "Grad School\n\nStudied abroad in Beijing (Peking University/ÂåóÂ§ß) ‚Üí internship with Huawei in Hong Kong (HKUST)\n\n\n\n\nStanford for MS in Computer Science (2012-2014)\nResearch Economist at UC Berkeley (2014-2015)\n\n\n\n\n\n\nColumbia (NYC) for PhD[+Postdoc] in Political Science (2015-2023)"
  },
  {
    "objectID": "w02/slides.html#dissertation-political-science-history",
    "href": "w02/slides.html#dissertation-political-science-history",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Dissertation (Political Science + History)",
    "text": "Dissertation (Political Science + History)\n‚ÄúOur Word is Our Weapon‚Äù: Text-Analyzing Wars of Ideas from the French Revolution to the First Intifada"
  },
  {
    "objectID": "w02/slides.html#research-labor-economics",
    "href": "w02/slides.html#research-labor-economics",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Research (Labor Economics)",
    "text": "Research (Labor Economics)\n\n\n\n‚ÄúMonopsony in Online Labor Markets‚Äù: Machine Learning to enhance causal estimates of the effect of job description language on uptake rate\n\n\n\n‚ÄúFreedom as Non-Domination in the Labor Market‚Äù: Game-theoretic models of workers‚Äô rights (monopsony vs.¬†labor discipline)\n\n\n\n\n\n‚ÄúUnsupervised Extraction of Workplace Rights and Duties from Collective Bargaining Agreements‚Äù: Linguistic (dependency) parses of contracts ‚Üí time series of worker vs.¬†employer rights and responsibilities over time"
  },
  {
    "objectID": "w02/slides.html#deterministic-processes",
    "href": "w02/slides.html#deterministic-processes",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Deterministic Processes",
    "text": "Deterministic Processes\n\nGiven a set of inputs, we can compute the outcome exactly\nExample: Given the radius of a circle, we can compute its area without any uncertainty. \\(r \\mapsto \\pi r^2\\)\n(The fact that we can compute the outcome doesn‚Äôt mean that it‚Äôs easy to do so! See, e.g., the double pendulum)\n\n\nImage credit: Tenor.com\nThe pendulum example points to the fact that the notion of a chaotic system, one which is ‚Äúsensitive to initial conditions‚Äù, is different from that of a stochastic system."
  },
  {
    "objectID": "w02/slides.html#holy-grail-deterministic-model-newtonian-physics",
    "href": "w02/slides.html#holy-grail-deterministic-model-newtonian-physics",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "‚ÄúHoly Grail‚Äù Deterministic Model: Newtonian Physics",
    "text": "‚ÄúHoly Grail‚Äù Deterministic Model: Newtonian Physics\n\n\n\n\n\n\n\n\n\n\ngrid\n\n \n\ncluster_01\n\n ‚ÄúNature‚Äù  \n\ncluster_02\n\n ‚ÄúScience‚Äù   \n\nObs\n\n Thing(s) we can see   \n\nUnd\n\n Underlying processes   \n\nUnd-&gt;Obs\n\n    \n\nModel\n\n Model   \n\nUnd-&gt;Model\n\n    \n\nModel-&gt;Obs\n\n   \n\n\n\n\n\n\n\n\n\n\n\n\n\ngrid\n\n \n\ncluster_04\n\n  Woolsthorpe Manor    \n\ncluster_03\n\n Isaac Newton   \n\nTree\n\n  Falling Apple     \n\nPhysics\n\n  Particle Interactions     \n\nPhysics-&gt;Tree\n\n    \n\nNewton\n\n Newton‚Äôs Laws   \n\nPhysics-&gt;Newton\n\n    \n\nNewton-&gt;Tree\n\n   \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\n\\leadsto F_g = G\\frac{m_1m_2}{r^2}\n\\]\nFigure¬†1: Newton‚Äôs Law of Universal Gravitation‚Üê Dr.¬†Zirkel follows Newton‚Äôs famous steps. Coloured wood engraving. Wellcome Collection (Public Domain)"
  },
  {
    "objectID": "w02/slides.html#but-what-happens-when",
    "href": "w02/slides.html#but-what-happens-when",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "But What Happens When‚Ä¶",
    "text": "But What Happens When‚Ä¶\n\\[\n\\text{Outcome}\\left(\\text{Dice Roll}\\right) = \\; ?\\frac{?_1?_2}{?^2}\n\\]\n\n\n\nPre-Enlightenment\n\n\n\n\nHans Sebald Beham, Fortuna (1541), CC BY 4.0, via Wikimedia Commons\n\n\n\n\nPost-Enlightenment\n\n\n\n\nBlaise Pascal, Trait√© du triangle arithm√©tique (1665). Public Domain, via Internet Archive"
  },
  {
    "objectID": "w02/slides.html#random-processes",
    "href": "w02/slides.html#random-processes",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Random Processes",
    "text": "Random Processes\n\n\n\nCan‚Äôt compute the outcome exactly, but can still say something about potential outcomes!\nExample: randomly chosen radius \\(r \\in [0,1]\\), what can we say about \\(A = \\pi r^2\\)?\n\nUnif: \\([0,\\pi]\\) equally likely\nExp: closer to \\(0\\) more likely"
  },
  {
    "objectID": "w02/slides.html#data-ground-truth-noise",
    "href": "w02/slides.html#data-ground-truth-noise",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Data = Ground Truth + Noise",
    "text": "Data = Ground Truth + Noise\n\nDepressing but true origin of statistics (as opposed to probability): the Plague üò∑\n\n\n\n\n\n\n\nGround Truth: The Great Plague (Lord Have Mercy on London, Unknown Artist, circa 1665, via Wikimedia Commons)\n\n\n\n\n\n\n\nNoisy Data (Recorded amidst chaos): London Bill of Mortality, 1665 (Public Domain, Wellcome Collection)"
  },
  {
    "objectID": "w02/slides.html#random-variables",
    "href": "w02/slides.html#random-variables",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Random Variables",
    "text": "Random Variables\n\nIn algebra, to solve problems we work with variables\nIn probability theory, to solve problems we work with random variables\nRecall the difference between random and deterministic: \\(A = \\pi r^2\\) tells us that, given a value of \\(r\\), we can solve for the unique value of \\(A\\)\nIn probability theory, however, there is no one ‚Äútrue‚Äù value of a random variable \\(X\\).\nLet \\(X = f(N)\\) mean that \\(X\\) is the result of a rolled die, where the die has \\(N\\) sides.\nPlugging in \\(N = 6\\) (standard 6-sided die) still doesn‚Äôt mean we know ‚Äúthe‚Äù value of \\(X\\). However, (if the die is fair) we do know\n\n\\[\n\\Pr(X = 1) = \\Pr(X = 2) = \\cdots = \\Pr(X = 6) = \\frac{1}{6}\n\\]"
  },
  {
    "objectID": "w02/slides.html#discrete-vs.-continuous",
    "href": "w02/slides.html#discrete-vs.-continuous",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Discrete vs.¬†Continuous",
    "text": "Discrete vs.¬†Continuous\n\nMany complicated definitions, often misleading or unintuitive!\nHow I want you to remember: How many possible values between two known values?\nDiscrete: e.g., number of siblings\n\nI have 2 siblings, you have 3 siblings‚Ä¶ How many values (sibling counts) in between?\n\nContinuous: e.g., temperature\n\nIt is 27.0¬∞ C in my room, 28.0¬∞ C in your room‚Ä¶ How many values (temperatures) in between?\n\nSo, if \\(X\\) is the result of a rolled die, is \\(X\\) discrete or continuous? How many values can be rolled between 3 and 4?"
  },
  {
    "objectID": "w02/slides.html#thinking-about-independence",
    "href": "w02/slides.html#thinking-about-independence",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Thinking About Independence",
    "text": "Thinking About Independence\n\nWe‚Äôll define it formally later; for now, this is our working definition:\n\n\n\n\n\n\n\nWorking Definition: Independence\n\n\nTwo random variables \\(X\\) and \\(Y\\) are independent if learning information about \\(X\\) does not give you information about the value of \\(Y\\), or vice-versa."
  },
  {
    "objectID": "w02/slides.html#na√Øve-definition-of-probability",
    "href": "w02/slides.html#na√Øve-definition-of-probability",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Na√Øve Definition of Probability",
    "text": "Na√Øve Definition of Probability\n\nSample Space: The set of all possible outcomes of an experiment\nEvent: A subset of the sample space\n\n\n\n\n\n\n\nNa√Øve Definition of Probability\n\n\nGiven a sample space \\(S\\), and an event \\(E \\subset S\\),\n\\[\n\\Pr(\\underbrace{E}_{\\text{event}}) = \\frac{\\text{\\# Favorable Outcomes}}{\\text{\\# Possible Outcomes}} = \\frac{|E|}{|S|}\n\\]"
  },
  {
    "objectID": "w02/slides.html#example-flipping-two-coins",
    "href": "w02/slides.html#example-flipping-two-coins",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Example: Flipping Two Coins",
    "text": "Example: Flipping Two Coins\n\n\n\n\n\n\nNa√Øve Definition of Probability\n\n\nGiven a sample space \\(S\\), and an event \\(E \\subset S\\),\n\\[\n\\Pr(\\underbrace{E}_{\\text{event}}) = \\frac{\\text{\\# Favorable Outcomes}}{\\text{\\# Possible Outcomes}} = \\frac{|E|}{|S|}\n\\]\n\n\n\n\nFlipping two coins:\n\nSample space \\(S = \\{TT, TH, HT, HH\\}\\)\nEvent \\(E_1\\): Result of first flip is \\(H\\), result of second flip is \\(T\\) \\(\\implies\\) \\(E_1 = \\{HT\\}\\).\nEvent \\(E_2\\): At least one \\(H\\) \\(\\implies\\) \\(E_2 = \\{TH, HT, HH\\}\\).\n\n\n\\[\n\\begin{align*}\n\\Pr(E_1) &= \\frac{|\\{HT\\}|}{|S|} = \\frac{|\\{HT\\}|}{|\\{TT, TH, HT, HH\\}|} = \\frac{1}{4} \\\\\n\\Pr(E_2) &= \\frac{|\\{TH, HT, HH\\}|}{|S|} = \\frac{|\\{TH, HT, HH\\}|}{|\\{TT, TH, HT, HH\\}|} = \\frac{3}{4}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "w02/slides.html#events-neq-outcomes",
    "href": "w02/slides.html#events-neq-outcomes",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Events \\(\\neq\\) Outcomes!",
    "text": "Events \\(\\neq\\) Outcomes!\n\nOutcomes are things, events are sets of things\nSubtle but extremely important distinction!\nIn the coin flip example:\n\nThe event \\(E_1 = \\{HT\\}\\) can be confused with the outcome \\(HT\\).\nSo, try to remember instead the event \\(E_2 = \\{TH, HT, HH\\}\\): it is more clear, in this case, how this event does not correspond to any individual outcome"
  },
  {
    "objectID": "w02/slides.html#back-to-the-na√Øve-definition",
    "href": "w02/slides.html#back-to-the-na√Øve-definition",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Back to the Na√Øve Definition",
    "text": "Back to the Na√Øve Definition\n\n\n\n\n\n\nNa√Øve Definition of Probability\n\n\nGiven a sample space \\(S\\), and an event \\(E \\subset S\\),\n\\[\n\\Pr(\\underbrace{E}_{\\text{event}}) = \\frac{\\text{\\# Favorable Outcomes}}{\\text{\\# Possible Outcomes}} = \\frac{|E|}{|S|}\n\\]\n\n\n\n\nThe na√Øve definition tells us that probabilities are just ratios of counts:\n\nCount the number of ways the event \\(E\\) can happen, count the total number of things that can happen, and divide!\n\nThis is why we begin studying probability by studying combinatorics: the mathematics of counting"
  },
  {
    "objectID": "w02/slides.html#combinatorics-ice-cream-possibilities",
    "href": "w02/slides.html#combinatorics-ice-cream-possibilities",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Combinatorics: Ice Cream Possibilities",
    "text": "Combinatorics: Ice Cream Possibilities\n\n\n\n\n\n\n\nThe \\(6 = 2 \\cdot 3\\) possible cone+flavor combinations which can result from choosing a flavor first and a cone type second.\n\n\n\n\n\n\n\nThe \\(6 = 3 \\cdot 2\\) possible cone+flavor combinations which can result from choosing a flavor first and a cone type second."
  },
  {
    "objectID": "w02/slides.html#grouping-vs.-ordering",
    "href": "w02/slides.html#grouping-vs.-ordering",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Grouping vs.¬†Ordering",
    "text": "Grouping vs.¬†Ordering\n\nIn standard statistics/combinatorics introductions you‚Äôll learn different counting formulas for when order matters vs.¬†when order doesn‚Äôt matter\nThis is not a mathematical distinction so much as a pragmatic distinction: what are you trying to accomplish by counting?\nProblems with extremely similar descriptions can differ in small detail, so that the units you need to distinguish between in one version differ from the units you need to distinguish between in the other."
  },
  {
    "objectID": "w02/slides.html#does-order-matter",
    "href": "w02/slides.html#does-order-matter",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Does Order Matter?",
    "text": "Does Order Matter?\n\n\n\n\n\n\nExample: Student Government vs.¬†Student Sports\n\n\n\nConsider a school where students can either try out for the swim team or run for a position in the student government\nThe swim team has 4 slots, but slots aren‚Äôt differentiated: you‚Äôre either on the team (one of the 4 chosen students) or not\nThe student government also has 4 slots, but there is a difference between the slots: first slot is President, second is Vice President, third is Secretary, and fourth is Treasurer.\n\n\n\n\n\nSimple case (for intuition): the school only has 4 students. In this case, how many ways are there to form the swim team? What about the student government?\n\nSwim team: \\(1\\) way. You have only one choice, to let all 4 students onto the team\nStudent government: \\(4 \\cdot 3 \\cdot 2 \\cdot 1 = 24\\) ways. You have to let all 4 students in, but you have a choice of who is President, Vice President, Secretary, and Treasurer\n\nHow did we get \\(4 \\cdot 3 \\cdot 2 \\cdot 1\\)? (Think about the ice cream example‚Ä¶)\n\nStart by choosing the President: 4 choices\nNow choose the Vice President: only 3 students left to choose from\nNow choose the Secretary: only 2 students left to choose from\nNow choose the Treasurer: only 1 student left to choose from"
  },
  {
    "objectID": "w02/slides.html#permutations-vs.-combinations",
    "href": "w02/slides.html#permutations-vs.-combinations",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Permutations vs.¬†Combinations",
    "text": "Permutations vs.¬†Combinations\n\nPermutations: How many ways can I choose groups of size \\(k\\) out of \\(n\\) total objects, where order within groups matters: \\(P_{n,k}\\) (sometimes written \\(_nP_k\\)).\n\nIn this case, we want to count \\((a,b)\\) and \\((b,a)\\) as two separate groups\n\nCombinations: How many ways can I choose groups of size \\(k\\) out of \\(n\\) total objects, where order in the groups doesn‚Äôt matter: \\(C_{n,k}\\) (sometimes written \\(_nC_k,\\binom{n}{k}\\)).\n\nIn this case, we don‚Äôt want to count \\((a, b)\\) and \\((b, a)\\) as two separate groups‚Ä¶\n\n\n\\[\n\\begin{align*}\nP_{n,k} = \\frac{n!}{(n-k)!}, \\; C_{n,k} = \\frac{n!}{k!(n-k)!}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "w02/slides.html#no-need-to-memorize",
    "href": "w02/slides.html#no-need-to-memorize",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "No Need to Memorize!",
    "text": "No Need to Memorize!\n\n\nKey point: you don‚Äôt have to remember these as two separate formulas!\nThe number of combinations is based on the number of permutations, but corrected for double counting: e.g., corrected for the fact that \\((a,b) \\neq (b,a)\\) when counting permutations but \\((a,b) = (b,a)\\) when counting combinations.\n\n\\[\nC_{n,k} = \\frac{P_{n,k}}{k!} \\genfrac{}{}{0pt}{}{\\leftarrow \\text{Permutations}}{\\leftarrow \\text{Duplicate groups}}\n\\]\nWhere does \\(k!\\) come from? (How many different orderings can we make of the same group?)\n\n\\(k = 2\\): \\((\\underbrace{\\boxed{\\phantom{a}}}_{\\text{2 choices}},\\underbrace{\\boxed{\\phantom{a}}}_{\\text{1 remaining choice}}) \\implies 2\\)\n\\(k = 3\\): \\((\\underbrace{\\boxed{\\phantom{a}}}_{\\text{3 choices}},\\underbrace{\\boxed{\\phantom{a}}}_{\\text{2 remaining choices}}, \\underbrace{\\boxed{\\phantom{a}}}_{\\text{1 remaining choice}}) \\implies 6\\)\n\\(k = 4\\): \\((\\underbrace{\\boxed{\\phantom{a}}}_{\\text{4 choices}}, \\underbrace{\\boxed{\\phantom{a}}}_{\\text{3 remaining choices}}, \\underbrace{\\boxed{\\phantom{a}}}_{\\text{2 remaining choices}}, \\underbrace{\\boxed{\\phantom{a}}}_{\\text{1 remaining choice}}) \\implies 24\\)"
  },
  {
    "objectID": "w02/slides.html#with-or-without-replacement",
    "href": "w02/slides.html#with-or-without-replacement",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "With or Without Replacement?",
    "text": "With or Without Replacement?\n\nBoils down to: can the same object be included in my sample more than once?\n\n\n\n\nWithout Replacement\nWith Replacement\n\n\n\n\nMost statistical problems: ‚ÄúCheck off‚Äù objects as you collect data about them, so that each observation in your data is unique\nVery special (but very important!) set of statistical problems: allow objects to appear in your sample multiple times, to ‚Äúsqueeze‚Äù more information out of the sample (called Bootstrapping‚Äîmuch more on this later in the course!)"
  },
  {
    "objectID": "w02/slides.html#how-many-possible-samples",
    "href": "w02/slides.html#how-many-possible-samples",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "How Many Possible Samples?",
    "text": "How Many Possible Samples?\n\n\n\n\n\n\nExample: How Many Possible Samples?\n\n\nFrom a population of \\(N = 3\\), how many ways can we take samples of size \\(k = 2\\)?\n\n\n\n\n\n\n\nWithout Replacement\nWith Replacement\n\n\n\n\n\\(3 \\cdot 2 = 6\\) ways (3 objects to choose from for first element of sample, 2 remaining objects to choose from for second element of sample)\n\\(3\\cdot 3 = 3^2 = 9\\) ways (3 objects to choose from for first element of sample, still 3 objects to choose from for second element of sample)\n\n\n\n\n\n\n\n\n\n\n\n\nResult: How Many Possible Samples\n\n\nFrom a population of size \\(N\\), how many ways can we take samples of size \\(k\\)? (Try to extrapolate from above examples before looking at answer!)\n\n\n\n\n\n\n\nWithout Replacement\nWith Replacement\n\n\n\n\n\\(\\displaystyle \\underbrace{N \\cdot (N-1) \\cdot \\cdots \\cdot (N - k + 1)}_{k\\text{ times}} = \\frac{N!}{(N - k )!}\\)(This formula should look somewhat familiar‚Ä¶)\n\\(\\displaystyle \\underbrace{N \\cdot N \\cdot \\cdots \\cdot N}_{k\\text{ times}} = N^k\\)"
  },
  {
    "objectID": "w02/slides.html#logic-sets-and-probability",
    "href": "w02/slides.html#logic-sets-and-probability",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Logic, Sets, and Probability",
    "text": "Logic, Sets, and Probability\n\nThere is a deep connection1 between the objects and operations of logic, set theory, and probability:\n\n\n\n\n\n\n\n\n\n\n\nLogic\nSet Theory\nProbability Theory\n\n\n\n\nObjects\nPredicates\\(p, q \\in \\{T, F\\}\\)\nSets\\(S = \\{a, b, \\ldots\\}\\)\nEvents\\(E = \\{TH, HT, HH\\}\\)\n\n\nConjunction\nAnd (\\(\\wedge\\))\\(p \\wedge q\\)\nIntersection (\\(\\cap\\))\\(A \\cap B\\)\nMultiplication (\\(\\times\\)):\\(\\Pr(E_1 \\cap E_2) = \\Pr(E_1)\\times \\Pr(E_2)\\)\n\n\nDisjunction\nOr (\\(\\vee\\))\\(p \\vee q\\)\nUnion (\\(\\cup\\))\\(A \\cup B\\)\nAddition (\\(+\\)): \\(\\Pr(E_1 \\cup E_2) =\\)\\(\\Pr(E_1) + \\Pr(E_2) - \\Pr(E_1 \\wedge E_2)\\)\n\n\nNegation\nNot (\\(\\neg\\))\\(\\neg p\\)\nComplement (\\(^c\\))\\(S^c\\)\nSubtract from 1\\(\\Pr(A^c) = 1 - \\Pr(A)\\)\n\n\n\nFor math majors, you can think of it as an isomorphism between the objects and operations of the three subjects"
  },
  {
    "objectID": "w02/slides.html#example-flipping-two-coins-1",
    "href": "w02/slides.html#example-flipping-two-coins-1",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Example: Flipping Two Coins",
    "text": "Example: Flipping Two Coins\n\nLogic: We can define 4 predicates:\n\n\\(p_1\\) = ‚ÄúFirst result is \\(H\\)‚Äù, \\(q_1\\) = ‚ÄúFirst result is \\(T\\)‚Äù\n\\(p_2\\) = ‚ÄúSecond result is \\(H\\)‚Äù, \\(q_2\\) = ‚ÄúSecond result is \\(T\\)‚Äù\n\nLogical formulas:\n\n\\(f_1 = p_1 \\wedge q_2\\): ‚ÄúFirst result is \\(H\\) and second result is \\(T\\)‚Äù\n\\(f_2 = p_1 \\vee q_2\\): ‚ÄúFirst result is \\(H\\) or second result is \\(T\\)‚Äù\n\\(f_3 = \\neg p_1\\): ‚ÄúFirst result is not \\(H\\)‚Äù\n\nThe issue?: We don‚Äôt know, until after the coins have been flipped, whether these are true or false!\nBut, we should still be able to say something about their likelihood, for example, whether \\(f_1\\) or \\(f_2\\) is more likely to happen‚Ä¶ Enter probability theory!"
  },
  {
    "objectID": "w02/slides.html#logic-rightarrow-probability",
    "href": "w02/slides.html#logic-rightarrow-probability",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Logic \\(\\rightarrow\\) Probability",
    "text": "Logic \\(\\rightarrow\\) Probability\n\nProbability theory lets us reason about the uncertainty surrounding logical predicates like \\(p\\) and \\(q\\), by:\n\nencoding them as sets of possibilities \\(P\\) and \\(Q\\), and\nrepresenting the uncertainty around any given possibility using a probability measure \\(\\Pr: S \\mapsto [0,1]\\),\n\nthus allowing us to reason about\n\nthe likelihood of these set-encoded predicates on their own: \\(\\Pr(P)\\) and \\(\\Pr(Q)\\), but also\ntheir logical connections: \\(\\Pr(p \\wedge q) = \\Pr(P \\cap Q)\\), \\(\\Pr(\\neg p) = \\Pr(P^c)\\), and so on."
  },
  {
    "objectID": "w02/slides.html#flipping-two-coins-logic-rightarrow-probability",
    "href": "w02/slides.html#flipping-two-coins-logic-rightarrow-probability",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Flipping Two Coins: Logic \\(\\rightarrow\\) Probability",
    "text": "Flipping Two Coins: Logic \\(\\rightarrow\\) Probability\n\nReturning to the two coins example: we can look at the predicates and see that they exhaust all possibilities, so that we can define a sample space \\(S = \\{TT, TH, HT, HH\\}\\) of all possible outcomes of our coin-flipping experiment, noting that \\(|S| = 4\\), so there are 4 possible outcomes.\nThen we can associate each predicate with an event, a subset of the sample space, and use our na√Øve definition to compute the probability of these events:\n\n\n\n\n\n\n\n\n\nPredicate\nEvent\nProbability\n\n\n\n\n\\(p_1\\) = ‚ÄúFirst result is \\(H\\)‚Äù\n\\(P_1 = \\{HT, HH\\}\\)\n\\(\\Pr(P_1) = \\frac{|P_1|}{|S|} = \\frac{2}{4} = \\frac{1}{2}\\)\n\n\n\\(q_1\\) = ‚ÄúFirst result is \\(T\\)‚Äù\n\\(Q_1 = \\{TT, TH\\}\\)\n\\(\\Pr(Q_1) = \\frac{|Q_1|}{|S|} = \\frac{2}{4} = \\frac{1}{2}\\)\n\n\n\\(p_2\\) = ‚ÄúSecond result is \\(H\\)‚Äù\n\\(P_2 = \\{TH, HH\\}\\)\n\\(\\Pr(P_2) = \\frac{|P_2|}{|S|} = \\frac{2}{4} = \\frac{1}{2}\\)\n\n\n\\(q_2\\) = ‚ÄúSecond result is \\(T\\)‚Äù\n\\(Q_2 = \\{TT, HT\\}\\)\n\\(\\Pr(Q_2) = \\frac{|Q_2|}{|S|} = \\frac{2}{4} = \\frac{1}{2}\\)"
  },
  {
    "objectID": "w02/slides.html#moving-from-predicates-to-formulas",
    "href": "w02/slides.html#moving-from-predicates-to-formulas",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Moving from Predicates to Formulas",
    "text": "Moving from Predicates to Formulas\n\nNotice that, in the four rows of the previous table, we were only computing the probabilities of ‚Äúsimple‚Äù events: events corresponding to a single predicate\nBut we promised that probability theory lets us compute probabilities for logical formulas as well! ‚Ä¶The magic of encoding events as sets becomes clear:\n\n\n\n\n\n\n\n\n\nFormula\nEvent\nProbability\n\n\n\n\n\\(f_1 = p_1 \\wedge q_2\\)\n\\[ \\begin{align*} F_1 &= P_1 \\cap Q_2 \\\\ &= \\{HT, HH\\} \\cap \\{TT, HT\\} \\\\ &= \\{HT\\} \\end{align*} \\]\n\\[\\begin{align*} \\Pr(F_1) &= \\Pr(\\{HT\\}) \\\\ &= \\frac{|\\{HT\\}|}{|S|} = \\frac{1}{4} \\end{align*}\\]\n\n\n\\(f_2 = p_1 \\vee q_2\\)\n\\[\\begin{align*} F_2 &= P_1 \\cup Q_2 \\\\ &= \\{HT, HH\\} \\cup \\{TT, HT\\} \\\\ &= \\{TT, HT, HH\\} \\end{align*}\\]\n\\[\\begin{align*} \\Pr(F_2) &= \\Pr(\\{TT, HT, HH\\}) \\\\ &= \\frac{|\\{TT, HT, HH\\}|}{|S|} = \\frac{3}{4} \\end{align*}\\]\n\n\n\\(f_3 = \\neg p_1\\)\n\\[\\begin{align*} F_3 &= P_1^c \\\\ &= \\{HT, HH\\}^c \\\\ &= \\{TT, TH\\}\\end{align*}\\]\n\\[\\begin{align*} \\Pr(F_3) &= \\Pr(\\{TT, TH\\}) \\\\ &= \\frac{|\\{TT, TH\\}|}{|S|} = \\frac{2}{4} = \\frac{1}{2} \\end{align*}\\]"
  },
  {
    "objectID": "w02/slides.html#using-rules-of-probability",
    "href": "w02/slides.html#using-rules-of-probability",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Using ‚ÄúRules‚Äù of Probability",
    "text": "Using ‚ÄúRules‚Äù of Probability\n\nHopefully, though, you found all this churning through set theory to be a bit tedious‚Ä¶\nThis is where rules of probability come from! They simplify set-theoretic computations into simple multiplications, additions, and subtractions:\n\n\\(\\Pr(A \\cap B) = \\Pr(A) \\times \\Pr(B)\\)\n\\(\\Pr(A \\cup B) = \\Pr(A) + \\Pr(B) - \\Pr(A \\cap B)\\)\n\\(\\Pr(A^c) = 1 - \\Pr(A)\\)\n\nSince we know probabilities of the ‚Äúsimple‚Äù events \\(P_1\\), \\(Q_1\\), \\(P_2\\), \\(Q_2\\), we don‚Äôt need to ‚Äúlook inside them‚Äù! Just take the probabilities and multiply/add/subtract as needed:\n\n\n\n\n\n\n\n\n\nFormula\nEvent\nProbability\n\n\n\n\n\\(f_1 = p_1 \\wedge q_2\\)\n\\(F_1 = P_1 \\cap Q_2\\)\n\\(\\Pr(F_1) = \\Pr(P_1) \\times \\Pr(Q_2) = \\frac{1}{2}\\times \\frac{1}{2} = \\frac{1}{4}\\)\n\n\n\\(f_2 = p_1 \\vee q_2\\)\n\\(F_2 = P_1 \\cup Q_2\\)\n\\[\\textstyle{\\begin{align*} \\textstyle \\Pr(F_2) &= \\Pr(P_1) + \\Pr(Q_2) - \\Pr(P_1 \\cap Q_2) \\\\ \\textstyle &= \\frac{1}{2} + \\frac{1}{2} - \\frac{1}{4} = \\frac{3}{4} \\end{align*}}\\]\n\n\n\\(f_3 = \\neg p_1\\)\n\\(F_3 = P_1^c\\)\n\\(\\Pr(F_3) = 1 - \\Pr(P_1) = 1 - \\frac{1}{2} = \\frac{1}{2}\\)"
  },
  {
    "objectID": "w02/slides.html#importing-results-from-logic",
    "href": "w02/slides.html#importing-results-from-logic",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "‚ÄúImporting‚Äù Results from Logic",
    "text": "‚ÄúImporting‚Äù Results from Logic\n\nThis deep connection between the three fields means that, if we have some useful theorem or formula from one field, we can immediately put it to use in another!\nFor example: DeMorgan‚Äôs Laws were developed in logic (DeMorgan was a 19th-century logician), and basically just tell us how to distribute logic operators:\n\n\\[\n\\begin{align*}\n\\underbrace{\\neg(p \\wedge q)}_{\\text{``}p\\text{ and }q\\text{'' is not true}} &\\iff \\underbrace{\\neg p \\vee \\neg q}_{p\\text{ is not true or }q\\text{ is not true}} \\\\\n\\underbrace{\\neg(p \\vee q)}_{\\text{``}p\\text{ or }q\\text{'' is not true}} &\\iff \\underbrace{\\neg p \\wedge \\neg q}_{p\\text{ is not true and }q\\text{ is not true}}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "w02/slides.html#converting-to-probability-theory",
    "href": "w02/slides.html#converting-to-probability-theory",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Converting to Probability Theory",
    "text": "Converting to Probability Theory\n\nSo, using the same principles we used in our coin flipping examples, we can consider events \\(P\\) and \\(Q\\), and get the following ‚Äútranslation‚Äù of DeMorgan‚Äôs Laws:\n\n\n\n\n\n\n\n\n\nLogic\nSet Theory\nProbability Theory\n\n\n\n\n\\(\\neg(p \\wedge q) = \\neg p \\vee \\neg q\\)\n\\((P \\cap Q)^c = P^c \\cup Q^c\\)\n\\(\\Pr((P \\cap Q)^c) = \\Pr(P^c \\cup Q^c)\\)\n\n\n\\(\\neg(p \\vee q) = \\neg p \\wedge \\neg q\\)\n\\((P \\cup Q)^c = P^c \\cap Q^c\\)\n\\(\\Pr((P \\cup Q)^c) = \\Pr(P^c \\cap Q^c)\\)\n\n\n\n\nNote that, since these are isomorphic to one another, we could have derived DeMorgan‚Äôs Laws from within probability theory, rather than the other way around:\n\n\\[\n\\begin{align*}\n\\Pr((P \\cap Q)^c) &= 1 - \\Pr(P \\cap Q) = 1 - \\Pr(P)\\Pr(Q) \\\\\n&= 1 - (1-\\Pr(P^c))(1 - \\Pr(Q^c)) \\\\\n&= 1 - [1 - \\Pr(P^c) - \\Pr(Q^c) + \\Pr(P^c)\\Pr(Q^c)] \\\\\n&= \\Pr(P^c) + \\Pr(Q^c) - \\Pr(P^c)\\Pr(Q^c) \\\\\n&= \\Pr(P^c) + \\Pr(Q^c) - \\Pr(P^c \\cap Q^c) \\\\\n&= \\Pr(P^c \\cup Q^c) \\; ‚úÖ\n\\end{align*}\n\\]"
  },
  {
    "objectID": "w02/slides.html#random-variables-1",
    "href": "w02/slides.html#random-variables-1",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Random Variables",
    "text": "Random Variables\n\nRecall our discussion of random variables: used by analogy to algebra, since we can do math with them:\nJust as \\(2 \\cdot 3\\) is shorthand for \\(2 + 2 + 2\\), we can define \\(X\\) as shorthand for the possible outcomes of a random process. \\[\n\\begin{align*}\nS = \\{ &\\text{result of dice roll is 1}, \\\\\n&\\text{result of dice roll is 2}, \\\\\n&\\text{result of dice roll is 3}, \\\\\n&\\text{result of dice roll is 4}, \\\\\n&\\text{result of dice roll is 5}, \\\\\n&\\text{result of dice roll is 6}\\} \\rightsquigarrow X \\in \\{1,\\ldots,6\\}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "w02/slides.html#random-variables-as-events",
    "href": "w02/slides.html#random-variables-as-events",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Random Variables as Events",
    "text": "Random Variables as Events\n\nEach value \\(v_X\\) that a random variable \\(X\\) can take on gives rise to an event \\(X = v_X\\): the event that the random variable \\(X\\) takes on value \\(v\\).\nSince \\(X = v_X\\) is an event, we can compute its probability \\(\\Pr(X = v_X)\\)!\n\n\n\n\nEvent in words\nEvent in terms of RV\n\n\n\n\nResult of dice roll is 1\n\\(X = 1\\)\n\n\nResult of dice roll is 2\n\\(X = 2\\)\n\n\nResult of dice roll is 3\n\\(X = 3\\)\n\n\nResult of dice roll is 4\n\\(X = 4\\)\n\n\nResult of dice roll is 5\n\\(X = 5\\)\n\n\nResult of dice roll is 6\n\\(X = 6\\)"
  },
  {
    "objectID": "w02/slides.html#doing-math-with-events",
    "href": "w02/slides.html#doing-math-with-events",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Doing Math with Events",
    "text": "Doing Math with Events\n\nWe‚Äôve seen how \\(\\Pr(\\cdot)\\) can ‚Äúencode‚Äù logical expressions involving uncertain outcomes.\nEven more powerful when paired with the notion of random variables: lets us also ‚Äúencode‚Äù mathematical expressions involving uncertain quantities!\nConsider an experiment where we roll two dice. Let \\(X\\) be the RV encoding the outcome of the first roll, and \\(Y\\) be the RV encoding the outcome of the second roll.\nWe can compute probabilities involving \\(X\\) and \\(Y\\) separately, e.g., \\(\\Pr(X = 1) = \\frac{1}{6}\\), but we can also reason probabilistically about mathematical expressions involving \\(X\\) and \\(Y\\)! For example, we can reason about their sum:\n\n\\[\n\\begin{align*}\n\\Pr(\\text{rolls sum to 10}) &= \\Pr(X + Y = 10) \\\\\n&= \\Pr(Y = 10 - X)\n\\end{align*}\n\\]\n\nOr about how the outcome of one roll will relate to the outcome of the other:\n\n\\[\n\\begin{align*}\n\\Pr(\\text{first roll above mean}) &= \\Pr\\left(X &gt; \\frac{X+Y}{2}\\right) \\\\\n&= \\Pr(2X &gt; X+Y) = \\Pr(X &gt; Y)\n\\end{align*}\n\\]"
  },
  {
    "objectID": "w02/slides.html#are-random-variables-all-powerful",
    "href": "w02/slides.html#are-random-variables-all-powerful",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Are Random Variables All-Powerful??",
    "text": "Are Random Variables All-Powerful??\n\nJust remember that probability \\(P(\\cdot)\\) is always probability of an event‚Äîrandom variables are just shorthand for quantifiable events.\nNot all events can be simplified via random variables!\n\n\\(\\text{catch a fish} \\mapsto P(\\text{trout}), P(\\text{bass}), \\ldots\\)\n\nWhat types of events can be quantified like this?\n\n(Hint: It has to do with a key topic in the early weeks of both DSAN 5000 and 5100‚Ä¶)\n\n\n\nThe answer is, broadly, any situation where you‚Äôre modeling things, like dice rolls, where mathematical operations like addition, multiplication, etc. make sense. So, if we‚Äôre modeling dice, it makes sense to say e.g.¬†‚Äúresult is 6‚Äù + ‚Äúresult is 3‚Äù = ‚Äútotal is 9‚Äù. More on the next page!"
  },
  {
    "objectID": "w02/slides.html#recall-types-of-variables",
    "href": "w02/slides.html#recall-types-of-variables",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Recall: Types of Variables",
    "text": "Recall: Types of Variables\n\nCategorical\n\nNo meaningful way to order values: \\(\\{\\text{trout}, \\text{bass}, \\ldots \\}\\)\n\nOrdinal\n\nCan place in order (bigger, smaller), though gaps aren‚Äôt meaningful: \\(\\{{\\color{orange}\\text{great}},{\\color{orange}\\text{greater}},{\\color{orange}\\text{greatest}}\\}\\)\n\\({\\color{orange}\\text{greater}} \\overset{?}{=} 2\\cdot {\\color{orange}\\text{great}} - 1\\)\n\nCardinal\n\nCan place in order, and gaps are meaningful \\(\\implies\\) can do ‚Äústandard‚Äù math with them! Example: \\(\\{{\\color{blue}1},{\\color{blue}2},\\ldots,{\\color{blue}10}\\}\\)\n\\({\\color{blue}7}\\overset{\\color{green}\\unicode{x2714}}{=} 2 \\cdot {\\color{blue}4} - 1\\)\nIf events have this structure (meaningful way to define multiplication, addition, subtraction), then we can analyze them as random variables"
  },
  {
    "objectID": "w02/slides.html#visualizing-discrete-rvs",
    "href": "w02/slides.html#visualizing-discrete-rvs",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Visualizing Discrete RVs",
    "text": "Visualizing Discrete RVs\n\nUltimate Probability Pro-Tip: When you hear ‚Äúdiscrete distribution‚Äù, think of a bar graph: \\(x\\)-axis = events, bar height = probability of events\nTwo coins example: \\(X\\) = RV representing number of heads obtained in two coin flips"
  },
  {
    "objectID": "w02/slides.html#preview-visualizing-continuous-rvs",
    "href": "w02/slides.html#preview-visualizing-continuous-rvs",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "(Preview:) Visualizing Continuous RVs",
    "text": "(Preview:) Visualizing Continuous RVs\n\nThis works even for continuous distributions, if you focus on the area under the curve instead of the height:"
  },
  {
    "objectID": "w02/slides.html#probability-theory-gives-us-distributions-for-rvs-not-numbers",
    "href": "w02/slides.html#probability-theory-gives-us-distributions-for-rvs-not-numbers",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Probability Theory Gives Us Distributions for RVs, not Numbers!",
    "text": "Probability Theory Gives Us Distributions for RVs, not Numbers!\n\nWe‚Äôre going beyond ‚Äúbase‚Äù probability theory if we want to summarize these distributions\nHowever, we can understand a lot about the full distribution by looking at some basic summary statistics. Most common way to summarize:\n\n\n\n\n\n\n\n\n\n\\(\\underbrace{\\text{point estimate}}_{\\text{mean/median}}\\)\n\\(\\pm\\)\n\\(\\underbrace{\\text{uncertainty}}_{\\text{variance/standard deviation}}\\)"
  },
  {
    "objectID": "w02/slides.html#example-game-reviews",
    "href": "w02/slides.html#example-game-reviews",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Example: Game Reviews",
    "text": "Example: Game Reviews\n\n(Data from Metacritic)"
  },
  {
    "objectID": "w02/slides.html#adding-a-single-line",
    "href": "w02/slides.html#adding-a-single-line",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Adding a Single Line",
    "text": "Adding a Single Line\n\n(Data from Metacritic)"
  },
  {
    "objectID": "w02/slides.html#or-a-single-ribbon",
    "href": "w02/slides.html#or-a-single-ribbon",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Or a Single Ribbon",
    "text": "Or a Single Ribbon"
  },
  {
    "objectID": "w02/slides.html#example-the-normal-distribution",
    "href": "w02/slides.html#example-the-normal-distribution",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Example: The Normal Distribution",
    "text": "Example: The Normal Distribution\n\n\n\n(The distribution you saw a few slides ago)\n\n\n\n\n\n\n\n\n‚ÄúRV \\(X\\) is normally distributed with mean \\({\\color{purple}\\mu}\\) and standard deviation \\({\\color{purple}\\sigma}\\)‚Äù\n\nTranslates to \\(X \\sim \\mathcal{N}(\\color{purple}{\\mu},\\color{purple}{\\sigma})\\)1\n\\(\\color{purple}{\\mu}\\) and \\(\\color{purple}{\\sigma}\\) are parameters2: the ‚Äúknobs‚Äù or ‚Äúsliders‚Äù which change the location/shape of the distribution\n\n\n\n\n\nThe parameters in this case give natural summaries of the data:\n\n\\({\\color{\\purple}\\mu}\\) = center (mean), \\({\\color{purple}\\sigma}\\) = [square root of] variance around center\n\nMean can usually be interpreted intuitively; for standard deviation, we usually use the 68-95-99.7 rule, which will make more sense relative to some real-world data‚Ä¶\n\nThroughout the course, this ‚Äúcalligraphic‚Äù font \\(\\mathcal{N}\\), \\(\\mathcal{D}\\), etc., will be used to denote distributionsThroughout the course, remember, purrple is for purrameters"
  },
  {
    "objectID": "w02/slides.html#real-data-and-the-68-95-99.7-rule",
    "href": "w02/slides.html#real-data-and-the-68-95-99.7-rule",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Real Data and the 68-95-99.7 Rule",
    "text": "Real Data and the 68-95-99.7 Rule\n\n\n\n\n\n\n\nFigure¬†2: Heights (cm) for 18K professional athletes\n\n\n\n\n\n\n\nFigure¬†3: The 68-95-99.7 Rule visualized (Wikimedia Commons)\n\n\n\n\nThe point estimate \\({\\color{purple}\\mu} = 186.48\\) is straightforward: the average height of the athletes is 186.48cm. Using the 68-95-99.7 Rule to interpret the SD, \\({\\color{purple}\\sigma} = 9.7\\), we get:\n\n\n\nAbout 68% of the heights fall between\n\n\n\n\n\n\n\n\n\n[\\({\\color{purple}\\mu} - 1\\cdot {\\color{purple}\\sigma}\\)\nand\n\\({\\color{purple}\\mu} + 1\\cdot {\\color{purple}\\sigma}\\)]\n\n\n[186.48 - 1 ¬∑ 9.7\nand\n186.48 + 1 ¬∑ 9.7]\n\n\n[176.78\nand\n196.18]\n\n\n\n\n\nAbout 95% of the heights fall between\n\n\n\n\n\n\n\n\n\n[\\({\\color{purple}\\mu} - 2 \\cdot {\\color{purple}\\sigma}\\)\nand\n\\({\\color{purple}\\mu} + 2 \\cdot {\\color{purple}\\sigma}\\)]\n\n\n[186.48 - 2 ¬∑ 9.7\nand\n186.48 + 2 ¬∑ 9.7]\n\n\n[167.08\nand\n205.88]"
  },
  {
    "objectID": "w02/slides.html#boxplots-comparing-multiple-distributions",
    "href": "w02/slides.html#boxplots-comparing-multiple-distributions",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Boxplots: Comparing Multiple Distributions",
    "text": "Boxplots: Comparing Multiple Distributions\n\n\n\n\n\n\nJhguch at en.wikipedia, CC BY-SA 2.5, via Wikimedia Commons\n\n\n\n\n\n\n\nProtonk, CC BY-SA 3.0, via Wikimedia Commons"
  },
  {
    "objectID": "w02/slides.html#another-option-joyplots",
    "href": "w02/slides.html#another-option-joyplots",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Another Option: Joyplots",
    "text": "Another Option: Joyplots\n\n\n\n\n\n\nFigure¬†4: (Iconic album cover)\n\n\n\n\n\n\nFigure¬†5: (Tooting my own horn)"
  },
  {
    "objectID": "w02/slides.html#multivariate-distributions-preview",
    "href": "w02/slides.html#multivariate-distributions-preview",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Multivariate Distributions: Preview",
    "text": "Multivariate Distributions: Preview\n\nThe bivariate normal distribution represents the distribution of two normally-distributed RVs \\(\\mathbf{X} = [\\begin{smallmatrix} X_1 & X_2\\end{smallmatrix}]\\), which may or may not be correlated:\n\n\n\\[\n\\mathbf{X} = \\begin{bmatrix}X_1 \\\\ X_2\\end{bmatrix}, \\; \\boldsymbol{\\mu} =\n%\\begin{bmatrix}\\mu_1 \\\\ \\mu_2\\end{bmatrix}\n\\begin{bmatrix}\\smash{\\overbrace{\\mu_1}^{\\mathbb{E}[X_1]}} \\\\ \\smash{\\underbrace{\\mu_2}_{\\mathbb{E}[X_2]}}\\end{bmatrix}\n, \\; \\mathbf{\\Sigma} = \\begin{bmatrix}\\smash{\\overbrace{\\sigma_1^2}^{\\text{Var}[X_1]}} & \\smash{\\overbrace{\\rho\\sigma_1\\sigma_2}^{\\text{Cov}[X_1,X_2]}} \\\\ \\smash{\\underbrace{\\rho\\sigma_2\\sigma_1}_{\\text{Cov}[X_2,X_1]}} & \\smash{\\underbrace{\\sigma_2^2}_{\\text{Var}[X_2]}}\\end{bmatrix}\n% \\begin{bmatrix}\\sigma_1^2 & \\rho\\sigma_1\\sigma_2 \\\\ \\rho\\sigma_2\\sigma_1 & \\sigma_2^2 \\end{bmatrix}\n% = \\begin{bmatrix}\\text{Var}[X_1] & \\text{Cov}[X_1,X_2] \\\\ \\text{Cov}[X_2,X_1] & \\text{Var}[X_2] \\end{bmatrix}\n\\]\n\n\nBy squishing all this information intro matrices, we can specify the parameters of multivariate-normally-distributed vectors of RVs similarly to how we specify single-dimensional normally-distributed RVs:\n\n\n\\[\n\\begin{align*}\n\\overbrace{X}^{\\mathclap{\\text{scalar}}} &\\sim \\mathcal{N}\\phantom{_k}(\\overbrace{\\mu}^{\\text{scalar}}, \\overbrace{\\sigma}^{\\text{scalar}}) \\tag{Univariate} \\\\\n\\underbrace{\\mathbf{X}}_{\\text{vector}} &\\sim \\boldsymbol{\\mathcal{N}}_k(\\smash{\\underbrace{\\boldsymbol{\\mu}}_{\\text{vector}}}, \\underbrace{\\mathbf{\\Sigma}}_{\\text{matrix}}) \\tag{Multivariate}\n\\end{align*}\n\\]\n\n\nNote: In the future I‚Äôll use the notation \\(\\mathbf{X}_{[a \\times b]}\\) to denote the dimensions of the vectors/matrices, like \\(\\mathbf{X}_{[k \\times 1]} \\sim \\boldsymbol{\\mathcal{N}}_k(\\boldsymbol{\\mu}_{[k \\times 1]}, \\mathbf{\\Sigma}_{[k \\times k]})\\)"
  },
  {
    "objectID": "w02/slides.html#visualizing-3d-distributions-projection",
    "href": "w02/slides.html#visualizing-3d-distributions-projection",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Visualizing 3D Distributions: Projection",
    "text": "Visualizing 3D Distributions: Projection\n\nSince most of our intuitions about plots come from 2D plots, it is extremely useful to be able to take a 3D plot like this and imagine ‚Äúprojecting‚Äù it down into different 2D plots:\n\n\n(Adapted via LaTeX from StackExchange discussion)"
  },
  {
    "objectID": "w02/slides.html#visualizing-3d-distributions-contours",
    "href": "w02/slides.html#visualizing-3d-distributions-contours",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Visualizing 3D Distributions: Contours",
    "text": "Visualizing 3D Distributions: Contours\n\nFrom Prof.¬†Hickman‚Äôs slides!"
  },
  {
    "objectID": "w02/slides.html#visualizing-3d-distributions-contours-1",
    "href": "w02/slides.html#visualizing-3d-distributions-contours-1",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Visualizing 3D Distributions: Contours",
    "text": "Visualizing 3D Distributions: Contours\n\n\n\n\n\nAlso from Prof.¬†Hickman‚Äôs slides!"
  },
  {
    "objectID": "w02/index.html",
    "href": "w02/index.html",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "",
    "text": "Open slides in new window ‚Üí"
  },
  {
    "objectID": "w02/index.html#prof.-jeff-introduction",
    "href": "w02/index.html#prof.-jeff-introduction",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Prof.¬†Jeff Introduction!",
    "text": "Prof.¬†Jeff Introduction!\n\nBorn and raised in NW DC ‚Üí high school in Rockville, MD\nUniversity of Maryland: Computer Science, Math, Economics (2008-2012)"
  },
  {
    "objectID": "w02/index.html#grad-school",
    "href": "w02/index.html#grad-school",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Grad School",
    "text": "Grad School\n\nStudied abroad in Beijing (Peking University/ÂåóÂ§ß) ‚Üí internship with Huawei in Hong Kong (HKUST)\n\n\n\n\nStanford for MS in Computer Science (2012-2014)\nResearch Economist at UC Berkeley (2014-2015)\n\n\n\n\n\n\nColumbia (NYC) for PhD[+Postdoc] in Political Science (2015-2023)"
  },
  {
    "objectID": "w02/index.html#dissertation-political-science-history",
    "href": "w02/index.html#dissertation-political-science-history",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Dissertation (Political Science + History)",
    "text": "Dissertation (Political Science + History)\n‚ÄúOur Word is Our Weapon‚Äù: Text-Analyzing Wars of Ideas from the French Revolution to the First Intifada"
  },
  {
    "objectID": "w02/index.html#research-labor-economics",
    "href": "w02/index.html#research-labor-economics",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Research (Labor Economics)",
    "text": "Research (Labor Economics)\n\n\n\n‚ÄúMonopsony in Online Labor Markets‚Äù: Machine Learning to enhance causal estimates of the effect of job description language on uptake rate\n\n\n\n‚ÄúFreedom as Non-Domination in the Labor Market‚Äù: Game-theoretic models of workers‚Äô rights (monopsony vs.¬†labor discipline)\n\n\n\n\n\n‚ÄúUnsupervised Extraction of Workplace Rights and Duties from Collective Bargaining Agreements‚Äù: Linguistic (dependency) parses of contracts ‚Üí time series of worker vs.¬†employer rights and responsibilities over time"
  },
  {
    "objectID": "w02/index.html#deterministic-processes",
    "href": "w02/index.html#deterministic-processes",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Deterministic Processes",
    "text": "Deterministic Processes\n\nGiven a set of inputs, we can compute the outcome exactly\nExample: Given the radius of a circle, we can compute its area without any uncertainty. \\(r \\mapsto \\pi r^2\\)\n(The fact that we can compute the outcome doesn‚Äôt mean that it‚Äôs easy to do so! See, e.g., the double pendulum)\n\n\n\n\nImage credit: Tenor.com\n\n\n\nThe pendulum example points to the fact that the notion of a chaotic system, one which is ‚Äúsensitive to initial conditions‚Äù, is different from that of a stochastic system."
  },
  {
    "objectID": "w02/index.html#holy-grail-deterministic-model-newtonian-physics",
    "href": "w02/index.html#holy-grail-deterministic-model-newtonian-physics",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "‚ÄúHoly Grail‚Äù Deterministic Model: Newtonian Physics",
    "text": "‚ÄúHoly Grail‚Äù Deterministic Model: Newtonian Physics\n\n\n\n\n\n\n\n\n\n\ngrid\n\n \n\ncluster_01\n\n ‚ÄúNature‚Äù  \n\ncluster_02\n\n ‚ÄúScience‚Äù   \n\nObs\n\n Thing(s) we can see   \n\nUnd\n\n Underlying processes   \n\nUnd-&gt;Obs\n\n    \n\nModel\n\n Model   \n\nUnd-&gt;Model\n\n    \n\nModel-&gt;Obs\n\n   \n\n\n\n\n\n\n\n\n\n\n\n\n\ngrid\n\n \n\ncluster_03\n\n Isaac Newton  \n\ncluster_04\n\n  Woolsthorpe Manor     \n\nTree\n\n  Falling Apple     \n\nPhysics\n\n  Particle Interactions     \n\nPhysics-&gt;Tree\n\n    \n\nNewton\n\n Newton‚Äôs Laws   \n\nPhysics-&gt;Newton\n\n    \n\nNewton-&gt;Tree\n\n   \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\n\\leadsto F_g = G\\frac{m_1m_2}{r^2}\n\\]\nFigure¬†1: Newton‚Äôs Law of Universal Gravitation‚Üê Dr.¬†Zirkel follows Newton‚Äôs famous steps. Coloured wood engraving. Wellcome Collection (Public Domain)"
  },
  {
    "objectID": "w02/index.html#but-what-happens-when",
    "href": "w02/index.html#but-what-happens-when",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "But What Happens When‚Ä¶",
    "text": "But What Happens When‚Ä¶\n\\[\n\\text{Outcome}\\left(\\text{Dice Roll}\\right) = \\; ?\\frac{?_1?_2}{?^2}\n\\]\n\n\n\nPre-Enlightenment\n\n\n\n\nHans Sebald Beham, Fortuna (1541), CC BY 4.0, via Wikimedia Commons\n\n\n\n\nPost-Enlightenment\n\n\n\n\nBlaise Pascal, Trait√© du triangle arithm√©tique (1665). Public Domain, via Internet Archive"
  },
  {
    "objectID": "w02/index.html#random-processes",
    "href": "w02/index.html#random-processes",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Random Processes",
    "text": "Random Processes\n\n\n\nCan‚Äôt compute the outcome exactly, but can still say something about potential outcomes!\nExample: randomly chosen radius \\(r \\in [0,1]\\), what can we say about \\(A = \\pi r^2\\)?\n\nUnif: \\([0,\\pi]\\) equally likely\nExp: closer to \\(0\\) more likely\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(ggplot2)\nN &lt;- 1000\nradii &lt;- rexp(N, 4)\ntitle &lt;- paste0(N, \" Exponentially-Distributed Radii\")\nplot_circ_with_distr(N, radii, title, alpha=0.15)\n\nWarning: Removed 1456 rows containing missing values (`geom_path()`)."
  },
  {
    "objectID": "w02/index.html#data-ground-truth-noise",
    "href": "w02/index.html#data-ground-truth-noise",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Data = Ground Truth + Noise",
    "text": "Data = Ground Truth + Noise\n\nDepressing but true origin of statistics (as opposed to probability): the Plague üò∑\n\n\n\n\n\n\n\nGround Truth: The Great Plague (Lord Have Mercy on London, Unknown Artist, circa 1665, via Wikimedia Commons)\n\n\n\n\n\n\n\nNoisy Data (Recorded amidst chaos): London Bill of Mortality, 1665 (Public Domain, Wellcome Collection)"
  },
  {
    "objectID": "w02/index.html#random-variables",
    "href": "w02/index.html#random-variables",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Random Variables",
    "text": "Random Variables\n\nIn algebra, to solve problems we work with variables\nIn probability theory, to solve problems we work with random variables\nRecall the difference between random and deterministic: \\(A = \\pi r^2\\) tells us that, given a value of \\(r\\), we can solve for the unique value of \\(A\\)\nIn probability theory, however, there is no one ‚Äútrue‚Äù value of a random variable \\(X\\).\nLet \\(X = f(N)\\) mean that \\(X\\) is the result of a rolled die, where the die has \\(N\\) sides.\nPlugging in \\(N = 6\\) (standard 6-sided die) still doesn‚Äôt mean we know ‚Äúthe‚Äù value of \\(X\\). However, (if the die is fair) we do know\n\n\\[\n\\Pr(X = 1) = \\Pr(X = 2) = \\cdots = \\Pr(X = 6) = \\frac{1}{6}\n\\]"
  },
  {
    "objectID": "w02/index.html#discrete-vs.-continuous",
    "href": "w02/index.html#discrete-vs.-continuous",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Discrete vs.¬†Continuous",
    "text": "Discrete vs.¬†Continuous\n\nMany complicated definitions, often misleading or unintuitive!\nHow I want you to remember: How many possible values between two known values?\nDiscrete: e.g., number of siblings\n\nI have 2 siblings, you have 3 siblings‚Ä¶ How many values (sibling counts) in between?\n\nContinuous: e.g., temperature\n\nIt is 27.0¬∞ C in my room, 28.0¬∞ C in your room‚Ä¶ How many values (temperatures) in between?\n\nSo, if \\(X\\) is the result of a rolled die, is \\(X\\) discrete or continuous? How many values can be rolled between 3 and 4?"
  },
  {
    "objectID": "w02/index.html#thinking-about-independence",
    "href": "w02/index.html#thinking-about-independence",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Thinking About Independence",
    "text": "Thinking About Independence\n\nWe‚Äôll define it formally later; for now, this is our working definition:\n\n\n\n\n\n\n\nWorking Definition: Independence\n\n\n\nTwo random variables \\(X\\) and \\(Y\\) are independent if learning information about \\(X\\) does not give you information about the value of \\(Y\\), or vice-versa."
  },
  {
    "objectID": "w02/index.html#na√Øve-definition-of-probability",
    "href": "w02/index.html#na√Øve-definition-of-probability",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Na√Øve Definition of Probability",
    "text": "Na√Øve Definition of Probability\n\nSample Space: The set of all possible outcomes of an experiment\nEvent: A subset of the sample space\n\n\n\n\n\n\n\nNa√Øve Definition of Probability\n\n\n\nGiven a sample space \\(S\\), and an event \\(E \\subset S\\),\n\\[\n\\Pr(\\underbrace{E}_{\\text{event}}) = \\frac{\\text{\\# Favorable Outcomes}}{\\text{\\# Possible Outcomes}} = \\frac{|E|}{|S|}\n\\]"
  },
  {
    "objectID": "w02/index.html#example-flipping-two-coins",
    "href": "w02/index.html#example-flipping-two-coins",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Example: Flipping Two Coins",
    "text": "Example: Flipping Two Coins\n\n\n\n\n\n\nNa√Øve Definition of Probability\n\n\n\nGiven a sample space \\(S\\), and an event \\(E \\subset S\\),\n\\[\n\\Pr(\\underbrace{E}_{\\text{event}}) = \\frac{\\text{\\# Favorable Outcomes}}{\\text{\\# Possible Outcomes}} = \\frac{|E|}{|S|}\n\\]\n\n\n\nFlipping two coins:\n\nSample space \\(S = \\{TT, TH, HT, HH\\}\\)\nEvent \\(E_1\\): Result of first flip is \\(H\\), result of second flip is \\(T\\) \\(\\implies\\) \\(E_1 = \\{HT\\}\\).\nEvent \\(E_2\\): At least one \\(H\\) \\(\\implies\\) \\(E_2 = \\{TH, HT, HH\\}\\).\n\n\n\\[\n\\begin{align*}\n\\Pr(E_1) &= \\frac{|\\{HT\\}|}{|S|} = \\frac{|\\{HT\\}|}{|\\{TT, TH, HT, HH\\}|} = \\frac{1}{4} \\\\\n\\Pr(E_2) &= \\frac{|\\{TH, HT, HH\\}|}{|S|} = \\frac{|\\{TH, HT, HH\\}|}{|\\{TT, TH, HT, HH\\}|} = \\frac{3}{4}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "w02/index.html#events-neq-outcomes",
    "href": "w02/index.html#events-neq-outcomes",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Events \\(\\neq\\) Outcomes!",
    "text": "Events \\(\\neq\\) Outcomes!\n\nOutcomes are things, events are sets of things\nSubtle but extremely important distinction!\nIn the coin flip example:\n\nThe event \\(E_1 = \\{HT\\}\\) can be confused with the outcome \\(HT\\).\nSo, try to remember instead the event \\(E_2 = \\{TH, HT, HH\\}\\): it is more clear, in this case, how this event does not correspond to any individual outcome"
  },
  {
    "objectID": "w02/index.html#back-to-the-na√Øve-definition",
    "href": "w02/index.html#back-to-the-na√Øve-definition",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Back to the Na√Øve Definition",
    "text": "Back to the Na√Øve Definition\n\n\n\n\n\n\nNa√Øve Definition of Probability\n\n\n\nGiven a sample space \\(S\\), and an event \\(E \\subset S\\),\n\\[\n\\Pr(\\underbrace{E}_{\\text{event}}) = \\frac{\\text{\\# Favorable Outcomes}}{\\text{\\# Possible Outcomes}} = \\frac{|E|}{|S|}\n\\]\n\n\n\nThe na√Øve definition tells us that probabilities are just ratios of counts:\n\nCount the number of ways the event \\(E\\) can happen, count the total number of things that can happen, and divide!\n\nThis is why we begin studying probability by studying combinatorics: the mathematics of counting"
  },
  {
    "objectID": "w02/index.html#combinatorics-ice-cream-possibilities",
    "href": "w02/index.html#combinatorics-ice-cream-possibilities",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Combinatorics: Ice Cream Possibilities",
    "text": "Combinatorics: Ice Cream Possibilities\n\n\n\n\n\n\n\nThe \\(6 = 2 \\cdot 3\\) possible cone+flavor combinations which can result from choosing a flavor first and a cone type second.\n\n\n\n\n\n\n\nThe \\(6 = 3 \\cdot 2\\) possible cone+flavor combinations which can result from choosing a flavor first and a cone type second."
  },
  {
    "objectID": "w02/index.html#grouping-vs.-ordering",
    "href": "w02/index.html#grouping-vs.-ordering",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Grouping vs.¬†Ordering",
    "text": "Grouping vs.¬†Ordering\n\nIn standard statistics/combinatorics introductions you‚Äôll learn different counting formulas for when order matters vs.¬†when order doesn‚Äôt matter\nThis is not a mathematical distinction so much as a pragmatic distinction: what are you trying to accomplish by counting?\nProblems with extremely similar descriptions can differ in small detail, so that the units you need to distinguish between in one version differ from the units you need to distinguish between in the other."
  },
  {
    "objectID": "w02/index.html#does-order-matter",
    "href": "w02/index.html#does-order-matter",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Does Order Matter?",
    "text": "Does Order Matter?\n\n\n\n\n\n\nExample: Student Government vs.¬†Student Sports\n\n\n\n\nConsider a school where students can either try out for the swim team or run for a position in the student government\nThe swim team has 4 slots, but slots aren‚Äôt differentiated: you‚Äôre either on the team (one of the 4 chosen students) or not\nThe student government also has 4 slots, but there is a difference between the slots: first slot is President, second is Vice President, third is Secretary, and fourth is Treasurer.\n\n\n\n\nSimple case (for intuition): the school only has 4 students. In this case, how many ways are there to form the swim team? What about the student government?\n\nSwim team: \\(1\\) way. You have only one choice, to let all 4 students onto the team\nStudent government: \\(4 \\cdot 3 \\cdot 2 \\cdot 1 = 24\\) ways. You have to let all 4 students in, but you have a choice of who is President, Vice President, Secretary, and Treasurer\n\nHow did we get \\(4 \\cdot 3 \\cdot 2 \\cdot 1\\)? (Think about the ice cream example‚Ä¶)\n\nStart by choosing the President: 4 choices\nNow choose the Vice President: only 3 students left to choose from\nNow choose the Secretary: only 2 students left to choose from\nNow choose the Treasurer: only 1 student left to choose from"
  },
  {
    "objectID": "w02/index.html#permutations-vs.-combinations",
    "href": "w02/index.html#permutations-vs.-combinations",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Permutations vs.¬†Combinations",
    "text": "Permutations vs.¬†Combinations\n\nPermutations: How many ways can I choose groups of size \\(k\\) out of \\(n\\) total objects, where order within groups matters: \\(P_{n,k}\\) (sometimes written \\(_nP_k\\)).\n\nIn this case, we want to count \\((a,b)\\) and \\((b,a)\\) as two separate groups\n\nCombinations: How many ways can I choose groups of size \\(k\\) out of \\(n\\) total objects, where order in the groups doesn‚Äôt matter: \\(C_{n,k}\\) (sometimes written \\(_nC_k,\\binom{n}{k}\\)).\n\nIn this case, we don‚Äôt want to count \\((a, b)\\) and \\((b, a)\\) as two separate groups‚Ä¶\n\n\n\\[\n\\begin{align*}\nP_{n,k} = \\frac{n!}{(n-k)!}, \\; C_{n,k} = \\frac{n!}{k!(n-k)!}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "w02/index.html#no-need-to-memorize",
    "href": "w02/index.html#no-need-to-memorize",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "No Need to Memorize!",
    "text": "No Need to Memorize!\n\n\nKey point: you don‚Äôt have to remember these as two separate formulas!\nThe number of combinations is based on the number of permutations, but corrected for double counting: e.g., corrected for the fact that \\((a,b) \\neq (b,a)\\) when counting permutations but \\((a,b) = (b,a)\\) when counting combinations.\n\n\\[\nC_{n,k} = \\frac{P_{n,k}}{k!} \\genfrac{}{}{0pt}{}{\\leftarrow \\text{Permutations}}{\\leftarrow \\text{Duplicate groups}}\n\\]\nWhere does \\(k!\\) come from? (How many different orderings can we make of the same group?)\n\n\\(k = 2\\): \\((\\underbrace{\\boxed{\\phantom{a}}}_{\\text{2 choices}},\\underbrace{\\boxed{\\phantom{a}}}_{\\text{1 remaining choice}}) \\implies 2\\)\n\\(k = 3\\): \\((\\underbrace{\\boxed{\\phantom{a}}}_{\\text{3 choices}},\\underbrace{\\boxed{\\phantom{a}}}_{\\text{2 remaining choices}}, \\underbrace{\\boxed{\\phantom{a}}}_{\\text{1 remaining choice}}) \\implies 6\\)\n\\(k = 4\\): \\((\\underbrace{\\boxed{\\phantom{a}}}_{\\text{4 choices}}, \\underbrace{\\boxed{\\phantom{a}}}_{\\text{3 remaining choices}}, \\underbrace{\\boxed{\\phantom{a}}}_{\\text{2 remaining choices}}, \\underbrace{\\boxed{\\phantom{a}}}_{\\text{1 remaining choice}}) \\implies 24\\)"
  },
  {
    "objectID": "w02/index.html#with-or-without-replacement",
    "href": "w02/index.html#with-or-without-replacement",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "With or Without Replacement?",
    "text": "With or Without Replacement?\n\nBoils down to: can the same object be included in my sample more than once?\n\n\n\n\nWithout Replacement\nWith Replacement\n\n\n\n\nMost statistical problems: ‚ÄúCheck off‚Äù objects as you collect data about them, so that each observation in your data is unique\nVery special (but very important!) set of statistical problems: allow objects to appear in your sample multiple times, to ‚Äúsqueeze‚Äù more information out of the sample (called Bootstrapping‚Äîmuch more on this later in the course!)"
  },
  {
    "objectID": "w02/index.html#how-many-possible-samples",
    "href": "w02/index.html#how-many-possible-samples",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "How Many Possible Samples?",
    "text": "How Many Possible Samples?\n\n\n\n\n\n\nExample: How Many Possible Samples?\n\n\n\nFrom a population of \\(N = 3\\), how many ways can we take samples of size \\(k = 2\\)?\n\n\n\n\n\n\n\nWithout Replacement\nWith Replacement\n\n\n\n\n\\(3 \\cdot 2 = 6\\) ways (3 objects to choose from for first element of sample, 2 remaining objects to choose from for second element of sample)\n\\(3\\cdot 3 = 3^2 = 9\\) ways (3 objects to choose from for first element of sample, still 3 objects to choose from for second element of sample)\n\n\n\n\n\n\n\n\n\n\n\nResult: How Many Possible Samples\n\n\n\nFrom a population of size \\(N\\), how many ways can we take samples of size \\(k\\)? (Try to extrapolate from above examples before looking at answer!)\n\n\n\n\n\n\n\nWithout Replacement\nWith Replacement\n\n\n\n\n\\(\\displaystyle \\underbrace{N \\cdot (N-1) \\cdot \\cdots \\cdot (N - k + 1)}_{k\\text{ times}} = \\frac{N!}{(N - k )!}\\)(This formula should look somewhat familiar‚Ä¶)\n\\(\\displaystyle \\underbrace{N \\cdot N \\cdot \\cdots \\cdot N}_{k\\text{ times}} = N^k\\)"
  },
  {
    "objectID": "w02/index.html#logic-sets-and-probability",
    "href": "w02/index.html#logic-sets-and-probability",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Logic, Sets, and Probability",
    "text": "Logic, Sets, and Probability\n\nThere is a deep connection1 between the objects and operations of logic, set theory, and probability:\n\n\n\n\n\n\n\n\n\n\n\nLogic\nSet Theory\nProbability Theory\n\n\n\n\nObjects\nPredicates\\(p, q \\in \\{T, F\\}\\)\nSets\\(S = \\{a, b, \\ldots\\}\\)\nEvents\\(E = \\{TH, HT, HH\\}\\)\n\n\nConjunction\nAnd (\\(\\wedge\\))\\(p \\wedge q\\)\nIntersection (\\(\\cap\\))\\(A \\cap B\\)\nMultiplication (\\(\\times\\)):\\(\\Pr(E_1 \\cap E_2) = \\Pr(E_1)\\times \\Pr(E_2)\\)\n\n\nDisjunction\nOr (\\(\\vee\\))\\(p \\vee q\\)\nUnion (\\(\\cup\\))\\(A \\cup B\\)\nAddition (\\(+\\)): \\(\\Pr(E_1 \\cup E_2) =\\)\\(\\Pr(E_1) + \\Pr(E_2) - \\Pr(E_1 \\wedge E_2)\\)\n\n\nNegation\nNot (\\(\\neg\\))\\(\\neg p\\)\nComplement (\\(^c\\))\\(S^c\\)\nSubtract from 1\\(\\Pr(A^c) = 1 - \\Pr(A)\\)"
  },
  {
    "objectID": "w02/index.html#example-flipping-two-coins-1",
    "href": "w02/index.html#example-flipping-two-coins-1",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Example: Flipping Two Coins",
    "text": "Example: Flipping Two Coins\n\nLogic: We can define 4 predicates:\n\n\\(p_1\\) = ‚ÄúFirst result is \\(H\\)‚Äù, \\(q_1\\) = ‚ÄúFirst result is \\(T\\)‚Äù\n\\(p_2\\) = ‚ÄúSecond result is \\(H\\)‚Äù, \\(q_2\\) = ‚ÄúSecond result is \\(T\\)‚Äù\n\nLogical formulas:\n\n\\(f_1 = p_1 \\wedge q_2\\): ‚ÄúFirst result is \\(H\\) and second result is \\(T\\)‚Äù\n\\(f_2 = p_1 \\vee q_2\\): ‚ÄúFirst result is \\(H\\) or second result is \\(T\\)‚Äù\n\\(f_3 = \\neg p_1\\): ‚ÄúFirst result is not \\(H\\)‚Äù\n\nThe issue?: We don‚Äôt know, until after the coins have been flipped, whether these are true or false!\nBut, we should still be able to say something about their likelihood, for example, whether \\(f_1\\) or \\(f_2\\) is more likely to happen‚Ä¶ Enter probability theory!"
  },
  {
    "objectID": "w02/index.html#logic-rightarrow-probability",
    "href": "w02/index.html#logic-rightarrow-probability",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Logic \\(\\rightarrow\\) Probability",
    "text": "Logic \\(\\rightarrow\\) Probability\n\nProbability theory lets us reason about the uncertainty surrounding logical predicates like \\(p\\) and \\(q\\), by:\n\nencoding them as sets of possibilities \\(P\\) and \\(Q\\), and\nrepresenting the uncertainty around any given possibility using a probability measure \\(\\Pr: S \\mapsto [0,1]\\),\n\nthus allowing us to reason about\n\nthe likelihood of these set-encoded predicates on their own: \\(\\Pr(P)\\) and \\(\\Pr(Q)\\), but also\ntheir logical connections: \\(\\Pr(p \\wedge q) = \\Pr(P \\cap Q)\\), \\(\\Pr(\\neg p) = \\Pr(P^c)\\), and so on."
  },
  {
    "objectID": "w02/index.html#flipping-two-coins-logic-rightarrow-probability",
    "href": "w02/index.html#flipping-two-coins-logic-rightarrow-probability",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Flipping Two Coins: Logic \\(\\rightarrow\\) Probability",
    "text": "Flipping Two Coins: Logic \\(\\rightarrow\\) Probability\n\nReturning to the two coins example: we can look at the predicates and see that they exhaust all possibilities, so that we can define a sample space \\(S = \\{TT, TH, HT, HH\\}\\) of all possible outcomes of our coin-flipping experiment, noting that \\(|S| = 4\\), so there are 4 possible outcomes.\nThen we can associate each predicate with an event, a subset of the sample space, and use our na√Øve definition to compute the probability of these events:\n\n\n\n\n\n\n\n\n\nPredicate\nEvent\nProbability\n\n\n\n\n\\(p_1\\) = ‚ÄúFirst result is \\(H\\)‚Äù\n\\(P_1 = \\{HT, HH\\}\\)\n\\(\\Pr(P_1) = \\frac{|P_1|}{|S|} = \\frac{2}{4} = \\frac{1}{2}\\)\n\n\n\\(q_1\\) = ‚ÄúFirst result is \\(T\\)‚Äù\n\\(Q_1 = \\{TT, TH\\}\\)\n\\(\\Pr(Q_1) = \\frac{|Q_1|}{|S|} = \\frac{2}{4} = \\frac{1}{2}\\)\n\n\n\\(p_2\\) = ‚ÄúSecond result is \\(H\\)‚Äù\n\\(P_2 = \\{TH, HH\\}\\)\n\\(\\Pr(P_2) = \\frac{|P_2|}{|S|} = \\frac{2}{4} = \\frac{1}{2}\\)\n\n\n\\(q_2\\) = ‚ÄúSecond result is \\(T\\)‚Äù\n\\(Q_2 = \\{TT, HT\\}\\)\n\\(\\Pr(Q_2) = \\frac{|Q_2|}{|S|} = \\frac{2}{4} = \\frac{1}{2}\\)"
  },
  {
    "objectID": "w02/index.html#moving-from-predicates-to-formulas",
    "href": "w02/index.html#moving-from-predicates-to-formulas",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Moving from Predicates to Formulas",
    "text": "Moving from Predicates to Formulas\n\nNotice that, in the four rows of the previous table, we were only computing the probabilities of ‚Äúsimple‚Äù events: events corresponding to a single predicate\nBut we promised that probability theory lets us compute probabilities for logical formulas as well! ‚Ä¶The magic of encoding events as sets becomes clear:\n\n\n\n\n\n\n\n\n\nFormula\nEvent\nProbability\n\n\n\n\n\\(f_1 = p_1 \\wedge q_2\\)\n\\[ \\begin{align*} F_1 &= P_1 \\cap Q_2 \\\\ &= \\{HT, HH\\} \\cap \\{TT, HT\\} \\\\ &= \\{HT\\} \\end{align*} \\]\n\\[\\begin{align*} \\Pr(F_1) &= \\Pr(\\{HT\\}) \\\\ &= \\frac{|\\{HT\\}|}{|S|} = \\frac{1}{4} \\end{align*}\\]\n\n\n\\(f_2 = p_1 \\vee q_2\\)\n\\[\\begin{align*} F_2 &= P_1 \\cup Q_2 \\\\ &= \\{HT, HH\\} \\cup \\{TT, HT\\} \\\\ &= \\{TT, HT, HH\\} \\end{align*}\\]\n\\[\\begin{align*} \\Pr(F_2) &= \\Pr(\\{TT, HT, HH\\}) \\\\ &= \\frac{|\\{TT, HT, HH\\}|}{|S|} = \\frac{3}{4} \\end{align*}\\]\n\n\n\\(f_3 = \\neg p_1\\)\n\\[\\begin{align*} F_3 &= P_1^c \\\\ &= \\{HT, HH\\}^c \\\\ &= \\{TT, TH\\}\\end{align*}\\]\n\\[\\begin{align*} \\Pr(F_3) &= \\Pr(\\{TT, TH\\}) \\\\ &= \\frac{|\\{TT, TH\\}|}{|S|} = \\frac{2}{4} = \\frac{1}{2} \\end{align*}\\]"
  },
  {
    "objectID": "w02/index.html#using-rules-of-probability",
    "href": "w02/index.html#using-rules-of-probability",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Using ‚ÄúRules‚Äù of Probability",
    "text": "Using ‚ÄúRules‚Äù of Probability\n\nHopefully, though, you found all this churning through set theory to be a bit tedious‚Ä¶\nThis is where rules of probability come from! They simplify set-theoretic computations into simple multiplications, additions, and subtractions:\n\n\\(\\Pr(A \\cap B) = \\Pr(A) \\times \\Pr(B)\\)\n\\(\\Pr(A \\cup B) = \\Pr(A) + \\Pr(B) - \\Pr(A \\cap B)\\)\n\\(\\Pr(A^c) = 1 - \\Pr(A)\\)\n\nSince we know probabilities of the ‚Äúsimple‚Äù events \\(P_1\\), \\(Q_1\\), \\(P_2\\), \\(Q_2\\), we don‚Äôt need to ‚Äúlook inside them‚Äù! Just take the probabilities and multiply/add/subtract as needed:\n\n\n\n\n\n\n\n\n\nFormula\nEvent\nProbability\n\n\n\n\n\\(f_1 = p_1 \\wedge q_2\\)\n\\(F_1 = P_1 \\cap Q_2\\)\n\\(\\Pr(F_1) = \\Pr(P_1) \\times \\Pr(Q_2) = \\frac{1}{2}\\times \\frac{1}{2} = \\frac{1}{4}\\)\n\n\n\\(f_2 = p_1 \\vee q_2\\)\n\\(F_2 = P_1 \\cup Q_2\\)\n\\[\\textstyle{\\begin{align*} \\textstyle \\Pr(F_2) &= \\Pr(P_1) + \\Pr(Q_2) - \\Pr(P_1 \\cap Q_2) \\\\ \\textstyle &= \\frac{1}{2} + \\frac{1}{2} - \\frac{1}{4} = \\frac{3}{4} \\end{align*}}\\]\n\n\n\\(f_3 = \\neg p_1\\)\n\\(F_3 = P_1^c\\)\n\\(\\Pr(F_3) = 1 - \\Pr(P_1) = 1 - \\frac{1}{2} = \\frac{1}{2}\\)"
  },
  {
    "objectID": "w02/index.html#importing-results-from-logic",
    "href": "w02/index.html#importing-results-from-logic",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "‚ÄúImporting‚Äù Results from Logic",
    "text": "‚ÄúImporting‚Äù Results from Logic\n\nThis deep connection between the three fields means that, if we have some useful theorem or formula from one field, we can immediately put it to use in another!\nFor example: DeMorgan‚Äôs Laws were developed in logic (DeMorgan was a 19th-century logician), and basically just tell us how to distribute logic operators:\n\n\\[\n\\begin{align*}\n\\underbrace{\\neg(p \\wedge q)}_{\\text{``}p\\text{ and }q\\text{'' is not true}} &\\iff \\underbrace{\\neg p \\vee \\neg q}_{p\\text{ is not true or }q\\text{ is not true}} \\\\\n\\underbrace{\\neg(p \\vee q)}_{\\text{``}p\\text{ or }q\\text{'' is not true}} &\\iff \\underbrace{\\neg p \\wedge \\neg q}_{p\\text{ is not true and }q\\text{ is not true}}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "w02/index.html#converting-to-probability-theory",
    "href": "w02/index.html#converting-to-probability-theory",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Converting to Probability Theory",
    "text": "Converting to Probability Theory\n\nSo, using the same principles we used in our coin flipping examples, we can consider events \\(P\\) and \\(Q\\), and get the following ‚Äútranslation‚Äù of DeMorgan‚Äôs Laws:\n\n\n\n\n\n\n\n\n\nLogic\nSet Theory\nProbability Theory\n\n\n\n\n\\(\\neg(p \\wedge q) = \\neg p \\vee \\neg q\\)\n\\((P \\cap Q)^c = P^c \\cup Q^c\\)\n\\(\\Pr((P \\cap Q)^c) = \\Pr(P^c \\cup Q^c)\\)\n\n\n\\(\\neg(p \\vee q) = \\neg p \\wedge \\neg q\\)\n\\((P \\cup Q)^c = P^c \\cap Q^c\\)\n\\(\\Pr((P \\cup Q)^c) = \\Pr(P^c \\cap Q^c)\\)\n\n\n\n\nNote that, since these are isomorphic to one another, we could have derived DeMorgan‚Äôs Laws from within probability theory, rather than the other way around:\n\n\\[\n\\begin{align*}\n\\Pr((P \\cap Q)^c) &= 1 - \\Pr(P \\cap Q) = 1 - \\Pr(P)\\Pr(Q) \\\\\n&= 1 - (1-\\Pr(P^c))(1 - \\Pr(Q^c)) \\\\\n&= 1 - [1 - \\Pr(P^c) - \\Pr(Q^c) + \\Pr(P^c)\\Pr(Q^c)] \\\\\n&= \\Pr(P^c) + \\Pr(Q^c) - \\Pr(P^c)\\Pr(Q^c) \\\\\n&= \\Pr(P^c) + \\Pr(Q^c) - \\Pr(P^c \\cap Q^c) \\\\\n&= \\Pr(P^c \\cup Q^c) \\; ‚úÖ\n\\end{align*}\n\\]"
  },
  {
    "objectID": "w02/index.html#random-variables-1",
    "href": "w02/index.html#random-variables-1",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Random Variables",
    "text": "Random Variables\n\nRecall our discussion of random variables: used by analogy to algebra, since we can do math with them:\nJust as \\(2 \\cdot 3\\) is shorthand for \\(2 + 2 + 2\\), we can define \\(X\\) as shorthand for the possible outcomes of a random process. \\[\n\\begin{align*}\nS = \\{ &\\text{result of dice roll is 1}, \\\\\n&\\text{result of dice roll is 2}, \\\\\n&\\text{result of dice roll is 3}, \\\\\n&\\text{result of dice roll is 4}, \\\\\n&\\text{result of dice roll is 5}, \\\\\n&\\text{result of dice roll is 6}\\} \\rightsquigarrow X \\in \\{1,\\ldots,6\\}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "w02/index.html#random-variables-as-events",
    "href": "w02/index.html#random-variables-as-events",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Random Variables as Events",
    "text": "Random Variables as Events\n\nEach value \\(v_X\\) that a random variable \\(X\\) can take on gives rise to an event \\(X = v_X\\): the event that the random variable \\(X\\) takes on value \\(v\\).\nSince \\(X = v_X\\) is an event, we can compute its probability \\(\\Pr(X = v_X)\\)!\n\n\n\n\nEvent in words\nEvent in terms of RV\n\n\n\n\nResult of dice roll is 1\n\\(X = 1\\)\n\n\nResult of dice roll is 2\n\\(X = 2\\)\n\n\nResult of dice roll is 3\n\\(X = 3\\)\n\n\nResult of dice roll is 4\n\\(X = 4\\)\n\n\nResult of dice roll is 5\n\\(X = 5\\)\n\n\nResult of dice roll is 6\n\\(X = 6\\)"
  },
  {
    "objectID": "w02/index.html#doing-math-with-events",
    "href": "w02/index.html#doing-math-with-events",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Doing Math with Events",
    "text": "Doing Math with Events\n\nWe‚Äôve seen how \\(\\Pr(\\cdot)\\) can ‚Äúencode‚Äù logical expressions involving uncertain outcomes.\nEven more powerful when paired with the notion of random variables: lets us also ‚Äúencode‚Äù mathematical expressions involving uncertain quantities!\nConsider an experiment where we roll two dice. Let \\(X\\) be the RV encoding the outcome of the first roll, and \\(Y\\) be the RV encoding the outcome of the second roll.\nWe can compute probabilities involving \\(X\\) and \\(Y\\) separately, e.g., \\(\\Pr(X = 1) = \\frac{1}{6}\\), but we can also reason probabilistically about mathematical expressions involving \\(X\\) and \\(Y\\)! For example, we can reason about their sum:\n\n\\[\n\\begin{align*}\n\\Pr(\\text{rolls sum to 10}) &= \\Pr(X + Y = 10) \\\\\n&= \\Pr(Y = 10 - X)\n\\end{align*}\n\\]\n\nOr about how the outcome of one roll will relate to the outcome of the other:\n\n\\[\n\\begin{align*}\n\\Pr(\\text{first roll above mean}) &= \\Pr\\left(X &gt; \\frac{X+Y}{2}\\right) \\\\\n&= \\Pr(2X &gt; X+Y) = \\Pr(X &gt; Y)\n\\end{align*}\n\\]"
  },
  {
    "objectID": "w02/index.html#are-random-variables-all-powerful",
    "href": "w02/index.html#are-random-variables-all-powerful",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Are Random Variables All-Powerful??",
    "text": "Are Random Variables All-Powerful??\n\nJust remember that probability \\(P(\\cdot)\\) is always probability of an event‚Äîrandom variables are just shorthand for quantifiable events.\nNot all events can be simplified via random variables!\n\n\\(\\text{catch a fish} \\mapsto P(\\text{trout}), P(\\text{bass}), \\ldots\\)\n\nWhat types of events can be quantified like this?\n\n(Hint: It has to do with a key topic in the early weeks of both DSAN 5000 and 5100‚Ä¶)\n\n\n\nThe answer is, broadly, any situation where you‚Äôre modeling things, like dice rolls, where mathematical operations like addition, multiplication, etc. make sense. So, if we‚Äôre modeling dice, it makes sense to say e.g.¬†‚Äúresult is 6‚Äù + ‚Äúresult is 3‚Äù = ‚Äútotal is 9‚Äù. More on the next page!"
  },
  {
    "objectID": "w02/index.html#recall-types-of-variables",
    "href": "w02/index.html#recall-types-of-variables",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Recall: Types of Variables",
    "text": "Recall: Types of Variables\n\nCategorical\n\nNo meaningful way to order values: \\(\\{\\text{trout}, \\text{bass}, \\ldots \\}\\)\n\nOrdinal\n\nCan place in order (bigger, smaller), though gaps aren‚Äôt meaningful: \\(\\{{\\color{orange}\\text{great}},{\\color{orange}\\text{greater}},{\\color{orange}\\text{greatest}}\\}\\)\n\\({\\color{orange}\\text{greater}} \\overset{?}{=} 2\\cdot {\\color{orange}\\text{great}} - 1\\)\n\nCardinal\n\nCan place in order, and gaps are meaningful \\(\\implies\\) can do ‚Äústandard‚Äù math with them! Example: \\(\\{{\\color{blue}1},{\\color{blue}2},\\ldots,{\\color{blue}10}\\}\\)\n\\({\\color{blue}7}\\overset{\\color{green}\\unicode{x2714}}{=} 2 \\cdot {\\color{blue}4} - 1\\)\nIf events have this structure (meaningful way to define multiplication, addition, subtraction), then we can analyze them as random variables"
  },
  {
    "objectID": "w02/index.html#visualizing-discrete-rvs",
    "href": "w02/index.html#visualizing-discrete-rvs",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Visualizing Discrete RVs",
    "text": "Visualizing Discrete RVs\n\nUltimate Probability Pro-Tip: When you hear ‚Äúdiscrete distribution‚Äù, think of a bar graph: \\(x\\)-axis = events, bar height = probability of events\nTwo coins example: \\(X\\) = RV representing number of heads obtained in two coin flips\n\n\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.2     ‚úî readr     2.1.4\n‚úî forcats   1.0.0     ‚úî stringr   1.5.0\n‚úî lubridate 1.9.2     ‚úî tibble    3.2.1\n‚úî purrr     1.0.2     ‚úî tidyr     1.3.0\n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors"
  },
  {
    "objectID": "w02/index.html#preview-visualizing-continuous-rvs",
    "href": "w02/index.html#preview-visualizing-continuous-rvs",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "(Preview:) Visualizing Continuous RVs",
    "text": "(Preview:) Visualizing Continuous RVs\n\nThis works even for continuous distributions, if you focus on the area under the curve instead of the height:\n\n\nfuncShaded &lt;- function(x, lower_bound, upper_bound) {\n    y &lt;- dnorm(x)\n    y[x &lt; lower_bound | x &gt; upper_bound] &lt;- NA\n    return(y)\n}\nfuncShadedBound1 &lt;- function(x) funcShaded(x, -Inf, 0)\nfuncShadedBound2 &lt;- function(x) funcShaded(x, 0.2, 1.8)\nfuncShadedBound3 &lt;- function(x) funcShaded(x, 2, Inf)\n\nnorm_plot &lt;- ggplot(data.frame(x=c(-3,3)), aes(x = x)) +\n    stat_function(fun = dnorm) +\n    labs(\n      title=\"Probability Density, X Normally Distributed\",\n      x=\"Possible Values of X\",\n      y=\"Probability Density\"\n    ) +\n    dsan_theme(\"half\") +\n    theme(legend.position = \"none\") +\n    coord_cartesian(clip = \"off\")\nlabel_df &lt;- tribble(\n  ~x, ~y, ~label,\n  -0.8, 0.1, \"Pr(X &lt; 0) = 0.5\",\n  1.0, 0.05, \"Pr(0.2 &lt; X &lt; 1.8)\\n= 0.385\",\n  2.5,0.1,\"Pr(X &gt; 1.96)\\n= 0.025\"\n)\nshaded_plot &lt;- norm_plot +\n  stat_function(fun = funcShadedBound1, geom = \"area\", fill=cbPalette[1], alpha = 0.5) +\n  stat_function(fun = funcShadedBound2, geom = \"area\", fill=cbPalette[2], alpha = 0.5) +\n  stat_function(fun = funcShadedBound3, geom = \"area\", fill=cbPalette[3], alpha = 0.5) +\n  geom_text(label_df, mapping=aes(x = x, y = y, label = label), size=6)\nshaded_plot"
  },
  {
    "objectID": "w02/index.html#probability-theory-gives-us-distributions-for-rvs-not-numbers",
    "href": "w02/index.html#probability-theory-gives-us-distributions-for-rvs-not-numbers",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Probability Theory Gives Us Distributions for RVs, not Numbers!",
    "text": "Probability Theory Gives Us Distributions for RVs, not Numbers!\n\nWe‚Äôre going beyond ‚Äúbase‚Äù probability theory if we want to summarize these distributions\nHowever, we can understand a lot about the full distribution by looking at some basic summary statistics. Most common way to summarize:\n\n\n\n\n\n\n\n\n\n\\(\\underbrace{\\text{point estimate}}_{\\text{mean/median}}\\)\n\\(\\pm\\)\n\\(\\underbrace{\\text{uncertainty}}_{\\text{variance/standard deviation}}\\)"
  },
  {
    "objectID": "w02/index.html#example-game-reviews",
    "href": "w02/index.html#example-game-reviews",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Example: Game Reviews",
    "text": "Example: Game Reviews\n\nlibrary(readr)\nfig_title &lt;- \"Review for a Popular Nintendo Switch Game\"\nfig_subtitle &lt;- \"(That I definitely didn't play for &gt;400 hours this summer...)\"\n#score_df &lt;- read_csv(\"https://gist.githubusercontent.com/jpowerj/8b2b6a50cef5a682db640e874a14646b/raw/bbe07891a90874d1fe624224c1b82212b1ac8378/totk_scores.csv\")\nscore_df &lt;- read_csv(\"https://gist.githubusercontent.com/jpowerj/8b2b6a50cef5a682db640e874a14646b/raw/e3c2b9d258380e817289fbb64f91ba9ed4357d62/totk_scores.csv\")\n\nRows: 145 Columns: 1\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\ndbl (1): score\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nmean_score &lt;- mean(score_df$score)\nlibrary(ggplot2)\nggplot(score_df, aes(x=score)) +\n  geom_histogram() +\n  #geom_vline(xintercept=mean_score) +\n  labs(\n    title=fig_title,\n    subtitle=fig_subtitle,\n    x=\"Review Score\",\n    y=\"Number of Reviews\"\n  ) +\n  dsan_theme(\"full\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n(Data from Metacritic)"
  },
  {
    "objectID": "w02/index.html#adding-a-single-line",
    "href": "w02/index.html#adding-a-single-line",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Adding a Single Line",
    "text": "Adding a Single Line\n\nlibrary(readr)\nmean_score &lt;- mean(score_df$score)\nmean_score_label &lt;- sprintf(\"%0.2f\", mean_score)\nlibrary(ggplot2)\nggplot(score_df, aes(x=score)) +\n  geom_histogram() +\n  geom_vline(aes(xintercept=mean_score, linetype=\"dashed\"), color=\"purple\", size=1) +\n  scale_linetype_manual(\"\", values=c(\"dashed\"=\"dashed\"), labels=c(\"dashed\"=\"Mean Score\")) +\n  # Add single additional tick\n  scale_x_continuous(breaks=c(60, 70, 80, 90, mean_score, 100), labels=c(\"60\",\"70\",\"80\",\"90\",mean_score_label,\"100\")) +\n  labs(\n    title=fig_title,\n    subtitle=fig_subtitle,\n    x=\"Review Score\",\n    y=\"Number of Reviews\"\n  ) +\n  dsan_theme(\"full\") +\n  theme(\n    legend.title = element_blank(),\n    legend.spacing.y = unit(0, \"mm\")\n  ) +\n  theme(axis.text.x = element_text(colour = c('black', 'black','black', 'black', 'purple', 'black')))\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `linewidth` instead.\n\n\nWarning: Vectorized input to `element_text()` is not officially supported.\n‚Ñπ Results may be unexpected or may change in future versions of ggplot2.\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n(Data from Metacritic)"
  },
  {
    "objectID": "w02/index.html#or-a-single-ribbon",
    "href": "w02/index.html#or-a-single-ribbon",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Or a Single Ribbon",
    "text": "Or a Single Ribbon\n\n\n\nlibrary(tibble)\nN &lt;- 10\n# Each x value gets 10 y values\nx &lt;- sort(rep(seq(1,10),10))\ny &lt;- x + rnorm(length(x), 0, 5)\ndf &lt;- tibble(x=x,y=y)\ntotal_N &lt;- nrow(df)\nggplot(df, aes(x=x,y=y)) +\n  geom_point(size=g_pointsize) +\n  dsan_theme(\"column\") +\n  labs(\n    title=paste0(\"N=\",total_N,\" Randomly-Generated Points\")\n  )\n\n\n\n\n\n# This time, just the means\nlibrary(dplyr)\nmean_df &lt;- df %&gt;% group_by(x) %&gt;% summarize(mean=mean(y), min=min(y), max=max(y))\nggplot(mean_df, aes(x=x, y=mean)) +\n  geom_ribbon(aes(ymin=min, ymax=max, fill=\"ribbon\"), alpha=0.5) +\n  geom_point(aes(color=\"mean\"), size=g_pointsize) +\n  geom_line(size=g_linesize) +\n  dsan_theme(\"half\") +\n  scale_color_manual(\"\", values=c(\"mean\"=\"black\"), labels=c(\"mean\"=\"Mean\")) +\n  scale_fill_manual(\"\", values=c(\"ribbon\"=cbPalette[1]), labels=c(\"ribbon\"=\"Range\")) +\n  remove_legend_title() +\n  labs(\n    title=paste0(\"Means of N=\",total_N,\" Randomly-Generated Points\")\n  )\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\nlibrary(tibble)\nN &lt;- 100\n# Each x value gets 10 y values\nx &lt;- sort(rep(seq(1,10),10))\ny &lt;- x + rnorm(length(x), 0, 1)\ndf &lt;- tibble(x=x,y=y)\ntotal_N &lt;- nrow(df)\nggplot(df, aes(x=x,y=y)) +\n  geom_point(size=g_pointsize) +\n  dsan_theme(\"column\") +\n  labs(\n    title=paste0(\"N=\",total_N,\" Randomly-Generated Points\")\n  )\n\n\n\n\n\n# This time, just the means\nlibrary(dplyr)\nmean_df &lt;- df %&gt;% group_by(x) %&gt;% summarize(mean=mean(y), min=min(y), max=max(y))\nggplot(mean_df, aes(x=x, y=mean)) +\n  geom_ribbon(aes(ymin=min, ymax=max, fill=\"ribbon\"), alpha=0.5) +\n  geom_point(aes(color=\"mean\"), size=g_pointsize) +\n  geom_line(size=g_linesize) +\n  dsan_theme(\"half\") +\n  scale_color_manual(\"\", values=c(\"mean\"=\"black\"), labels=c(\"mean\"=\"Mean\")) +\n  scale_fill_manual(\"\", values=c(\"ribbon\"=cbPalette[1]), labels=c(\"ribbon\"=\"Range\")) +\n  remove_legend_title() +\n  labs(\n    title=paste0(\"Means of N=\",total_N,\" Randomly-Generated Points\")\n  )"
  },
  {
    "objectID": "w02/index.html#example-the-normal-distribution",
    "href": "w02/index.html#example-the-normal-distribution",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Example: The Normal Distribution",
    "text": "Example: The Normal Distribution\n\n\n\n(The distribution you saw a few slides ago)\n\n\nvlines_std_normal &lt;- tibble::tribble(\n  ~x, ~xend, ~y, ~yend, ~Params,\n  0, 0, 0, dnorm(0), \"Mean\",\n  -2, -2, 0, dnorm(-2), \"SD\",\n  -1, -1, 0, dnorm(-1), \"SD\",\n  1, 1, 0, dnorm(1), \"SD\",\n  2, 2, 0, dnorm(2), \"SD\"\n)\nggplot(data.frame(x = c(-3, 3)), aes(x = x)) +\n    stat_function(fun = dnorm, linewidth = g_linewidth) +\n    geom_segment(data=vlines_std_normal, aes(x=x, xend=xend, y=y, yend=yend, linetype = Params), linewidth = g_linewidth, color=\"purple\") +\n    geom_area(stat = \"function\", fun = dnorm, fill = cbPalette[1], xlim = c(-3, 3), alpha=0.2) +\n    #geom_area(stat = \"function\", fun = dnorm, fill = \"blue\", xlim = c(0, 2))\n    dsan_theme(\"quarter\") +\n    labs(\n      x = \"v\",\n      y = \"Density f(v)\"\n    )\n\n\n\n\n\n\n‚ÄúRV \\(X\\) is normally distributed with mean \\({\\color{purple}\\mu}\\) and standard deviation \\({\\color{purple}\\sigma}\\)‚Äù\n\nTranslates to \\(X \\sim \\mathcal{N}(\\color{purple}{\\mu},\\color{purple}{\\sigma})\\)2\n\\(\\color{purple}{\\mu}\\) and \\(\\color{purple}{\\sigma}\\) are parameters3: the ‚Äúknobs‚Äù or ‚Äúsliders‚Äù which change the location/shape of the distribution\n\n\n\n\n\nThe parameters in this case give natural summaries of the data:\n\n\\({\\color{\\purple}\\mu}\\) = center (mean), \\({\\color{purple}\\sigma}\\) = [square root of] variance around center\n\nMean can usually be interpreted intuitively; for standard deviation, we usually use the 68-95-99.7 rule, which will make more sense relative to some real-world data‚Ä¶"
  },
  {
    "objectID": "w02/index.html#real-data-and-the-68-95-99.7-rule",
    "href": "w02/index.html#real-data-and-the-68-95-99.7-rule",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Real Data and the 68-95-99.7 Rule",
    "text": "Real Data and the 68-95-99.7 Rule\n\n\n\n\n\n\n\nFigure¬†2: Heights (cm) for 18K professional athletes\n\n\n\n\n\n\n\nFigure¬†3: The 68-95-99.7 Rule visualized (Wikimedia Commons)\n\n\n\n\nThe point estimate \\({\\color{purple}\\mu} = 186.48\\) is straightforward: the average height of the athletes is 186.48cm. Using the 68-95-99.7 Rule to interpret the SD, \\({\\color{purple}\\sigma} = 9.7\\), we get:\n\n\n\nAbout 68% of the heights fall between\n\n\n\n\n\n\n\n\n\n[\\({\\color{purple}\\mu} - 1\\cdot {\\color{purple}\\sigma}\\)\nand\n\\({\\color{purple}\\mu} + 1\\cdot {\\color{purple}\\sigma}\\)]\n\n\n[186.48 - 1 ¬∑ 9.7\nand\n186.48 + 1 ¬∑ 9.7]\n\n\n[176.78\nand\n196.18]\n\n\n\n\n\nAbout 95% of the heights fall between\n\n\n\n\n\n\n\n\n\n[\\({\\color{purple}\\mu} - 2 \\cdot {\\color{purple}\\sigma}\\)\nand\n\\({\\color{purple}\\mu} + 2 \\cdot {\\color{purple}\\sigma}\\)]\n\n\n[186.48 - 2 ¬∑ 9.7\nand\n186.48 + 2 ¬∑ 9.7]\n\n\n[167.08\nand\n205.88]"
  },
  {
    "objectID": "w02/index.html#boxplots-comparing-multiple-distributions",
    "href": "w02/index.html#boxplots-comparing-multiple-distributions",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Boxplots: Comparing Multiple Distributions",
    "text": "Boxplots: Comparing Multiple Distributions\n\n\n\n\n\n\nJhguch at en.wikipedia, CC BY-SA 2.5, via Wikimedia Commons\n\n\n\n\n\n\n\nProtonk, CC BY-SA 3.0, via Wikimedia Commons"
  },
  {
    "objectID": "w02/index.html#another-option-joyplots",
    "href": "w02/index.html#another-option-joyplots",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Another Option: Joyplots",
    "text": "Another Option: Joyplots\n\n\n\n\n\n\nFigure¬†4: (Iconic album cover)\n\n\n\n\n\n\nFigure¬†5: (Tooting my own horn)"
  },
  {
    "objectID": "w02/index.html#multivariate-distributions-preview",
    "href": "w02/index.html#multivariate-distributions-preview",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Multivariate Distributions: Preview",
    "text": "Multivariate Distributions: Preview\n\nThe bivariate normal distribution represents the distribution of two normally-distributed RVs \\(\\mathbf{X} = [\\begin{smallmatrix} X_1 & X_2\\end{smallmatrix}]\\), which may or may not be correlated:\n\n\n\\[\n\\mathbf{X} = \\begin{bmatrix}X_1 \\\\ X_2\\end{bmatrix}, \\; \\boldsymbol{\\mu} =\n%\\begin{bmatrix}\\mu_1 \\\\ \\mu_2\\end{bmatrix}\n\\begin{bmatrix}\\smash{\\overbrace{\\mu_1}^{\\mathbb{E}[X_1]}} \\\\ \\smash{\\underbrace{\\mu_2}_{\\mathbb{E}[X_2]}}\\end{bmatrix}\n, \\; \\mathbf{\\Sigma} = \\begin{bmatrix}\\smash{\\overbrace{\\sigma_1^2}^{\\text{Var}[X_1]}} & \\smash{\\overbrace{\\rho\\sigma_1\\sigma_2}^{\\text{Cov}[X_1,X_2]}} \\\\ \\smash{\\underbrace{\\rho\\sigma_2\\sigma_1}_{\\text{Cov}[X_2,X_1]}} & \\smash{\\underbrace{\\sigma_2^2}_{\\text{Var}[X_2]}}\\end{bmatrix}\n% \\begin{bmatrix}\\sigma_1^2 & \\rho\\sigma_1\\sigma_2 \\\\ \\rho\\sigma_2\\sigma_1 & \\sigma_2^2 \\end{bmatrix}\n% = \\begin{bmatrix}\\text{Var}[X_1] & \\text{Cov}[X_1,X_2] \\\\ \\text{Cov}[X_2,X_1] & \\text{Var}[X_2] \\end{bmatrix}\n\\]\n\n\nBy squishing all this information intro matrices, we can specify the parameters of multivariate-normally-distributed vectors of RVs similarly to how we specify single-dimensional normally-distributed RVs:\n\n\n\\[\n\\begin{align*}\n\\overbrace{X}^{\\mathclap{\\text{scalar}}} &\\sim \\mathcal{N}\\phantom{_k}(\\overbrace{\\mu}^{\\text{scalar}}, \\overbrace{\\sigma}^{\\text{scalar}}) \\tag{Univariate} \\\\\n\\underbrace{\\mathbf{X}}_{\\text{vector}} &\\sim \\boldsymbol{\\mathcal{N}}_k(\\smash{\\underbrace{\\boldsymbol{\\mu}}_{\\text{vector}}}, \\underbrace{\\mathbf{\\Sigma}}_{\\text{matrix}}) \\tag{Multivariate}\n\\end{align*}\n\\]\n\n\nNote: In the future I‚Äôll use the notation \\(\\mathbf{X}_{[a \\times b]}\\) to denote the dimensions of the vectors/matrices, like \\(\\mathbf{X}_{[k \\times 1]} \\sim \\boldsymbol{\\mathcal{N}}_k(\\boldsymbol{\\mu}_{[k \\times 1]}, \\mathbf{\\Sigma}_{[k \\times k]})\\)"
  },
  {
    "objectID": "w02/index.html#visualizing-3d-distributions-projection",
    "href": "w02/index.html#visualizing-3d-distributions-projection",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Visualizing 3D Distributions: Projection",
    "text": "Visualizing 3D Distributions: Projection\n\nSince most of our intuitions about plots come from 2D plots, it is extremely useful to be able to take a 3D plot like this and imagine ‚Äúprojecting‚Äù it down into different 2D plots:\n\n\n\n\n(Adapted via LaTeX from StackExchange discussion)"
  },
  {
    "objectID": "w02/index.html#visualizing-3d-distributions-contours",
    "href": "w02/index.html#visualizing-3d-distributions-contours",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Visualizing 3D Distributions: Contours",
    "text": "Visualizing 3D Distributions: Contours\n\n\n\nFrom Prof.¬†Hickman‚Äôs slides!"
  },
  {
    "objectID": "w02/index.html#visualizing-3d-distributions-contours-1",
    "href": "w02/index.html#visualizing-3d-distributions-contours-1",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Visualizing 3D Distributions: Contours",
    "text": "Visualizing 3D Distributions: Contours\n\n\n\nAlso from Prof.¬†Hickman‚Äôs slides!"
  },
  {
    "objectID": "w02/index.html#footnotes",
    "href": "w02/index.html#footnotes",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor math majors, you can think of it as an isomorphism between the objects and operations of the three subjects‚Ü©Ô∏é\nThroughout the course, this ‚Äúcalligraphic‚Äù font \\(\\mathcal{N}\\), \\(\\mathcal{D}\\), etc., will be used to denote distributions‚Ü©Ô∏é\nThroughout the course, remember, purrple is for purrameters‚Ü©Ô∏é"
  },
  {
    "objectID": "w03/slides.html#recap-1",
    "href": "w03/slides.html#recap-1",
    "title": "Week 3: Conditional Probability",
    "section": "Recap",
    "text": "Recap\n\nLogic \\(\\rightarrow\\) Set Theory \\(\\rightarrow\\) Probability Theory\nEntirety of probability theory can be derived from two axioms:\n\n\n\n\nThe Entirety of Probability Theory Follows From‚Ä¶\n\n\nAxiom 1 (Unitarity): \\(\\Pr(\\Omega) = 1\\) (The probability that something happens is 1)\nAxiom 2 (\\(\\sigma\\)-additivity): For mutually-exclusive events \\(E_1, E_2, \\ldots\\),\n\\[\n\\underbrace{\\Pr\\left(\\bigcup_{i=1}^{\\infty}E_i\\right)}_{\\Pr(E_1\\text{ occurs }\\vee E_2\\text{ occurs } \\vee \\cdots)} = \\underbrace{\\sum_{i=1}^{\\infty}\\Pr(E_i)}_{\\Pr(E_1\\text{ occurs}) + \\Pr(E_2\\text{ occurs}) + \\cdots}\n\\]\n\n\n\n\nBut what does ‚Äúmutually exclusive‚Äù mean‚Ä¶?"
  },
  {
    "objectID": "w03/slides.html#venn-diagrams-sets",
    "href": "w03/slides.html#venn-diagrams-sets",
    "title": "Week 3: Conditional Probability",
    "section": "Venn Diagrams: Sets",
    "text": "Venn Diagrams: Sets\n\n\n\n\n\n\n\n\n\\[\n\\begin{align*}\n&A = \\{0, 1, 2\\}, \\; B = \\{4, 5, 6\\} \\\\\n&\\implies A \\cap B = \\varnothing\n\\end{align*}\n\\]\nFigure¬†1: Mutually-exclusive (disjoint) sets\n\n\n\n\n\n\n\n\\[\n\\begin{align*}\n&A = \\{1, 2, 3\\}, \\; B = \\{3, 4, 5\\} \\\\\n&\\implies A \\cap B = \\{3\\}\n\\end{align*}\n\\]\nFigure¬†2: Non-mutually-exclusive sets"
  },
  {
    "objectID": "w03/slides.html#venn-diagrams-events-dice",
    "href": "w03/slides.html#venn-diagrams-events-dice",
    "title": "Week 3: Conditional Probability",
    "section": "Venn Diagrams: Events (Dice)",
    "text": "Venn Diagrams: Events (Dice)\n\\[\n\\begin{align*}\nA &= \\{\\text{Roll is even}\\} = \\{2, 4, 6\\} \\\\\nB &= \\{\\text{Roll is odd}\\} = \\{1, 3, 5\\} \\\\\nC &= \\{\\text{Roll is in Fibonnaci sequence}\\} = \\{1, 2, 3, 5\\}\n\\end{align*}\n\\]\n\n\n\n\n\n\n\n\n\n\nSet 1\nSet 2\nIntersection\nMutually Exclusive?\nCan Happen Simultaneously?\n\n\n\n\n\\(A\\)\n\\(B\\)\n\\(A \\cap B = \\varnothing\\)\nYes\nNo\n\n\n\\(A\\)\n\\(C\\)\n\\(A \\cap C = \\{2\\}\\)\nNo\nYes\n\n\n\\(B\\)\n\\(C\\)\n\\(B \\cap C = \\{1, 3, 5\\}\\)\nNo\nYes"
  },
  {
    "objectID": "w03/slides.html#rules-of-probability",
    "href": "w03/slides.html#rules-of-probability",
    "title": "Week 3: Conditional Probability",
    "section": "‚ÄúRules‚Äù of Probability",
    "text": "‚ÄúRules‚Äù of Probability\n\n(Remember: not ‚Äúrules‚Äù but ‚Äúfacts resulting from the logic \\(\\leftrightarrow\\) probability connection‚Äù)\n\n\n\n\n‚ÄúRules‚Äù of Probability\n\n\nFor logical predicates \\(p, q \\in \\{T, F\\}\\), events \\(P, Q\\) defined so \\(P\\) = event that \\(p\\) becomes true, \\(Q\\) = event that \\(q\\) becomes true,\n\nLogical AND = Probabilistic Multiplication\n\n\\[\n\\Pr(p \\wedge q) = \\Pr(P \\cap Q) = \\Pr(P) \\cdot \\Pr(Q)\n\\]\n\nLogical OR = Probabilistic Addition\n\n\\[\n\\Pr(p \\vee q) = \\Pr(P \\cup Q) = \\Pr(P) + \\Pr(Q) - \\underbrace{\\Pr(P \\cap Q)}_{\\text{(see rule 1)}}\n\\]\n\nLogical NOT = Probabilistic Complement\n\n\\[\n\\Pr(\\neg p) = \\Pr(P^c) = 1 - \\Pr(P)\n\\]"
  },
  {
    "objectID": "w03/slides.html#conditional-probability",
    "href": "w03/slides.html#conditional-probability",
    "title": "Week 3: Conditional Probability",
    "section": "Conditional Probability",
    "text": "Conditional Probability\n\nUsually if someone asks you probabilistic questions, like\n\n‚ÄúWhat is the likelihood that [our team] wins?‚Äù\n‚ÄúDo you think it will rain tomorrow?‚Äù and so on\n\nYou don‚Äôt guess a random number, you consider and incorporate evidence.\nExample: \\(\\Pr(\\text{rain})\\) on its own, without any other info? A tough question‚Ä¶ maybe \\(0.5\\)?\nIn reality, we would think about\n\n\\(\\Pr(\\text{rain} \\mid \\text{month of the year})\\)\n\\(\\Pr(\\text{rain} \\mid \\text{where we live})\\)\n\\(\\Pr(\\text{rain} \\mid \\text{did it rain yesterday?})\\)\n\nPsychologically, breaks down into two steps: (1) Think of a baseline probability, (2) Update baseline probability to incorporate relevant evidence (more on this in a bit‚Ä¶)\nAlso recall from last week: all probability is conditional probability, even if just conditioned on ‚Äúsomething happened‚Äù (\\(\\Omega\\), the thing defined so \\(\\Pr(\\Omega) = 1\\))"
  },
  {
    "objectID": "w03/slides.html#na√Øve-definition-2.0",
    "href": "w03/slides.html#na√Øve-definition-2.0",
    "title": "Week 3: Conditional Probability",
    "section": "Na√Øve Definition 2.0",
    "text": "Na√Øve Definition 2.0\n\n\n\n[Slightly Less] Na√Øve Definition of Probability\n\n\n\\[\n\\Pr(A \\mid B) = \\frac{\\text{\\# of Desired Outcomes in world where }B\\text{ happened}}{\\text{\\# Total outcomes in world where }B\\text{ happened}} = \\frac{|B \\cap A|}{|B|}\n\\]\n\n\n\n\n\n\n\n\n\n\n\nWorld Name\nWeather in World\nLikelihood of Rain Today\n\n\n\n\n\\(R\\)\nRained for the past 5 days\n\\(\\Pr(\\text{rain} \\mid R) &gt; 0.5\\)\n\n\n\\(M\\)\nMix of rain and non-rain over past 5 days\n\\(\\Pr(\\text{rain} \\mid M) \\approx 0.5\\)\n\n\n\\(S\\)\nSunny for the past 5 days\n\\(\\Pr(\\text{rain} \\mid S) &lt; 0.5\\)"
  },
  {
    "objectID": "w03/slides.html#law-of-total-probability",
    "href": "w03/slides.html#law-of-total-probability",
    "title": "Week 3: Conditional Probability",
    "section": "Law of Total Probability",
    "text": "Law of Total Probability\n\nSuppose the events \\(B_1, \\ldots, B_k\\) form a partition of the space \\(S\\) and \\(\\Pr(B_j) &gt; 0 \\forall j\\).\nThen, for every event \\(A\\) in \\(S\\),\n\n\\[\n\\Pr(A) = \\sum_{i=1}^k \\Pr(B_j)\\Pr(A \\mid B_j)\n\\]\n\nProbability of an event is the sum of its conditional probabilities across all conditions.\nIn other words: \\(A\\) is some event, \\(B_1, \\ldots, B_n\\) are mutually exclusive events filling entire sample-space, then\n\n\\[\n\\Pr(A) = \\Pr(A \\mid B_1)\\Pr(B_1) + \\Pr(A \\mid B_2)\\Pr(B_2) + \\cdots + \\Pr(A \\mid B_n)\\Pr(B_n)\n\\]\ni.e.¬†Compute the probability by summing over all possible cases."
  },
  {
    "objectID": "w03/slides.html#example",
    "href": "w03/slides.html#example",
    "title": "Week 3: Conditional Probability",
    "section": "Example",
    "text": "Example\n\nProbabilities of completing a job on time, with and without rain, are 0.42 and 0.90 respectively.\nProbability it will rain is 0.45. What is the probability the job will be completed on time?\n\\(A\\) = job will be completed on time, \\(B\\) = rain\n\n\\[\n\\Pr(B) = 0.45 \\implies \\Pr(B^c) = 1 - \\Pr(B) = 0.55.\n\\]\n\nNote: Events \\(B\\) and \\(B^c\\) are exclusive and form partitions of the sample space \\(S\\)\nWe know \\(\\Pr(A \\mid B) = 0.24\\), \\(\\Pr(A \\mid B^c) = 0.9\\).\nBy the Law of Total Probability, we have\n\n\\[\n\\begin{align*}\n\\Pr(A) &= \\Pr(B)\\Pr(A \\mid B) + \\Pr(B^c)\\Pr(A \\mid B^c) \\\\\n&= 0.45(0.42) + 0.55(0.9) = 0.189 + 0.495 = 0684.\n\\end{align*}\n\\]\nSo, the probability that the job will be completed on time is 0.684. (source)"
  },
  {
    "objectID": "w03/slides.html#deriving-bayes-theorem",
    "href": "w03/slides.html#deriving-bayes-theorem",
    "title": "Week 3: Conditional Probability",
    "section": "Deriving Bayes‚Äô Theorem",
    "text": "Deriving Bayes‚Äô Theorem\n\nLiterally just a re-writing of the conditional probability definition (don‚Äôt be scared)!\n\n\n\n\nFor two events \\(A\\) and \\(B\\), definition of conditional probability says that\n\n\\[\n\\begin{align*}\n\\Pr(A \\mid B) &= \\frac{\\Pr(A \\cap B)}{\\Pr(B)} \\tag{1} \\\\\n\\Pr(B \\mid A) &= \\frac{\\Pr(B \\cap A)}{\\Pr(A)} \\tag{2}\n\\end{align*}\n\\]\n\nMultiply to get rid of fractions\n\n\\[\n\\begin{align*}\n\\Pr(A \\mid B)\\Pr(B) &= \\Pr(A \\cap B) \\tag{1*} \\\\\n\\Pr(B \\mid A)\\Pr(A) &= \\Pr(B \\cap A) \\tag{2*}\n\\end{align*}\n\\]\n\n\nBut set intersection is associative (just like multiplication‚Ä¶), \\(A \\cap B = B \\cap A\\)! So, we know LHS of \\((\\text{1*})\\) = LHS of \\((\\text{2*})\\):\n\n\\[\n\\Pr(A \\mid B)\\Pr(B) = \\Pr(B \\mid A)\\Pr(A)\n\\]\n\nDivide both sides by \\(\\Pr(B)\\) to get a new definition of \\(\\Pr(A \\mid B)\\), Bayes‚Äô Theorem!\n\n\n\n\\[\n\\boxed{\\Pr(A \\mid B) = \\frac{\\Pr(B \\mid A)\\Pr(A)}{\\Pr(B)}}\n\\]\nFigure¬†3: Bayes‚Äô Theorem"
  },
  {
    "objectID": "w03/slides.html#why-is-this-helpful",
    "href": "w03/slides.html#why-is-this-helpful",
    "title": "Week 3: Conditional Probability",
    "section": "Why Is This Helpful?",
    "text": "Why Is This Helpful?\n\n\n\nBayes‚Äô Theorem\n\n\nFor any two events \\(A\\) and \\(B\\), \\[\n\\Pr(A \\mid B) = \\frac{\\Pr(B \\mid A)\\Pr(A)}{\\Pr(B)}\n\\]\n\n\n\n\nIn words (as exciting as I can make it, for now): Bayes‚Äô Theorem allows us to take information about \\(B \\mid A\\) and use it to infer information about \\(A \\mid B\\)\nIt isn‚Äôt until you work through some examples that this becomes mind-blowing, the most powerful equation we have for inferring unknowns from knowns‚Ä¶\nConsider \\(A = \\{\\text{person has disease}\\}\\), \\(B = \\{\\text{person tests positive for disease}\\}\\)\n\nIs \\(A\\) observable on its own? No, but‚Ä¶\n\nIs \\(B\\) observable on its own? Yes, and\nCan we infer information about \\(A\\) from knowing \\(B\\)? Also Yes, thanks to Bayes!\n\nTherefore, we can use \\(B\\) to infer information about \\(A\\), i.e., calculate \\(\\Pr(A \\mid B)\\)‚Ä¶"
  },
  {
    "objectID": "w03/slides.html#why-is-this-helpful-for-data-science",
    "href": "w03/slides.html#why-is-this-helpful-for-data-science",
    "title": "Week 3: Conditional Probability",
    "section": "Why Is This Helpful for Data Science?",
    "text": "Why Is This Helpful for Data Science?\n\nIt merges probability theory and hypothesis testing into a single framework:\n\n\\[\n\\Pr(\\text{hypothesis} \\mid \\text{data}) = \\frac{\\Pr(\\text{data} \\mid \\text{hypothesis})\\Pr(\\text{hypothesis})}{\\Pr(\\text{data})}\n\\]"
  },
  {
    "objectID": "w03/slides.html#probability-forwards-and-backwards",
    "href": "w03/slides.html#probability-forwards-and-backwards",
    "title": "Week 3: Conditional Probability",
    "section": "Probability Forwards and Backwards",
    "text": "Probability Forwards and Backwards\n\nTwo discrete RVs:\n\nWeather on a given day, \\(W \\in \\{\\textsf{Rain},\\textsf{Sun}\\}\\)\nAction that day, \\(A \\in \\{\\textsf{Go}, \\textsf{Stay}\\}\\): go to party or stay in and watch movie\n\nData-generating process: if \\(\\textsf{Sun}\\), rolls a die \\(R\\) and goes out unless \\(R = 6\\). If \\(\\textsf{Rain}\\), flips a coin and goes out if \\(\\textsf{H}\\).\nProbabilistic Graphical Model (PGM):"
  },
  {
    "objectID": "w03/slides.html#section",
    "href": "w03/slides.html#section",
    "title": "Week 3: Conditional Probability",
    "section": "",
    "text": "So, if we know \\(W = \\textsf{Sun}\\), what is \\(P(A = \\textsf{Go})\\)? \\[\n\\begin{align*}\nP(A = \\textsf{Go} \\mid W) &= 1 - P(R = 6) \\\\\n&= 1 - \\frac{1}{6} = \\frac{5}{6}\n\\end{align*}\n\\]\nConditional probability lets us go forwards (left to right):\n\n\n\n\n\n\n\nBut what if we want to perform inference going backwards?"
  },
  {
    "objectID": "w03/slides.html#section-1",
    "href": "w03/slides.html#section-1",
    "title": "Week 3: Conditional Probability",
    "section": "",
    "text": "If we see Ana at the party, we know \\(A = \\textsf{Go}\\)\nWhat does this tell us about the weather?\nIntuitively, we should increase our degree of belief that \\(W = \\textsf{Sun}\\). But, by how much?\nWe don‚Äôt know \\(P(W \\mid A)\\), only \\(P(A \\mid W)\\)‚Ä¶"
  },
  {
    "objectID": "w03/slides.html#section-2",
    "href": "w03/slides.html#section-2",
    "title": "Week 3: Conditional Probability",
    "section": "",
    "text": "\\[\nP(W = \\textsf{Sun} \\mid A = \\textsf{Go}) = \\frac{\\overbrace{P(A = \\textsf{Go} \\mid W = \\textsf{Sun})}^{5/6~ ‚úÖ}\\overbrace{P(W = \\textsf{Sun})}^{‚ùì}}{\\underbrace{P(A = \\textsf{Go})}_{‚ùì}}\n\\]\n\nWe‚Äôve seen \\(P(W = \\textsf{Sun})\\) before, it‚Äôs our prior: the probability without having any additional relevant knowledge. So, let‚Äôs say 50/50. \\(P(W = \\textsf{Sun}) = \\frac{1}{2}\\)\nIf we lived in Seattle, we could pick \\(P(W = \\textsf{Sun}) = \\frac{1}{4}\\)"
  },
  {
    "objectID": "w03/slides.html#section-3",
    "href": "w03/slides.html#section-3",
    "title": "Week 3: Conditional Probability",
    "section": "",
    "text": "\\[\nP(W = \\textsf{Sun} \\mid A = \\textsf{Go}) = \\frac{\\overbrace{P(A = \\textsf{Go} \\mid W = \\textsf{Sunny})}^{5/6~ ‚úÖ}\\overbrace{P(W = \\textsf{Sun})}^{1/2~ ‚úÖ}}{\\underbrace{P(A = \\textsf{Go})}_{‚ùì}}\n\\]\n\n\\(P(A = \\textsf{Go})\\) is trickier: the probability that Ana goes out regardless of what the weather is. But there are only two possible weather outcomes! So we just compute\n\n\\[\n\\begin{align*}\n&P(A = \\textsf{Go}) = \\sum_{\\omega \\in S(W)}P(A = \\textsf{Go}, \\omega) = \\sum_{\\omega \\in S(W)}P(A = \\textsf{Go} \\mid \\omega)P(\\omega) \\\\\n&= P(A = \\textsf{Go} \\mid W = \\textsf{Rain})P(W = \\textsf{Rain}) + P(A = \\textsf{Go} \\mid W = \\textsf{Sun})P(W = \\textsf{Sun}) \\\\\n&= \\left( \\frac{1}{2} \\right)\\left( \\frac{1}{2} \\right) + \\left( \\frac{5}{6} \\right)\\left( \\frac{1}{2} \\right) = \\frac{1}{4} + \\frac{5}{12} = \\frac{2}{3}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "w03/slides.html#putting-it-all-together",
    "href": "w03/slides.html#putting-it-all-together",
    "title": "Week 3: Conditional Probability",
    "section": "Putting it All Together",
    "text": "Putting it All Together\n\\[\n\\begin{align*}\nP(W = \\textsf{Sun} \\mid A = \\textsf{Go}) &= \\frac{\\overbrace{P(A = \\textsf{Go} \\mid W = \\textsf{Sunny})}^{3/4~ ‚úÖ}\\overbrace{P(W = \\textsf{Sun})}^{1/2~ ‚úÖ}}{\\underbrace{P(A = \\textsf{Go})}_{1/2~ ‚úÖ}} \\\\\n&= \\frac{\\left(\\frac{3}{4}\\right)\\left(\\frac{1}{2}\\right)}{\\frac{1}{2}} = \\frac{\\frac{3}{8}}{\\frac{1}{2}} = \\frac{3}{4}.\n\\end{align*}\n\\]\n\nGiven that we see Ana at the party, we should update our beliefs, so that \\(P(W = \\textsf{Sun}) = \\frac{3}{4}, P(W = \\textsf{Rain}) = \\frac{1}{4}\\)."
  },
  {
    "objectID": "w03/slides.html#a-scarier-example",
    "href": "w03/slides.html#a-scarier-example",
    "title": "Week 3: Conditional Probability",
    "section": "A Scarier Example",
    "text": "A Scarier Example\n\nBo worries he has a rare disease. He takes a test with 99% accuracy and tests positive. What‚Äôs the probability Bo has the disease? (Intuition: 99%? ‚Ä¶Let‚Äôs do the math!)\n\n\n\n\n\\(H \\in \\{\\textsf{sick}, \\textsf{healthy}\\}, T \\in \\{\\textsf{T}^+, \\textsf{T}^-\\}\\)\nThe test: 99% accurate. \\(\\Pr(T = \\textsf{T}^+ \\mid H = \\textsf{sick}) = 0.99\\), \\(\\Pr(T = \\textsf{T}^- \\mid H = \\textsf{healthy}) = 0.99\\).\nThe disease: 1 in 10K. \\(\\Pr(H = \\textsf{sick}) = \\frac{1}{10000}\\)\nWhat do we want to know? \\(\\Pr(H = \\textsf{sick} \\mid T = \\textsf{T}^+)\\)\nHow do we get there?\n\n\n\n\n\nThis photo, originally thought to be of Thomas Bayes, turns out to be probably someone else‚Ä¶ \\(\\Pr(\\textsf{Bayes})\\)?\n\n\n\n\n\n\\(H\\) for health, \\(T\\) for test result\nPhoto credit: https://thedatascientist.com/wp-content/uploads/2019/04/reverend-thomas-bayes.jpg"
  },
  {
    "objectID": "w03/slides.html#section-4",
    "href": "w03/slides.html#section-4",
    "title": "Week 3: Conditional Probability",
    "section": "",
    "text": "\\[\n\\begin{align*}\n\\Pr(H = \\textsf{sick} \\mid T = \\textsf{T}^+) &= \\frac{\\Pr(T = \\textsf{T}^+ \\mid H = \\textsf{sick})\\Pr(H = \\textsf{sick})}{\\Pr(T = \\textsf{T}^+)} \\\\\n&= \\frac{(0.99)\\left(\\frac{1}{10000}\\right)}{(0.99)\\left( \\frac{1}{10000} \\right) + (0.01)\\left( \\frac{9999}{10000} \\right)}\n\\end{align*}\n\\]\n\np_sick &lt;- 1 / 10000\np_healthy &lt;- 1 - p_sick\np_pos_given_sick &lt;- 0.99\np_neg_given_sick &lt;- 1 - p_pos_given_sick\np_neg_given_healthy &lt;- 0.99\np_pos_given_healthy &lt;- 1 - p_neg_given_healthy\nnumer &lt;- p_pos_given_sick * p_sick\ndenom1 &lt;- numer\ndenom2 &lt;- p_pos_given_healthy * p_healthy\nfinal_prob &lt;- numer / (denom1 + denom2)\nfinal_prob\n\n[1] 0.009803922\n\n\n\n‚Ä¶ Less than 1% üò±"
  },
  {
    "objectID": "w03/slides.html#proof-in-the-pudding",
    "href": "w03/slides.html#proof-in-the-pudding",
    "title": "Week 3: Conditional Probability",
    "section": "Proof in the Pudding",
    "text": "Proof in the Pudding\n\nLet‚Äôs generate a dataset of 5,000 people, using \\(\\Pr(\\textsf{Disease}) = \\frac{1}{10000}\\)\n\n\n\nCode\nlibrary(tibble)\nlibrary(dplyr)\n# Disease rarity\np_disease &lt;- 1 / 10000\n# 1K people\nnum_people &lt;- 10000\n# Give them ids\nppl_df &lt;- tibble(id=seq(1,num_people))\n# Whether they have the disease or not\nhas_disease &lt;- rbinom(num_people, 1, p_disease)\nppl_df &lt;- ppl_df %&gt;% mutate(has_disease=has_disease)\ndisp(ppl_df %&gt;% head())"
  },
  {
    "objectID": "w03/slides.html#binary-variable-trick",
    "href": "w03/slides.html#binary-variable-trick",
    "title": "Week 3: Conditional Probability",
    "section": "Binary Variable Trick",
    "text": "Binary Variable Trick\n\nSince has_disease \\(\\in \\{0, 1\\}\\), we can use\n\nsum(has_disease) to obtain the count of people with the disease, or\nmean(has_disease) to obtain the proportion of people who have the disease\n\nTo see this (or, if you forget in the future), just make a fake dataset with a binary variable and 3 rows, and think about sums vs.¬†means of that variable:\n\n\n\n\n\nCode\nbinary_df &lt;- tibble(\n  id=c(1,2,3),\n  x=c(0,1,0)\n)\ndisp(binary_df)\n\n\n\n\n\n\n\n\nTaking the sum tells us: one row where x == 1:\n\n\nCode\nsum(binary_df$x)\n\n\n[1] 1\n\n\nTaking the mean tells us: 1/3 of rows have x == 1:\n\n\nCode\nmean(binary_df$x)\n\n\n[1] 0.3333333"
  },
  {
    "objectID": "w03/slides.html#applying-this-to-the-disease-data",
    "href": "w03/slides.html#applying-this-to-the-disease-data",
    "title": "Week 3: Conditional Probability",
    "section": "Applying This to the Disease Data",
    "text": "Applying This to the Disease Data\n\nIf we want the number of people who have the disease:\n\n\n\nCode\n# Compute the *number* of people who have the disease\nsum(ppl_df$has_disease)\n\n\n[1] 1\n\n\n\nIf we want the proportion of people who have the disease:\n\n\n\nCode\n# Compute the *proportion* of people who have the disease\nmean(ppl_df$has_disease)\n\n\n[1] 1e-04\n\n\n\n(And if you dislike scientific notation like I do‚Ä¶)\n\n\n\nCode\nformat(mean(ppl_df$has_disease), scientific = FALSE)\n\n\n[1] \"0.0001\"\n\n\n\n(Foreshadowing Monte Carlo methods)"
  },
  {
    "objectID": "w03/slides.html#data-generating-process-test-results",
    "href": "w03/slides.html#data-generating-process-test-results",
    "title": "Week 3: Conditional Probability",
    "section": "Data-Generating Process: Test Results",
    "text": "Data-Generating Process: Test Results\n\n\nCode\nlibrary(dplyr)\n# Data Generating Process\ntake_test &lt;- function(is_sick) {\n  if (is_sick) {\n    return(rbinom(1,1,p_pos_given_sick))\n  } else {\n    return(rbinom(1,1,p_pos_given_healthy))\n  }\n}\nppl_df['test_result'] &lt;- unlist(lapply(ppl_df$has_disease, take_test))\nnum_positive &lt;- sum(ppl_df$test_result)\np_positive &lt;- mean(ppl_df$test_result)\nwriteLines(paste0(num_positive,\" positive tests / \",num_people,\" total = \",p_positive))\n\n\n111 positive tests / 10000 total = 0.0111"
  },
  {
    "objectID": "w03/slides.html#zooming-in-on-positive-tests",
    "href": "w03/slides.html#zooming-in-on-positive-tests",
    "title": "Week 3: Conditional Probability",
    "section": "Zooming In On Positive Tests",
    "text": "Zooming In On Positive Tests\n\n\n\n\n\n\n\n\n\n\n\nBo doesn‚Äôt have it, and neither do 110 of the 111 total people who tested positive!\nBut, in the real world, we only observe \\(T\\)"
  },
  {
    "objectID": "w03/slides.html#zooming-in-on-disease-havers",
    "href": "w03/slides.html#zooming-in-on-disease-havers",
    "title": "Week 3: Conditional Probability",
    "section": "Zooming In On Disease-Havers",
    "text": "Zooming In On Disease-Havers\n\nWhat if we look at only those who actually have the disease? Maybe the cost of 111 people panicking is worth it if we correctly catch those who do have it?\n\n\n\nCode\ndisp(ppl_df[ppl_df$has_disease == 1,])\n\n\n\n\n\n\n\nIs this always going to be the case?\n\n\n\n\nNum with disease: 0\nProportion with disease: 0\nNumber of positive tests: 45\n\n\n\n\n\n\n\n\n\n\nNum with disease: 0\nProportion with disease: 0\nNumber of positive tests: 62"
  },
  {
    "objectID": "w03/slides.html#worst-case-worlds",
    "href": "w03/slides.html#worst-case-worlds",
    "title": "Week 3: Conditional Probability",
    "section": "Worst-Case Worlds",
    "text": "Worst-Case Worlds\n\n\n\n\nWorld #415 / 1000 (5000 people):\n# A tibble: 2 √ó 3\n     id has_disease test_result\n  &lt;int&gt;       &lt;int&gt;       &lt;int&gt;\n1   617           1           1\n2  2192           1           0\n\n\nWorld #503 / 1000 (5000 people):\n# A tibble: 1 √ó 3\n     id has_disease test_result\n  &lt;int&gt;       &lt;int&gt;       &lt;int&gt;\n1  1669           1           0\n\n\nWorld #729 / 1000 (5000 people):\n# A tibble: 3 √ó 3\n     id has_disease test_result\n  &lt;int&gt;       &lt;int&gt;       &lt;int&gt;\n1  1378           1           1\n2  1427           1           1\n3  4062           1           0\n\n\n[1] \"0.0000008\"\n\n\n\nHow unlikely is this? Math:\n\\[\n\\begin{align*}\n\\Pr(\\textsf{T}^- \\cap \\textsf{Sick}) &= \\Pr(\\textsf{T}^- \\mid \\textsf{Sick})\\Pr(\\textsf{Sick}) \\\\\n&= (0.01)\\frac{1}{10000} \\\\\n&= \\frac{1}{1000000}\n\\end{align*}\n\\]\nComputers:\n\n\nFalse Negatives: 1, Total Cases: 1000000\n\n\nFalse Negative Rate: 0.000001\n\n\n(Perfect match!)"
  },
  {
    "objectID": "w03/slides.html#bayes-takeaway",
    "href": "w03/slides.html#bayes-takeaway",
    "title": "Week 3: Conditional Probability",
    "section": "Bayes: Takeaway",
    "text": "Bayes: Takeaway\n\nBayesian approach allows new evidence to be weighed against existing evidence, with statistically principled way to derive these weights:\n\n\\[\n\\begin{array}{ccccc}\n\\Pr_{\\text{post}}(\\mathcal{H}) &\\hspace{-6mm}\\propto &\\hspace{-6mm} \\Pr(X \\mid \\mathcal{H}) &\\hspace{-6mm} \\times &\\hspace{-6mm} \\Pr_{\\text{pre}}(\\mathcal{H}) \\\\\n\\text{Posterior} &\\hspace{-6mm}\\propto &\\hspace{-6mm}\\text{Evidence} &\\hspace{-6mm} \\times &\\hspace{-6mm} \\text{Prior}\n\\end{array}\n\\]"
  },
  {
    "objectID": "w03/slides.html#monte-carlo-methods-overview",
    "href": "w03/slides.html#monte-carlo-methods-overview",
    "title": "Week 3: Conditional Probability",
    "section": "Monte Carlo Methods: Overview",
    "text": "Monte Carlo Methods: Overview\n\nYou already saw an example, in our rare disease simulation!\nGenerally, using computers (rather than math, ‚Äúby hand‚Äù) to estimate probabilistic quantities\n\n\n\nPros:\n\nMost real-world processes have no analytic solution\nStep-by-step breakdown of complex processes\n\n\nCons:\n\nCan require immense computing power\n‚ö†Ô∏è Can generate incorrect answers ‚ö†Ô∏è\n\n\n\n\nBy step-by-step I mean, a lot of the time you are just walking through, generating the next column using previously-generated columns. Like we did in the example above, generating test_result based on has_disease."
  },
  {
    "objectID": "w03/slides.html#birthday-problem",
    "href": "w03/slides.html#birthday-problem",
    "title": "Week 3: Conditional Probability",
    "section": "Birthday Problem",
    "text": "Birthday Problem\n\n\n\n30 people gather in a room together. What is the probability that two of them share the same birthday?\nAnalytic solution is fun, but requires some thought‚Ä¶ Monte Carlo it!\n\n\n\n\nCode\ngen_bday_room &lt;- function(room_num=NULL) {\n  num_people &lt;- 30\n  num_days &lt;- 366\n  ppl_df &lt;- tibble(id=seq(1,num_people))\nbirthdays &lt;- sample(1:num_days, num_people,replace = T)\n  ppl_df['birthday'] &lt;- birthdays\n  if (!is.null(room_num)) {\n    ppl_df &lt;- ppl_df %&gt;% mutate(room_num=room_num) %&gt;% relocate(room_num)\n  }\n  return(ppl_df)\n}\nppl_df &lt;- gen_bday_room(1)\ndisp(ppl_df %&gt;% head()) #, obs_per_page = 3)"
  },
  {
    "objectID": "w03/slides.html#section-5",
    "href": "w03/slides.html#section-5",
    "title": "Week 3: Conditional Probability",
    "section": "",
    "text": "# A tibble: 0 √ó 0"
  },
  {
    "objectID": "w03/slides.html#section-6",
    "href": "w03/slides.html#section-6",
    "title": "Week 3: Conditional Probability",
    "section": "",
    "text": "Let‚Äôs try more rooms‚Ä¶\n\n\n\n# Get tibbles for each room\nlibrary(purrr)\ngen_bday_rooms &lt;- function(num_rooms) {\n  rooms_df &lt;- tibble()\n  for (r in seq(1, num_rooms)) {\n      cur_room &lt;- gen_bday_room(r)\n      rooms_df &lt;- bind_rows(rooms_df, cur_room)\n  }\n  return(rooms_df)\n}\nnum_rooms &lt;- 10\nrooms_df &lt;- gen_bday_rooms(num_rooms)\nrooms_df %&gt;% group_by(room_num) %&gt;% group_map(~ get_shared_bdays(.x, is_grouped=TRUE))\n\n[[1]]\n# A tibble: 3 √ó 3\n      i     j  bday\n  &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1     3     6   116\n2     7    12   287\n3    19    30   267\n\n[[2]]\n# A tibble: 0 √ó 0\n\n[[3]]\n# A tibble: 1 √ó 3\n      i     j  bday\n  &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1     7    23   138\n\n[[4]]\n# A tibble: 1 √ó 3\n      i     j  bday\n  &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1     6    18    72\n\n[[5]]\n# A tibble: 2 √ó 3\n      i     j  bday\n  &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1     8    10   255\n2    16    30    66\n\n[[6]]\n# A tibble: 0 √ó 0\n\n[[7]]\n# A tibble: 1 √ó 3\n      i     j  bday\n  &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1    11    23   333\n\n[[8]]\n# A tibble: 1 √ó 3\n      i     j  bday\n  &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1     2    17   328\n\n[[9]]\n# A tibble: 2 √ó 3\n      i     j  bday\n  &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1     2     4   283\n2    13    21    28\n\n[[10]]\n# A tibble: 2 √ó 3\n      i     j  bday\n  &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1     8    23   135\n2    11    12   204\n\n\n\nNumber of shared birthdays per room:\n\n# Now just get the # shared bdays\nshared_per_room &lt;- rooms_df %&gt;%\n    group_by(room_num) %&gt;%\n    group_map(~ get_shared_bdays(.x, is_grouped = TRUE, return_num=TRUE))\nshared_per_room &lt;- unlist(shared_per_room)\nshared_per_room\n\n [1] 3 0 1 1 2 0 1 1 2 2\n\n\n\n\\(\\widehat{\\Pr}(\\text{shared})\\)\n\n\n\n[1] 0.8"
  },
  {
    "objectID": "w03/slides.html#section-7",
    "href": "w03/slides.html#section-7",
    "title": "Week 3: Conditional Probability",
    "section": "",
    "text": "How about A THOUSAND ROOMS?\n\n\n\n  [1] 1 0 0 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 0 1 1\n [38] 1 0 1 0 1 1 1 0 0 1 0 0 0 1 1 1 0 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1\n [75] 1 0 1 0 1 0 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 1 0 1\n\n\n\n\\(\\widehat{\\Pr}(\\text{shared bday})\\)?\n\n\n\n[1] 0.72\n\n\n\nThe analytic solution: \\(\\Pr(\\text{shared} \\mid k\\text{ people in room}) = 1 - \\frac{366!}{366^{k}(366-k)!}\\)\nIn our case: \\(1 - \\frac{366!}{366^{30}(366-30)!} = 1 - \\frac{366!}{366^{30}336!} = 1 - \\frac{\\prod_{i=337}^{366}i}{366^{30}}\\)\nR can juust barely handle these numbers:\n\n\n\n[1] 0.7053034"
  },
  {
    "objectID": "w03/slides.html#wrapping-up",
    "href": "w03/slides.html#wrapping-up",
    "title": "Week 3: Conditional Probability",
    "section": "Wrapping Up",
    "text": "Wrapping Up"
  },
  {
    "objectID": "w03/slides.html#final-note-functions-of-random-variables",
    "href": "w03/slides.html#final-note-functions-of-random-variables",
    "title": "Week 3: Conditional Probability",
    "section": "Final Note: Functions of Random Variables",
    "text": "Final Note: Functions of Random Variables\n\n\\(X \\sim U[0,1], Y \\sim U[0,1]\\).\n\\(P(Y &lt; X^2)\\)?\nThe hard way: solve analytically\nThe easy way: simulate!"
  },
  {
    "objectID": "w03/slides.html#lab-2-demonstrations",
    "href": "w03/slides.html#lab-2-demonstrations",
    "title": "Week 3: Conditional Probability",
    "section": "Lab 2 Demonstrations",
    "text": "Lab 2 Demonstrations\nLab 2 Demonstrations"
  },
  {
    "objectID": "w03/slides.html#lab-2-assignment-overview",
    "href": "w03/slides.html#lab-2-assignment-overview",
    "title": "Week 3: Conditional Probability",
    "section": "Lab 2 Assignment Overview",
    "text": "Lab 2 Assignment Overview\nLab 2 Assignment"
  },
  {
    "objectID": "w03/index.html",
    "href": "w03/index.html",
    "title": "Week 3: Conditional Probability",
    "section": "",
    "text": "Open slides in new window ‚Üí"
  },
  {
    "objectID": "w03/index.html#recap-1",
    "href": "w03/index.html#recap-1",
    "title": "Week 3: Conditional Probability",
    "section": "Recap",
    "text": "Recap\n\nLogic \\(\\rightarrow\\) Set Theory \\(\\rightarrow\\) Probability Theory\nEntirety of probability theory can be derived from two axioms:\n\n\n\n\n\n\n\nThe Entirety of Probability Theory Follows From‚Ä¶\n\n\n\nAxiom 1 (Unitarity): \\(\\Pr(\\Omega) = 1\\) (The probability that something happens is 1)\nAxiom 2 (\\(\\sigma\\)-additivity): For mutually-exclusive events \\(E_1, E_2, \\ldots\\),\n\\[\n\\underbrace{\\Pr\\left(\\bigcup_{i=1}^{\\infty}E_i\\right)}_{\\Pr(E_1\\text{ occurs }\\vee E_2\\text{ occurs } \\vee \\cdots)} = \\underbrace{\\sum_{i=1}^{\\infty}\\Pr(E_i)}_{\\Pr(E_1\\text{ occurs}) + \\Pr(E_2\\text{ occurs}) + \\cdots}\n\\]\n\n\n\nBut what does ‚Äúmutually exclusive‚Äù mean‚Ä¶?"
  },
  {
    "objectID": "w03/index.html#venn-diagrams-sets",
    "href": "w03/index.html#venn-diagrams-sets",
    "title": "Week 3: Conditional Probability",
    "section": "Venn Diagrams: Sets",
    "text": "Venn Diagrams: Sets\n\n\n\n\n\n\n\n\n\\[\n\\begin{align*}\n&A = \\{0, 1, 2\\}, \\; B = \\{4, 5, 6\\} \\\\\n&\\implies A \\cap B = \\varnothing\n\\end{align*}\n\\]\nFigure¬†1: Mutually-exclusive (disjoint) sets\n\n\n\n\n\n\n\n\\[\n\\begin{align*}\n&A = \\{1, 2, 3\\}, \\; B = \\{3, 4, 5\\} \\\\\n&\\implies A \\cap B = \\{3\\}\n\\end{align*}\n\\]\nFigure¬†2: Non-mutually-exclusive sets"
  },
  {
    "objectID": "w03/index.html#venn-diagrams-events-dice",
    "href": "w03/index.html#venn-diagrams-events-dice",
    "title": "Week 3: Conditional Probability",
    "section": "Venn Diagrams: Events (Dice)",
    "text": "Venn Diagrams: Events (Dice)\n\\[\n\\begin{align*}\nA &= \\{\\text{Roll is even}\\} = \\{2, 4, 6\\} \\\\\nB &= \\{\\text{Roll is odd}\\} = \\{1, 3, 5\\} \\\\\nC &= \\{\\text{Roll is in Fibonnaci sequence}\\} = \\{1, 2, 3, 5\\}\n\\end{align*}\n\\]\n\n\n\n\n\n\n\n\n\n\nSet 1\nSet 2\nIntersection\nMutually Exclusive?\nCan Happen Simultaneously?\n\n\n\n\n\\(A\\)\n\\(B\\)\n\\(A \\cap B = \\varnothing\\)\nYes\nNo\n\n\n\\(A\\)\n\\(C\\)\n\\(A \\cap C = \\{2\\}\\)\nNo\nYes\n\n\n\\(B\\)\n\\(C\\)\n\\(B \\cap C = \\{1, 3, 5\\}\\)\nNo\nYes"
  },
  {
    "objectID": "w03/index.html#rules-of-probability",
    "href": "w03/index.html#rules-of-probability",
    "title": "Week 3: Conditional Probability",
    "section": "‚ÄúRules‚Äù of Probability",
    "text": "‚ÄúRules‚Äù of Probability\n\n(Remember: not ‚Äúrules‚Äù but ‚Äúfacts resulting from the logic \\(\\leftrightarrow\\) probability connection‚Äù)\n\n\n\n\n\n\n\n‚ÄúRules‚Äù of Probability\n\n\n\nFor logical predicates \\(p, q \\in \\{T, F\\}\\), events \\(P, Q\\) defined so \\(P\\) = event that \\(p\\) becomes true, \\(Q\\) = event that \\(q\\) becomes true,\n\nLogical AND = Probabilistic Multiplication\n\n\\[\n\\Pr(p \\wedge q) = \\Pr(P \\cap Q) = \\Pr(P) \\cdot \\Pr(Q)\n\\]\n\nLogical OR = Probabilistic Addition\n\n\\[\n\\Pr(p \\vee q) = \\Pr(P \\cup Q) = \\Pr(P) + \\Pr(Q) - \\underbrace{\\Pr(P \\cap Q)}_{\\text{(see rule 1)}}\n\\]\n\nLogical NOT = Probabilistic Complement\n\n\\[\n\\Pr(\\neg p) = \\Pr(P^c) = 1 - \\Pr(P)\n\\]"
  },
  {
    "objectID": "w03/index.html#conditional-probability",
    "href": "w03/index.html#conditional-probability",
    "title": "Week 3: Conditional Probability",
    "section": "Conditional Probability",
    "text": "Conditional Probability\n\nUsually if someone asks you probabilistic questions, like\n\n‚ÄúWhat is the likelihood that [our team] wins?‚Äù\n‚ÄúDo you think it will rain tomorrow?‚Äù and so on\n\nYou don‚Äôt guess a random number, you consider and incorporate evidence.\nExample: \\(\\Pr(\\text{rain})\\) on its own, without any other info? A tough question‚Ä¶ maybe \\(0.5\\)?\nIn reality, we would think about\n\n\\(\\Pr(\\text{rain} \\mid \\text{month of the year})\\)\n\\(\\Pr(\\text{rain} \\mid \\text{where we live})\\)\n\\(\\Pr(\\text{rain} \\mid \\text{did it rain yesterday?})\\)\n\nPsychologically, breaks down into two steps: (1) Think of a baseline probability, (2) Update baseline probability to incorporate relevant evidence (more on this in a bit‚Ä¶)\nAlso recall from last week: all probability is conditional probability, even if just conditioned on ‚Äúsomething happened‚Äù (\\(\\Omega\\), the thing defined so \\(\\Pr(\\Omega) = 1\\))"
  },
  {
    "objectID": "w03/index.html#na√Øve-definition-2.0",
    "href": "w03/index.html#na√Øve-definition-2.0",
    "title": "Week 3: Conditional Probability",
    "section": "Na√Øve Definition 2.0",
    "text": "Na√Øve Definition 2.0\n\n\n\n\n\n\n[Slightly Less] Na√Øve Definition of Probability\n\n\n\n\\[\n\\Pr(A \\mid B) = \\frac{\\text{\\# of Desired Outcomes in world where }B\\text{ happened}}{\\text{\\# Total outcomes in world where }B\\text{ happened}} = \\frac{|B \\cap A|}{|B|}\n\\]\n\n\n\n\n\n\n\n\n\n\nWorld Name\nWeather in World\nLikelihood of Rain Today\n\n\n\n\n\\(R\\)\nRained for the past 5 days\n\\(\\Pr(\\text{rain} \\mid R) &gt; 0.5\\)\n\n\n\\(M\\)\nMix of rain and non-rain over past 5 days\n\\(\\Pr(\\text{rain} \\mid M) \\approx 0.5\\)\n\n\n\\(S\\)\nSunny for the past 5 days\n\\(\\Pr(\\text{rain} \\mid S) &lt; 0.5\\)"
  },
  {
    "objectID": "w03/index.html#law-of-total-probability",
    "href": "w03/index.html#law-of-total-probability",
    "title": "Week 3: Conditional Probability",
    "section": "Law of Total Probability",
    "text": "Law of Total Probability\n\nSuppose the events \\(B_1, \\ldots, B_k\\) form a partition of the space \\(S\\) and \\(\\Pr(B_j) &gt; 0 \\forall j\\).\nThen, for every event \\(A\\) in \\(S\\),\n\n\\[\n\\Pr(A) = \\sum_{i=1}^k \\Pr(B_j)\\Pr(A \\mid B_j)\n\\]\n\nProbability of an event is the sum of its conditional probabilities across all conditions.\nIn other words: \\(A\\) is some event, \\(B_1, \\ldots, B_n\\) are mutually exclusive events filling entire sample-space, then\n\n\\[\n\\Pr(A) = \\Pr(A \\mid B_1)\\Pr(B_1) + \\Pr(A \\mid B_2)\\Pr(B_2) + \\cdots + \\Pr(A \\mid B_n)\\Pr(B_n)\n\\]\ni.e.¬†Compute the probability by summing over all possible cases."
  },
  {
    "objectID": "w03/index.html#example",
    "href": "w03/index.html#example",
    "title": "Week 3: Conditional Probability",
    "section": "Example",
    "text": "Example\n\nProbabilities of completing a job on time, with and without rain, are 0.42 and 0.90 respectively.\nProbability it will rain is 0.45. What is the probability the job will be completed on time?\n\\(A\\) = job will be completed on time, \\(B\\) = rain\n\n\\[\n\\Pr(B) = 0.45 \\implies \\Pr(B^c) = 1 - \\Pr(B) = 0.55.\n\\]\n\nNote: Events \\(B\\) and \\(B^c\\) are exclusive and form partitions of the sample space \\(S\\)\nWe know \\(\\Pr(A \\mid B) = 0.24\\), \\(\\Pr(A \\mid B^c) = 0.9\\).\nBy the Law of Total Probability, we have\n\n\\[\n\\begin{align*}\n\\Pr(A) &= \\Pr(B)\\Pr(A \\mid B) + \\Pr(B^c)\\Pr(A \\mid B^c) \\\\\n&= 0.45(0.42) + 0.55(0.9) = 0.189 + 0.495 = 0684.\n\\end{align*}\n\\]\nSo, the probability that the job will be completed on time is 0.684. (source)"
  },
  {
    "objectID": "w03/index.html#deriving-bayes-theorem",
    "href": "w03/index.html#deriving-bayes-theorem",
    "title": "Week 3: Conditional Probability",
    "section": "Deriving Bayes‚Äô Theorem",
    "text": "Deriving Bayes‚Äô Theorem\n\nLiterally just a re-writing of the conditional probability definition (don‚Äôt be scared)!\n\n\n\n\nFor two events \\(A\\) and \\(B\\), definition of conditional probability says that\n\n\\[\n\\begin{align*}\n\\Pr(A \\mid B) &= \\frac{\\Pr(A \\cap B)}{\\Pr(B)} \\tag{1} \\\\\n\\Pr(B \\mid A) &= \\frac{\\Pr(B \\cap A)}{\\Pr(A)} \\tag{2}\n\\end{align*}\n\\]\n\nMultiply to get rid of fractions\n\n\\[\n\\begin{align*}\n\\Pr(A \\mid B)\\Pr(B) &= \\Pr(A \\cap B) \\tag{1*} \\\\\n\\Pr(B \\mid A)\\Pr(A) &= \\Pr(B \\cap A) \\tag{2*}\n\\end{align*}\n\\]\n\n\nBut set intersection is associative (just like multiplication‚Ä¶), \\(A \\cap B = B \\cap A\\)! So, we know LHS of \\((\\text{1*})\\) = LHS of \\((\\text{2*})\\):\n\n\\[\n\\Pr(A \\mid B)\\Pr(B) = \\Pr(B \\mid A)\\Pr(A)\n\\]\n\nDivide both sides by \\(\\Pr(B)\\) to get a new definition of \\(\\Pr(A \\mid B)\\), Bayes‚Äô Theorem!\n\n\n\n\\[\n\\boxed{\\Pr(A \\mid B) = \\frac{\\Pr(B \\mid A)\\Pr(A)}{\\Pr(B)}}\n\\]\nFigure¬†3: Bayes‚Äô Theorem"
  },
  {
    "objectID": "w03/index.html#why-is-this-helpful",
    "href": "w03/index.html#why-is-this-helpful",
    "title": "Week 3: Conditional Probability",
    "section": "Why Is This Helpful?",
    "text": "Why Is This Helpful?\n\n\n\n\n\n\nBayes‚Äô Theorem\n\n\n\nFor any two events \\(A\\) and \\(B\\), \\[\n\\Pr(A \\mid B) = \\frac{\\Pr(B \\mid A)\\Pr(A)}{\\Pr(B)}\n\\]\n\n\n\nIn words (as exciting as I can make it, for now): Bayes‚Äô Theorem allows us to take information about \\(B \\mid A\\) and use it to infer information about \\(A \\mid B\\)\nIt isn‚Äôt until you work through some examples that this becomes mind-blowing, the most powerful equation we have for inferring unknowns from knowns‚Ä¶\nConsider \\(A = \\{\\text{person has disease}\\}\\), \\(B = \\{\\text{person tests positive for disease}\\}\\)\n\nIs \\(A\\) observable on its own? No, but‚Ä¶\n\nIs \\(B\\) observable on its own? Yes, and\nCan we infer information about \\(A\\) from knowing \\(B\\)? Also Yes, thanks to Bayes!\n\nTherefore, we can use \\(B\\) to infer information about \\(A\\), i.e., calculate \\(\\Pr(A \\mid B)\\)‚Ä¶"
  },
  {
    "objectID": "w03/index.html#why-is-this-helpful-for-data-science",
    "href": "w03/index.html#why-is-this-helpful-for-data-science",
    "title": "Week 3: Conditional Probability",
    "section": "Why Is This Helpful for Data Science?",
    "text": "Why Is This Helpful for Data Science?\n\nIt merges probability theory and hypothesis testing into a single framework:\n\n\\[\n\\Pr(\\text{hypothesis} \\mid \\text{data}) = \\frac{\\Pr(\\text{data} \\mid \\text{hypothesis})\\Pr(\\text{hypothesis})}{\\Pr(\\text{data})}\n\\]"
  },
  {
    "objectID": "w03/index.html#probability-forwards-and-backwards",
    "href": "w03/index.html#probability-forwards-and-backwards",
    "title": "Week 3: Conditional Probability",
    "section": "Probability Forwards and Backwards",
    "text": "Probability Forwards and Backwards\n\nTwo discrete RVs:\n\nWeather on a given day, \\(W \\in \\{\\textsf{Rain},\\textsf{Sun}\\}\\)\nAction that day, \\(A \\in \\{\\textsf{Go}, \\textsf{Stay}\\}\\): go to party or stay in and watch movie\n\nData-generating process: if \\(\\textsf{Sun}\\), rolls a die \\(R\\) and goes out unless \\(R = 6\\). If \\(\\textsf{Rain}\\), flips a coin and goes out if \\(\\textsf{H}\\).\nProbabilistic Graphical Model (PGM):"
  },
  {
    "objectID": "w03/index.html#section",
    "href": "w03/index.html#section",
    "title": "Week 3: Conditional Probability",
    "section": "",
    "text": "So, if we know \\(W = \\textsf{Sun}\\), what is \\(P(A = \\textsf{Go})\\)? \\[\n\\begin{align*}\nP(A = \\textsf{Go} \\mid W) &= 1 - P(R = 6) \\\\\n&= 1 - \\frac{1}{6} = \\frac{5}{6}\n\\end{align*}\n\\]\nConditional probability lets us go forwards (left to right):\n\n\n\n\n\n\n\nBut what if we want to perform inference going backwards?"
  },
  {
    "objectID": "w03/index.html#section-1",
    "href": "w03/index.html#section-1",
    "title": "Week 3: Conditional Probability",
    "section": "",
    "text": "If we see Ana at the party, we know \\(A = \\textsf{Go}\\)\nWhat does this tell us about the weather?\nIntuitively, we should increase our degree of belief that \\(W = \\textsf{Sun}\\). But, by how much?\nWe don‚Äôt know \\(P(W \\mid A)\\), only \\(P(A \\mid W)\\)‚Ä¶"
  },
  {
    "objectID": "w03/index.html#section-2",
    "href": "w03/index.html#section-2",
    "title": "Week 3: Conditional Probability",
    "section": "",
    "text": "\\[\nP(W = \\textsf{Sun} \\mid A = \\textsf{Go}) = \\frac{\\overbrace{P(A = \\textsf{Go} \\mid W = \\textsf{Sun})}^{5/6~ ‚úÖ}\\overbrace{P(W = \\textsf{Sun})}^{‚ùì}}{\\underbrace{P(A = \\textsf{Go})}_{‚ùì}}\n\\]\n\nWe‚Äôve seen \\(P(W = \\textsf{Sun})\\) before, it‚Äôs our prior: the probability without having any additional relevant knowledge. So, let‚Äôs say 50/50. \\(P(W = \\textsf{Sun}) = \\frac{1}{2}\\)\nIf we lived in Seattle, we could pick \\(P(W = \\textsf{Sun}) = \\frac{1}{4}\\)"
  },
  {
    "objectID": "w03/index.html#section-3",
    "href": "w03/index.html#section-3",
    "title": "Week 3: Conditional Probability",
    "section": "",
    "text": "\\[\nP(W = \\textsf{Sun} \\mid A = \\textsf{Go}) = \\frac{\\overbrace{P(A = \\textsf{Go} \\mid W = \\textsf{Sunny})}^{5/6~ ‚úÖ}\\overbrace{P(W = \\textsf{Sun})}^{1/2~ ‚úÖ}}{\\underbrace{P(A = \\textsf{Go})}_{‚ùì}}\n\\]\n\n\\(P(A = \\textsf{Go})\\) is trickier: the probability that Ana goes out regardless of what the weather is. But there are only two possible weather outcomes! So we just compute\n\n\\[\n\\begin{align*}\n&P(A = \\textsf{Go}) = \\sum_{\\omega \\in S(W)}P(A = \\textsf{Go}, \\omega) = \\sum_{\\omega \\in S(W)}P(A = \\textsf{Go} \\mid \\omega)P(\\omega) \\\\\n&= P(A = \\textsf{Go} \\mid W = \\textsf{Rain})P(W = \\textsf{Rain}) + P(A = \\textsf{Go} \\mid W = \\textsf{Sun})P(W = \\textsf{Sun}) \\\\\n&= \\left( \\frac{1}{2} \\right)\\left( \\frac{1}{2} \\right) + \\left( \\frac{5}{6} \\right)\\left( \\frac{1}{2} \\right) = \\frac{1}{4} + \\frac{5}{12} = \\frac{2}{3}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "w03/index.html#putting-it-all-together",
    "href": "w03/index.html#putting-it-all-together",
    "title": "Week 3: Conditional Probability",
    "section": "Putting it All Together",
    "text": "Putting it All Together\n\\[\n\\begin{align*}\nP(W = \\textsf{Sun} \\mid A = \\textsf{Go}) &= \\frac{\\overbrace{P(A = \\textsf{Go} \\mid W = \\textsf{Sunny})}^{3/4~ ‚úÖ}\\overbrace{P(W = \\textsf{Sun})}^{1/2~ ‚úÖ}}{\\underbrace{P(A = \\textsf{Go})}_{1/2~ ‚úÖ}} \\\\\n&= \\frac{\\left(\\frac{3}{4}\\right)\\left(\\frac{1}{2}\\right)}{\\frac{1}{2}} = \\frac{\\frac{3}{8}}{\\frac{1}{2}} = \\frac{3}{4}.\n\\end{align*}\n\\]\n\nGiven that we see Ana at the party, we should update our beliefs, so that \\(P(W = \\textsf{Sun}) = \\frac{3}{4}, P(W = \\textsf{Rain}) = \\frac{1}{4}\\)."
  },
  {
    "objectID": "w03/index.html#a-scarier-example",
    "href": "w03/index.html#a-scarier-example",
    "title": "Week 3: Conditional Probability",
    "section": "A Scarier Example",
    "text": "A Scarier Example\n\nBo worries he has a rare disease. He takes a test with 99% accuracy and tests positive. What‚Äôs the probability Bo has the disease? (Intuition: 99%? ‚Ä¶Let‚Äôs do the math!)\n\n\n\n\n\\(H \\in \\{\\textsf{sick}, \\textsf{healthy}\\}, T \\in \\{\\textsf{T}^+, \\textsf{T}^-\\}\\)\nThe test: 99% accurate. \\(\\Pr(T = \\textsf{T}^+ \\mid H = \\textsf{sick}) = 0.99\\), \\(\\Pr(T = \\textsf{T}^- \\mid H = \\textsf{healthy}) = 0.99\\).\nThe disease: 1 in 10K. \\(\\Pr(H = \\textsf{sick}) = \\frac{1}{10000}\\)\nWhat do we want to know? \\(\\Pr(H = \\textsf{sick} \\mid T = \\textsf{T}^+)\\)\nHow do we get there?\n\n\n\n\n\nThis photo, originally thought to be of Thomas Bayes, turns out to be probably someone else‚Ä¶ \\(\\Pr(\\textsf{Bayes})\\)?\n\n\n\n\n\n\\(H\\) for health, \\(T\\) for test result\nPhoto credit: https://thedatascientist.com/wp-content/uploads/2019/04/reverend-thomas-bayes.jpg"
  },
  {
    "objectID": "w03/index.html#section-4",
    "href": "w03/index.html#section-4",
    "title": "Week 3: Conditional Probability",
    "section": "",
    "text": "\\[\n\\begin{align*}\n\\Pr(H = \\textsf{sick} \\mid T = \\textsf{T}^+) &= \\frac{\\Pr(T = \\textsf{T}^+ \\mid H = \\textsf{sick})\\Pr(H = \\textsf{sick})}{\\Pr(T = \\textsf{T}^+)} \\\\\n&= \\frac{(0.99)\\left(\\frac{1}{10000}\\right)}{(0.99)\\left( \\frac{1}{10000} \\right) + (0.01)\\left( \\frac{9999}{10000} \\right)}\n\\end{align*}\n\\]\n\np_sick &lt;- 1 / 10000\np_healthy &lt;- 1 - p_sick\np_pos_given_sick &lt;- 0.99\np_neg_given_sick &lt;- 1 - p_pos_given_sick\np_neg_given_healthy &lt;- 0.99\np_pos_given_healthy &lt;- 1 - p_neg_given_healthy\nnumer &lt;- p_pos_given_sick * p_sick\ndenom1 &lt;- numer\ndenom2 &lt;- p_pos_given_healthy * p_healthy\nfinal_prob &lt;- numer / (denom1 + denom2)\nfinal_prob\n\n[1] 0.009803922\n\n\n\n‚Ä¶ Less than 1% üò±"
  },
  {
    "objectID": "w03/index.html#proof-in-the-pudding",
    "href": "w03/index.html#proof-in-the-pudding",
    "title": "Week 3: Conditional Probability",
    "section": "Proof in the Pudding",
    "text": "Proof in the Pudding\n\nLet‚Äôs generate a dataset of 5,000 people, using \\(\\Pr(\\textsf{Disease}) = \\frac{1}{10000}\\)\n\n\n\nCode\nlibrary(tibble)\nlibrary(dplyr)\n# Disease rarity\np_disease &lt;- 1 / 10000\n# 1K people\nnum_people &lt;- 10000\n# Give them ids\nppl_df &lt;- tibble(id=seq(1,num_people))\n# Whether they have the disease or not\nhas_disease &lt;- rbinom(num_people, 1, p_disease)\nppl_df &lt;- ppl_df %&gt;% mutate(has_disease=has_disease)\ndisp(ppl_df %&gt;% head())"
  },
  {
    "objectID": "w03/index.html#binary-variable-trick",
    "href": "w03/index.html#binary-variable-trick",
    "title": "Week 3: Conditional Probability",
    "section": "Binary Variable Trick",
    "text": "Binary Variable Trick\n\nSince has_disease \\(\\in \\{0, 1\\}\\), we can use\n\nsum(has_disease) to obtain the count of people with the disease, or\nmean(has_disease) to obtain the proportion of people who have the disease\n\nTo see this (or, if you forget in the future), just make a fake dataset with a binary variable and 3 rows, and think about sums vs.¬†means of that variable:\n\n\n\n\n\nCode\nbinary_df &lt;- tibble(\n  id=c(1,2,3),\n  x=c(0,1,0)\n)\ndisp(binary_df)\n\n\n\n\n\n\n\n\nTaking the sum tells us: one row where x == 1:\n\n\nCode\nsum(binary_df$x)\n\n\n[1] 1\n\n\nTaking the mean tells us: 1/3 of rows have x == 1:\n\n\nCode\nmean(binary_df$x)\n\n\n[1] 0.3333333"
  },
  {
    "objectID": "w03/index.html#applying-this-to-the-disease-data",
    "href": "w03/index.html#applying-this-to-the-disease-data",
    "title": "Week 3: Conditional Probability",
    "section": "Applying This to the Disease Data",
    "text": "Applying This to the Disease Data\n\nIf we want the number of people who have the disease:\n\n\n\nCode\n# Compute the *number* of people who have the disease\nsum(ppl_df$has_disease)\n\n\n[1] 1\n\n\n\nIf we want the proportion of people who have the disease:\n\n\n\nCode\n# Compute the *proportion* of people who have the disease\nmean(ppl_df$has_disease)\n\n\n[1] 1e-04\n\n\n\n(And if you dislike scientific notation like I do‚Ä¶)\n\n\n\nCode\nformat(mean(ppl_df$has_disease), scientific = FALSE)\n\n\n[1] \"0.0001\"\n\n\n\n(Foreshadowing Monte Carlo methods)"
  },
  {
    "objectID": "w03/index.html#data-generating-process-test-results",
    "href": "w03/index.html#data-generating-process-test-results",
    "title": "Week 3: Conditional Probability",
    "section": "Data-Generating Process: Test Results",
    "text": "Data-Generating Process: Test Results\n\n\nCode\nlibrary(dplyr)\n# Data Generating Process\ntake_test &lt;- function(is_sick) {\n  if (is_sick) {\n    return(rbinom(1,1,p_pos_given_sick))\n  } else {\n    return(rbinom(1,1,p_pos_given_healthy))\n  }\n}\nppl_df['test_result'] &lt;- unlist(lapply(ppl_df$has_disease, take_test))\nnum_positive &lt;- sum(ppl_df$test_result)\np_positive &lt;- mean(ppl_df$test_result)\nwriteLines(paste0(num_positive,\" positive tests / \",num_people,\" total = \",p_positive))\n\n\n111 positive tests / 10000 total = 0.0111\n\n\n\ndisp(ppl_df %&gt;% head(50), obs_per_page = 3)"
  },
  {
    "objectID": "w03/index.html#zooming-in-on-positive-tests",
    "href": "w03/index.html#zooming-in-on-positive-tests",
    "title": "Week 3: Conditional Probability",
    "section": "Zooming In On Positive Tests",
    "text": "Zooming In On Positive Tests\n\n\n\npos_ppl &lt;- ppl_df %&gt;% filter(test_result == 1)\ndisp(pos_ppl, obs_per_page = 10)\n\n\n\n\n\n\n\n\nBo doesn‚Äôt have it, and neither do 110 of the 111 total people who tested positive!\nBut, in the real world, we only observe \\(T\\)"
  },
  {
    "objectID": "w03/index.html#zooming-in-on-disease-havers",
    "href": "w03/index.html#zooming-in-on-disease-havers",
    "title": "Week 3: Conditional Probability",
    "section": "Zooming In On Disease-Havers",
    "text": "Zooming In On Disease-Havers\n\nWhat if we look at only those who actually have the disease? Maybe the cost of 111 people panicking is worth it if we correctly catch those who do have it?\n\n\n\nCode\ndisp(ppl_df[ppl_df$has_disease == 1,])\n\n\n\n\n\n\n\nIs this always going to be the case?\n\n\n\n\nNum with disease: 1\nProportion with disease: 0.0002\nNumber of positive tests: 53\n\n\n\n\n\n\n\n\n\ndisp(simulate_disease(5000, 1/10000))\n\nNum with disease: 3\nProportion with disease: 0.0006\nNumber of positive tests: 63"
  },
  {
    "objectID": "w03/index.html#worst-case-worlds",
    "href": "w03/index.html#worst-case-worlds",
    "title": "Week 3: Conditional Probability",
    "section": "Worst-Case Worlds",
    "text": "Worst-Case Worlds\n\n\n\nfor (i in seq(1,1000)) {\n  sim_result &lt;- simulate_disease(5000, 1/10000, verbose = FALSE, return_all_detected = FALSE, return_df = FALSE, return_info = TRUE)\n  if (!sim_result$all_detected) {\n    writeLines(paste0(\"World #\",i,\" / 1000 (\",sim_result$num_people,\" people):\"))\n    print(sim_result$df)\n    writeLines('\\n')\n  }\n}\n\nWorld #413 / 1000 (5000 people):\n# A tibble: 2 √ó 3\n     id has_disease test_result\n  &lt;int&gt;       &lt;int&gt;       &lt;int&gt;\n1  2617           1           1\n2  4192           1           0\n\n\nWorld #501 / 1000 (5000 people):\n# A tibble: 1 √ó 3\n     id has_disease test_result\n  &lt;int&gt;       &lt;int&gt;       &lt;int&gt;\n1  3669           1           0\n\n\nWorld #670 / 1000 (5000 people):\n# A tibble: 1 √ó 3\n     id has_disease test_result\n  &lt;int&gt;       &lt;int&gt;       &lt;int&gt;\n1  1979           1           0\n\n\nWorld #698 / 1000 (5000 people):\n# A tibble: 1 √ó 3\n     id has_disease test_result\n  &lt;int&gt;       &lt;int&gt;       &lt;int&gt;\n1  1272           1           0\n\nformat(4 / 5000000, scientific = FALSE)\n\n[1] \"0.0000008\"\n\n\n\nHow unlikely is this? Math:\n\\[\n\\begin{align*}\n\\Pr(\\textsf{T}^- \\cap \\textsf{Sick}) &= \\Pr(\\textsf{T}^- \\mid \\textsf{Sick})\\Pr(\\textsf{Sick}) \\\\\n&= (0.01)\\frac{1}{10000} \\\\\n&= \\frac{1}{1000000}\n\\end{align*}\n\\]\nComputers:\n\nresult_df &lt;- simulate_disease(1000000, 1/10000, verbose = FALSE, return_full_df = TRUE)\nfalse_negatives &lt;- result_df[result_df$has_disease == 1 & result_df$test_result == 0,]\nnum_false_negatives &lt;- nrow(false_negatives)\nwriteLines(paste0(\"False Negatives: \",num_false_negatives,\", Total Cases: \", nrow(result_df)))\n\nFalse Negatives: 1, Total Cases: 1000000\n\nfalse_negative_rate &lt;- num_false_negatives / nrow(result_df)\nfalse_negative_rate_decimal &lt;- format(false_negative_rate, scientific = FALSE)\nwriteLines(paste0(\"False Negative Rate: \", false_negative_rate_decimal))\n\nFalse Negative Rate: 0.000001\n\n\n(Perfect match!)"
  },
  {
    "objectID": "w03/index.html#bayes-takeaway",
    "href": "w03/index.html#bayes-takeaway",
    "title": "Week 3: Conditional Probability",
    "section": "Bayes: Takeaway",
    "text": "Bayes: Takeaway\n\nBayesian approach allows new evidence to be weighed against existing evidence, with statistically principled way to derive these weights:\n\n\\[\n\\begin{array}{ccccc}\n\\Pr_{\\text{post}}(\\mathcal{H}) &\\hspace{-6mm}\\propto &\\hspace{-6mm} \\Pr(X \\mid \\mathcal{H}) &\\hspace{-6mm} \\times &\\hspace{-6mm} \\Pr_{\\text{pre}}(\\mathcal{H}) \\\\\n\\text{Posterior} &\\hspace{-6mm}\\propto &\\hspace{-6mm}\\text{Evidence} &\\hspace{-6mm} \\times &\\hspace{-6mm} \\text{Prior}\n\\end{array}\n\\]"
  },
  {
    "objectID": "w03/index.html#monte-carlo-methods-overview",
    "href": "w03/index.html#monte-carlo-methods-overview",
    "title": "Week 3: Conditional Probability",
    "section": "Monte Carlo Methods: Overview",
    "text": "Monte Carlo Methods: Overview\n\nYou already saw an example, in our rare disease simulation!\nGenerally, using computers (rather than math, ‚Äúby hand‚Äù) to estimate probabilistic quantities\n\n\n\nPros:\n\nMost real-world processes have no analytic solution\nStep-by-step breakdown of complex processes\n\n\nCons:\n\nCan require immense computing power\n‚ö†Ô∏è Can generate incorrect answers ‚ö†Ô∏è\n\n\n\n\nBy step-by-step I mean, a lot of the time you are just walking through, generating the next column using previously-generated columns. Like we did in the example above, generating test_result based on has_disease."
  },
  {
    "objectID": "w03/index.html#birthday-problem",
    "href": "w03/index.html#birthday-problem",
    "title": "Week 3: Conditional Probability",
    "section": "Birthday Problem",
    "text": "Birthday Problem\n\n\n\n30 people gather in a room together. What is the probability that two of them share the same birthday?\nAnalytic solution is fun, but requires some thought‚Ä¶ Monte Carlo it!\n\n\n\n\nCode\ngen_bday_room &lt;- function(room_num=NULL) {\n  num_people &lt;- 30\n  num_days &lt;- 366\n  ppl_df &lt;- tibble(id=seq(1,num_people))\nbirthdays &lt;- sample(1:num_days, num_people,replace = T)\n  ppl_df['birthday'] &lt;- birthdays\n  if (!is.null(room_num)) {\n    ppl_df &lt;- ppl_df %&gt;% mutate(room_num=room_num) %&gt;% relocate(room_num)\n  }\n  return(ppl_df)\n}\nppl_df &lt;- gen_bday_room(1)\ndisp(ppl_df %&gt;% head()) #, obs_per_page = 3)"
  },
  {
    "objectID": "w03/index.html#section-5",
    "href": "w03/index.html#section-5",
    "title": "Week 3: Conditional Probability",
    "section": "",
    "text": "# Inefficient version (return_num=FALSE) is for: if you want tibbles of *all* shared bdays for each room\nget_shared_bdays &lt;- function(df, is_grouped=NULL, return_num=FALSE, return_bool=FALSE) {\n  bday_pairs &lt;- tibble()\n  for (i in 1:(nrow(df)-1)) {\n    i_data &lt;- df[i,]\n    i_bday &lt;- i_data$birthday\n    for (j in (i+1):nrow(df)) {\n      j_data &lt;- df[j,]\n      j_bday &lt;- j_data$birthday\n      # Check if they're the same\n      same_bday &lt;- i_bday == j_bday\n      if (same_bday) {\n        if (return_bool) {\n          return(1)\n        }\n        pair_data &lt;- tibble(i=i,j=j,bday=i_bday)\n        if (!is.null(is_grouped)) {\n          i_room &lt;- i_data$room_num\n          pair_data['room'] &lt;- i_room\n        }\n        bday_pairs &lt;- bind_rows(bday_pairs, pair_data)\n      }\n    }\n  }\n  if (return_bool) {\n    return(0)\n  }\n  if (return_num) {\n    return(nrow(bday_pairs))\n  }\n  return(bday_pairs)\n}\n#get_shared_bdays(ppl_df)\nget_shared_bdays(ppl_df)\n\n# A tibble: 2 √ó 3\n      i     j  bday\n  &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1     6    27   114\n2     8    28   337"
  },
  {
    "objectID": "w03/index.html#section-6",
    "href": "w03/index.html#section-6",
    "title": "Week 3: Conditional Probability",
    "section": "",
    "text": "Let‚Äôs try more rooms‚Ä¶\n\n\n\n# Get tibbles for each room\nlibrary(purrr)\ngen_bday_rooms &lt;- function(num_rooms) {\n  rooms_df &lt;- tibble()\n  for (r in seq(1, num_rooms)) {\n      cur_room &lt;- gen_bday_room(r)\n      rooms_df &lt;- bind_rows(rooms_df, cur_room)\n  }\n  return(rooms_df)\n}\nnum_rooms &lt;- 10\nrooms_df &lt;- gen_bday_rooms(num_rooms)\nrooms_df %&gt;% group_by(room_num) %&gt;% group_map(~ get_shared_bdays(.x, is_grouped=TRUE))\n\nWarning: Unknown or uninitialised column: `room_num`.\nUnknown or uninitialised column: `room_num`.\nUnknown or uninitialised column: `room_num`.\nUnknown or uninitialised column: `room_num`.\nUnknown or uninitialised column: `room_num`.\nUnknown or uninitialised column: `room_num`.\nUnknown or uninitialised column: `room_num`.\nUnknown or uninitialised column: `room_num`.\nUnknown or uninitialised column: `room_num`.\nUnknown or uninitialised column: `room_num`.\nUnknown or uninitialised column: `room_num`.\nUnknown or uninitialised column: `room_num`.\nUnknown or uninitialised column: `room_num`.\nUnknown or uninitialised column: `room_num`.\nUnknown or uninitialised column: `room_num`.\nUnknown or uninitialised column: `room_num`.\n\n\n[[1]]\n# A tibble: 0 √ó 0\n\n[[2]]\n# A tibble: 1 √ó 3\n      i     j  bday\n  &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1     7    19   242\n\n[[3]]\n# A tibble: 2 √ó 3\n      i     j  bday\n  &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1     2    30    45\n2     5    18    59\n\n[[4]]\n# A tibble: 1 √ó 3\n      i     j  bday\n  &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1     7    14   300\n\n[[5]]\n# A tibble: 1 √ó 3\n      i     j  bday\n  &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1     9    20   195\n\n[[6]]\n# A tibble: 4 √ó 3\n      i     j  bday\n  &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1     1    12   118\n2     1    28   118\n3     5    27   287\n4    12    28   118\n\n[[7]]\n# A tibble: 2 √ó 3\n      i     j  bday\n  &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1     9    18     6\n2    15    28    78\n\n[[8]]\n# A tibble: 1 √ó 3\n      i     j  bday\n  &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1     1    20    29\n\n[[9]]\n# A tibble: 2 √ó 3\n      i     j  bday\n  &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1     8     9    62\n2    10    24   321\n\n[[10]]\n# A tibble: 2 √ó 3\n      i     j  bday\n  &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1    16    20    86\n2    19    22   163\n\n\n\nNumber of shared birthdays per room:\n\n# Now just get the # shared bdays\nshared_per_room &lt;- rooms_df %&gt;%\n    group_by(room_num) %&gt;%\n    group_map(~ get_shared_bdays(.x, is_grouped = TRUE, return_num=TRUE))\n\nWarning: Unknown or uninitialised column: `room_num`.\nUnknown or uninitialised column: `room_num`.\nUnknown or uninitialised column: `room_num`.\nUnknown or uninitialised column: `room_num`.\nUnknown or uninitialised column: `room_num`.\nUnknown or uninitialised column: `room_num`.\nUnknown or uninitialised column: `room_num`.\nUnknown or uninitialised column: `room_num`.\nUnknown or uninitialised column: `room_num`.\nUnknown or uninitialised column: `room_num`.\nUnknown or uninitialised column: `room_num`.\nUnknown or uninitialised column: `room_num`.\nUnknown or uninitialised column: `room_num`.\nUnknown or uninitialised column: `room_num`.\nUnknown or uninitialised column: `room_num`.\nUnknown or uninitialised column: `room_num`.\n\nshared_per_room &lt;- unlist(shared_per_room)\nshared_per_room\n\n [1] 0 1 2 1 1 4 2 1 2 2\n\n\n\n\\(\\widehat{\\Pr}(\\text{shared})\\)\n\n\nsum(shared_per_room &gt; 0) / num_rooms\n\n[1] 0.9"
  },
  {
    "objectID": "w03/index.html#section-7",
    "href": "w03/index.html#section-7",
    "title": "Week 3: Conditional Probability",
    "section": "",
    "text": "How about A THOUSAND ROOMS?\n\n\nnum_rooms_many &lt;- 100\nmany_rooms_df &lt;- gen_bday_rooms(num_rooms_many)\nanyshared_per_room &lt;- many_rooms_df %&gt;%\n    group_by(room_num) %&gt;%\n    group_map(~ get_shared_bdays(.x, is_grouped = TRUE, return_bool = TRUE))\nanyshared_per_room &lt;- unlist(anyshared_per_room)\nanyshared_per_room\n\n  [1] 1 0 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 0\n [38] 1 0 1 1 1 1 0 0 1 0 1 1 0 0 1 0 1 1 1 1 1 1 0 1 1 0 1 0 1 0 1 1 0 0 1 1 0\n [75] 0 1 1 1 1 0 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0\n\n\n\n\\(\\widehat{\\Pr}(\\text{shared bday})\\)?\n\n\n# And now the probability estimate\nsum(anyshared_per_room &gt; 0) / num_rooms_many\n\n[1] 0.66\n\n\n\nThe analytic solution: \\(\\Pr(\\text{shared} \\mid k\\text{ people in room}) = 1 - \\frac{366!}{366^{k}(366-k)!}\\)\nIn our case: \\(1 - \\frac{366!}{366^{30}(366-30)!} = 1 - \\frac{366!}{366^{30}336!} = 1 - \\frac{\\prod_{i=337}^{366}i}{366^{30}}\\)\nR can juust barely handle these numbers:\n\n\n(exact_solution &lt;- 1 - (prod(seq(337,366))) / (366^30))\n\n[1] 0.7053034"
  },
  {
    "objectID": "w03/index.html#wrapping-up",
    "href": "w03/index.html#wrapping-up",
    "title": "Week 3: Conditional Probability",
    "section": "Wrapping Up",
    "text": "Wrapping Up\n\nlibrary(ggplot2)\noptions(ggplot2.discrete.colour = cbPalette)\nglobal_theme &lt;- ggplot2::theme_classic() + ggplot2::theme(\n    plot.title = element_text(hjust = 0.5, size = 18),\n    axis.title = element_text(size = 16),\n    axis.text = element_text(size = 14),\n    legend.title = element_text(size = 16, hjust = 0.5),\n    legend.text = element_text(size = 14),\n    legend.box.background = element_rect(colour = \"black\")\n)\nknitr::opts_chunk$set(fig.align = \"center\")\ng_pointsize &lt;- 6\n# Bday problem\ntrials_per_roomsize &lt;- 3\nbday_est_lbounds &lt;- c()\nbday_est_means &lt;- c()\nbday_est_ubounds &lt;- c()\nsample_sizes &lt;- c()\nfor (num_rooms_many in c(10,50,100,500, 1000)) {\n  cur_size_ests &lt;- c()\n  for (trial_num in seq(1,trials_per_roomsize)) {\n    many_rooms_df &lt;- gen_bday_rooms(num_rooms_many)\n    anyshared_per_room &lt;- many_rooms_df %&gt;%\n        group_by(room_num) %&gt;%\n        group_map(~ get_shared_bdays(.x, is_grouped = TRUE, return_bool = TRUE))\n    anyshared_per_room &lt;- unlist(anyshared_per_room)\n    cur_est &lt;- sum(anyshared_per_room &gt; 0) / num_rooms_many\n    cur_size_ests &lt;- c(cur_size_ests, cur_est)\n  }\n  bday_est_lbounds &lt;- c(bday_est_lbounds, min(cur_size_ests))\n  bday_est_ubounds &lt;- c(bday_est_ubounds, max(cur_size_ests))\n  bday_est_means &lt;- c(bday_est_means, mean(cur_size_ests))\n  sample_sizes &lt;- c(sample_sizes, num_rooms_many)\n}\nresult_df &lt;- tibble(n=sample_sizes,est=bday_est_means, lbound=bday_est_lbounds, ubound=bday_est_ubounds)\nbase_plot &lt;- ggplot(result_df, aes(x=n, y=est)) +\n  geom_point(aes(color=\"black\")) +\n  geom_line(color=\"black\") +\n  geom_ribbon(aes(ymin = lbound, ymax = ubound, fill = cbPalette[1]), alpha = 0.3) +\n      geom_hline(aes(yintercept = exact_solution, linetype = \"dashed\"), color = \"purple\") +\n      scale_color_manual(\"\", values = c(\"black\", \"purple\"), labels = c(\"Sample Mean X\", \"True Mean mu\")) +\n      scale_linetype_manual(\"\", values = \"dashed\", labels = \"True Mean mu\") +\n      scale_fill_manual(\"\", values = cbPalette[1], labels = \"95% CI\") +\n      global_theme +\n      theme(\n          legend.title = element_blank(),\n          legend.spacing.y = unit(0, \"mm\")\n      ) +\n      labs(\n          title = \"Monte Carlo Estimates of Birthday Problem Solution\",\n          x = \"n (Sample Size)\",\n          y = \"Estimate\"\n      )\nlog_plot &lt;- base_plot + scale_x_log10(breaks=c(10,100,1000,10000,100000), labels=c(\"10\",\"100\",\"1000\",\"10000\",\"100000\"))\nlog_plot"
  },
  {
    "objectID": "w03/index.html#final-note-functions-of-random-variables",
    "href": "w03/index.html#final-note-functions-of-random-variables",
    "title": "Week 3: Conditional Probability",
    "section": "Final Note: Functions of Random Variables",
    "text": "Final Note: Functions of Random Variables\n\n\\(X \\sim U[0,1], Y \\sim U[0,1]\\).\n\\(P(Y &lt; X^2)\\)?\nThe hard way: solve analytically\nThe easy way: simulate!"
  },
  {
    "objectID": "w03/index.html#lab-2-demonstrations",
    "href": "w03/index.html#lab-2-demonstrations",
    "title": "Week 3: Conditional Probability",
    "section": "Lab 2 Demonstrations",
    "text": "Lab 2 Demonstrations\nLab 2 Demonstrations"
  },
  {
    "objectID": "w03/index.html#lab-2-assignment-overview",
    "href": "w03/index.html#lab-2-assignment-overview",
    "title": "Week 3: Conditional Probability",
    "section": "Lab 2 Assignment Overview",
    "text": "Lab 2 Assignment Overview\nLab 2 Assignment"
  },
  {
    "objectID": "w04/index.html",
    "href": "w04/index.html",
    "title": "Week 4: Discrete Probability Distributions",
    "section": "",
    "text": "Open slides in new window ‚Üí"
  },
  {
    "objectID": "w04/index.html#discrete-vs.-continuous",
    "href": "w04/index.html#discrete-vs.-continuous",
    "title": "Week 4: Discrete Probability Distributions",
    "section": "Discrete vs.¬†Continuous",
    "text": "Discrete vs.¬†Continuous\n\n\n\nDiscrete = ‚ÄúEasy mode‚Äù: Based (intuitively) on sets\n\\(\\Pr(A)\\): Four marbles \\(\\{A, B, C, D\\}\\) in box, all equally likely, what is the probability I pull out \\(A\\)?\n\n\nlibrary(tibble)\nlibrary(ggplot2)\ndisc_df &lt;- tribble(\n  ~x, ~y, ~label,\n  0, 0, \"A\",\n  0, 1, \"B\",\n  1, 0, \"C\",\n  1, 1, \"D\"\n)\nggplot(disc_df, aes(x=x, y=y, label=label)) +\n    geom_point(size=g_pointsize) +\n    geom_text(\n      size=g_textsize,\n      hjust=1.5,\n      vjust=-0.5\n    ) +\n    xlim(-0.5,1.5) + ylim(-0.5,1.5) +\n    coord_fixed() +\n    dsan_theme(\"quarter\") +\n    labs(\n      title=\"Discrete Probability Space in N\"\n    )\n\n\n\n\n\\[\n\\Pr(A) = \\underbrace{\\frac{|\\{A\\}|}{|\\Omega|}}_{\\mathclap{\\small \\text{Probability }\\textbf{mass}}} = \\frac{1}{|\\{A,B,C,D\\}|} = \\frac{1}{4}\n\\]\n\n\nContinuous = ‚ÄúHard mode‚Äù: Based (intuitively) on areas\n\\(\\Pr(A)\\): If I throw a dart at this square, what is the probability that I hit region \\(A\\)?\n\n\nlibrary(ggforce)\nggplot(disc_df, aes(x=x, y=y, label=label)) +\n    xlim(-0.5,1.5) + ylim(-0.5,1.5) +\n    geom_rect(aes(xmin = -0.5, xmax = 1.5, ymin = -0.5, ymax = 1.5), fill=cbPalette[1], color=\"black\", alpha=0.3) +\n    geom_circle(aes(x0=x, y0=y, r=0.25), fill=cbPalette[2]) +\n    coord_fixed() +\n    dsan_theme(\"quarter\") +\n    geom_text(\n      size=g_textsize,\n      #hjust=1.75,\n      #vjust=-0.75\n    ) +\n    geom_text(\n      data=data.frame(label=\"Œ©\"),\n      aes(x=-0.4,y=1.39),\n      parse=TRUE,\n      size=g_textsize\n    ) +\n    labs(\n      title=expression(\"Continuous Probability Space in \"*R^2)\n    )\n\n\n\n\n\\[\n\\Pr(A) = \\underbrace{\\frac{\\text{Area}(\\{A\\})}{\\text{Area}(\\Omega)}}_{\\mathclap{\\small \\text{Probability }\\textbf{density}}} = \\frac{\\pi r^2}{s^2} = \\frac{\\pi \\left(\\frac{1}{4}\\right)^2}{4} = \\frac{\\pi}{64}\n\\]"
  },
  {
    "objectID": "w04/index.html#the-technical-difference-tldr",
    "href": "w04/index.html#the-technical-difference-tldr",
    "title": "Week 4: Discrete Probability Distributions",
    "section": "The Technical Difference tl;dr",
    "text": "The Technical Difference tl;dr\n\nCountable Sets: Can be put into 1-to-1 correspondence with the natural numbers \\(\\mathbb{N}\\)\n\nWhat are you doing when you‚Äôre counting? Saying ‚Äúfirst‚Äù, ‚Äúsecond‚Äù, ‚Äúthird‚Äù, ‚Ä¶\nYou‚Äôre pairing each object with a natural number! \\(\\{(\\texttt{a},1),(\\texttt{b},2),\\ldots,(\\texttt{z},26)\\}\\) \n\nUncountable Sets: Cannot be put into 1-to-1 correspondence with the natural numbers.\n\n\\(\\mathbb{R}\\) is uncountable. Intuition: Try counting the real numbers. Proof1 \\[\n\\text{Assume }\\exists (f: \\mathbb{R} \\leftrightarrow \\mathbb{N}) =\n\\begin{array}{|c|c|c|c|c|c|c|}\\hline\n\\mathbb{R} & & & & & & \\Leftrightarrow \\mathbb{N} \\\\ \\hline\n\\color{orange}{3} & . & 1 & 4 & 1 & \\cdots & \\Leftrightarrow 1 \\\\\\hline\n4 & . & \\color{orange}{9} & 9 & 9 & \\cdots & \\Leftrightarrow 2 \\\\\\hline\n0 & . & 1 & \\color{orange}{2} & 3 & \\cdots &\\Leftrightarrow 3 \\\\\\hline\n1 & . & 2 & 3 & \\color{orange}{4} & \\cdots & \\Leftrightarrow 4 \\\\\\hline\n\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\\hline\n\\end{array} \\overset{\\color{blue}{y_{[i]}} = \\color{orange}{x_{[i]}} \\overset{\\mathbb{Z}_{10}}{+} 1}{\\longrightarrow} \\color{blue}{y = 4.035 \\ldots} \\Leftrightarrow \\; ?\n\\]\n\n\n\n\nFun math challenge: Is \\(\\mathbb{Q}\\) countable? See this appendix slide for why the answer is yes, despite the fact that \\(\\forall x, y \\in \\mathbb{Q} \\left[ \\frac{x+y}{2} \\in \\mathbb{Q} \\right]\\)"
  },
  {
    "objectID": "w04/index.html#the-practical-difference",
    "href": "w04/index.html#the-practical-difference",
    "title": "Week 4: Discrete Probability Distributions",
    "section": "The Practical Difference",
    "text": "The Practical Difference\n\nThis part of the course (discrete probability): \\(\\Pr(X = v), v \\in \\mathcal{R}_X \\subseteq \\mathbb{N}\\)\n\nExample: \\(\\Pr(\\)\\() = \\Pr(X = 3), 3 \\in \\{1,2,3,4,5,6\\} \\subseteq \\mathbb{N}\\)\n\nNext part of the course (continuous probability): \\(\\Pr(X \\in V), v \\subseteq \\mathbb{R}\\)\n\nExample: \\(\\Pr(X \\geq 2\\pi) = \\Pr(X \\in [\\pi,\\infty)), [\\pi,\\infty) \\subseteq \\mathbb{R}\\)\n\nWhy do they have to be in separate parts?\n\n\\[\n\\Pr(X = 2\\pi) = \\frac{\\text{Area}(\\overbrace{2\\pi}^{\\mathclap{\\small \\text{Single point}}})}{\\text{Area}(\\underbrace{\\mathbb{R}}_{\\mathclap{\\small \\text{(Uncountably) Infinite set of points}}})} = 0\n\\]"
  },
  {
    "objectID": "w04/index.html#probability-mass-vs.-probability-density",
    "href": "w04/index.html#probability-mass-vs.-probability-density",
    "title": "Week 4: Discrete Probability Distributions",
    "section": "Probability Mass vs.¬†Probability Density",
    "text": "Probability Mass vs.¬†Probability Density\n\nCumulative Distribution Function (CDF): \\(F_X(v) = \\Pr(X \\leq v)\\)\nFor discrete RV \\(X\\), Probability Mass Function (pmf) \\(p_X(v)\\): \\[\n\\begin{align*}\np_X(v) &= \\Pr(X = v) = F_X(v) - F_X(v-1) \\\\\n\\implies F_X(v) &= \\sum_{\\{w \\in \\mathcal{R}_X: \\; w \\leq v\\}}p_X(w)\n\\end{align*}\n\\]\nFor continuous RV \\(X\\) (\\(\\mathcal{R}_X \\subseteq \\mathbb{R}\\)), Probability Density Function (pdf) \\(f_X(v)\\): \\[\n\\begin{align*}\nf_X(v) &= \\frac{d}{dx}F_X(v) \\\\\n\\implies F_X(v) &= \\int_{-\\infty}^v f_X(w)dw\n\\end{align*}\n\\]\n\n\n\nFrustratingly, the CDF/pmf/pdf is usually written using \\(X\\) and \\(x\\), like \\(F_X(x) = \\Pr(X \\leq x)\\). To me this is extremely confusing, since the capitalized \\(X\\) is a random variable (not a number) while the lowercase \\(x\\) is some particular value, like \\(3\\). So, to emphasize this difference, I use \\(X\\) for the RV and \\(v\\) for the value at which we‚Äôre checking the CDF/pmf/pdf.\nAlso note the capitalized CDF but lowercase pmf/pdf, matching the mathematical notation where \\(f_X(v)\\) is the derivative of \\(F_X(v)\\)."
  },
  {
    "objectID": "w04/index.html#probability-density-neq-probability",
    "href": "w04/index.html#probability-density-neq-probability",
    "title": "Week 4: Discrete Probability Distributions",
    "section": "Probability Density \\(\\neq\\) Probability",
    "text": "Probability Density \\(\\neq\\) Probability\n\nBEWARE: \\(f_X(v) \\neq \\Pr(X = v)\\)!\nLong story short, for continuous variables, \\(\\Pr(X = v) = 0\\)2\nHence, we instead construct a PDF \\(f_X(v)\\) that enables us to calculate \\(\\Pr(X \\in [a,b])\\) by integrating: \\(f_X(v)\\) is whatever function satisfies \\(\\Pr(X \\in [a,b]) = \\int_{a}^bf_X(v)dv\\).\ni.e., instead of \\(p_X(v) = \\Pr(X = v)\\) from discrete world, the relevant function here is \\(f_X(v)\\), the probability density of \\(X\\) at \\(v\\).\nIf we really want to get something like the ‚Äúprobability of a value‚Äù in a continuous space üò™, we can get something kind of like this by using fancy limits \\[\nf_X(v) = \\lim_{\\varepsilon \\to 0}\\frac{P(X \\in [v-\\varepsilon, v + \\varepsilon])}{2\\varepsilon} = \\lim_{\\varepsilon \\to 0}\\frac{F(v + \\varepsilon) - F(v - \\varepsilon)}{2\\varepsilon} = \\frac{d}{dx}F_X(v)\n\\]"
  },
  {
    "objectID": "w04/index.html#bernoulli-distribution",
    "href": "w04/index.html#bernoulli-distribution",
    "title": "Week 4: Discrete Probability Distributions",
    "section": "Bernoulli Distribution",
    "text": "Bernoulli Distribution\n\nSingle trial with two outcomes, ‚Äúsuccess‚Äù (1) or ‚Äúfailure‚Äù (0): basic model of a coin flip (heads = 1, tails = 0)\n\\(X \\sim \\text{Bern}({\\color{purple} p}) \\implies \\mathcal{R}_X = \\{0,1\\}, \\; \\Pr(X = 1) = {\\color{purple}p}\\).\n\n\nlibrary(ggplot2)\nlibrary(tibble)\nbern_tibble &lt;- tribble(\n  ~Outcome, ~Probability, ~Color,\n  \"Failure\", 0.2, cbPalette[1],\n  \"Success\", 0.8, cbPalette[2]\n)\nggplot(data = bern_tibble, aes(x=Outcome, y=Probability)) +\n  geom_bar(aes(fill=Outcome), stat = \"identity\") +\n  dsan_theme(\"half\") +\n  labs(\n    y = \"Probability Mass\"\n  ) +\n  scale_fill_manual(values=c(cbPalette[1], cbPalette[2])) +\n  remove_legend()"
  },
  {
    "objectID": "w04/index.html#binomial-distribution",
    "href": "w04/index.html#binomial-distribution",
    "title": "Week 4: Discrete Probability Distributions",
    "section": "Binomial Distribution",
    "text": "Binomial Distribution\n\nNumber of successes in \\({\\color{purple}N}\\) Bernoulli trials. \\(X \\sim \\text{Binom}({\\color{purple}N},{\\color{purple}k},{\\color{purple}p}) \\implies \\mathcal{R}_X = \\{0, 1, \\ldots, N\\}\\)\n\n\\(P(X = k) = \\binom{N}{k}p^k(1-p)^{N-k}\\): probability of \\(k\\) successes out of \\(N\\) trials.\n\\(\\binom{N}{k} = \\frac{N!}{k!(N-k)!}\\): ‚ÄúBinomial coefficient‚Äù. How many groups of size \\(k\\) can be formed?3"
  },
  {
    "objectID": "w04/index.html#visualizing-the-binomial",
    "href": "w04/index.html#visualizing-the-binomial",
    "title": "Week 4: Discrete Probability Distributions",
    "section": "Visualizing the Binomial",
    "text": "Visualizing the Binomial\n\n\nCode\nk &lt;- seq(0, 10)\nprob &lt;- dbinom(k, 10, 0.5)\nbar_data &lt;- tibble(k, prob)\nggplot(bar_data, aes(x=k, y=prob)) +\n  geom_bar(stat=\"identity\", fill=cbPalette[1]) +\n  labs(\n    title=\"Binomial Distribution, N = 10, p = 0.5\",\n    y=\"Probability Mass\"\n  ) +\n  scale_x_continuous(breaks=seq(0,10)) +\n  dsan_theme(\"half\")\n\n\n\n\n\n\n\n\n\n\nSo who can tell me, from this plot, the approximate probability of getting 4 heads when flipping a coin 10 times?"
  },
  {
    "objectID": "w04/index.html#multiple-classes-multinomial-distribution",
    "href": "w04/index.html#multiple-classes-multinomial-distribution",
    "title": "Week 4: Discrete Probability Distributions",
    "section": "Multiple Classes: Multinomial Distribution",
    "text": "Multiple Classes: Multinomial Distribution\n\nBernoulli only allows two outcomes: success or failure.\nWhat if we‚Äôre predicting soccer match outcomes?\n\n\\(X_i \\in \\{\\text{Win}, \\text{Loss}, \\text{Draw}\\}\\)\n\nCategorical Distribution: Generalization of Bernoulli to \\(k\\) outcomes. \\(X \\sim \\text{Categorical}(\\mathbf{p} = \\{p_1, p_2, \\ldots, p_k\\}), \\sum_{i=1}^kp_i = 1\\).\n\n\\(P(X = k) = p_k\\)\n\nMultinomial Distribution: Generalization of Binomial to \\(k\\) outcomes.\n\\(\\mathbf{X} \\sim \\text{Multinom}(N,k,\\mathbf{p}=\\{p_1,p_2,\\ldots,p_k\\}), \\sum_{i=1}^kp_i=1\\)\n\n\\(P(\\mathbf{X} = \\{x_1,x_2\\ldots,x_k\\}) = \\frac{N!}{x_1!x_2!\\cdots x_k!}p_1^{x_1}p_2^{x_2}\\cdots p_k^{x_k}\\)\n\\(P(\\text{30 wins}, \\text{4 losses}, \\text{4 draws}) = \\frac{38!}{30!4!4!}p_{\\text{win}}^{30}p_{\\text{lose}}^4p_{\\text{draw}}^4\\)."
  },
  {
    "objectID": "w04/index.html#geometric-distribution",
    "href": "w04/index.html#geometric-distribution",
    "title": "Week 4: Discrete Probability Distributions",
    "section": "Geometric Distribution",
    "text": "Geometric Distribution\n\nGeometric: Likelihood that we need \\({\\color{purple}k}\\) trials to get our first success. \\(X \\sim \\text{Geom}({\\color{purple}k},{\\color{purple}p}) \\implies \\mathcal{R}_X = \\{0, 1, \\ldots\\}\\)\n\n\\(P(X = k) = \\underbrace{(1-p)^{k-1}}_{\\small k - 1\\text{ failures}}\\cdot \\underbrace{p}_{\\mathclap{\\small \\text{success}}}\\)\nProbability of \\(k-1\\) failures followed by a success\n\n\n\nlibrary(ggplot2)\nk &lt;- seq(0, 8)\nprob &lt;- dgeom(k, 0.5)\nbar_data &lt;- tibble(k, prob)\nggplot(bar_data, aes(x = k, y = prob)) +\n    geom_bar(stat = \"identity\", fill = cbPalette[1]) +\n    labs(\n        title = \"Geometric Distribution, p = 0.5\",\n        y = \"Probability Mass\"\n    ) +\n    scale_x_continuous(breaks = seq(0, 8)) +\n    dsan_theme(\"half\")"
  },
  {
    "objectID": "w04/index.html#less-common-but-important-distributions",
    "href": "w04/index.html#less-common-but-important-distributions",
    "title": "Week 4: Discrete Probability Distributions",
    "section": "Less Common (But Important) Distributions",
    "text": "Less Common (But Important) Distributions\n\nDiscrete Uniform: \\(N\\) equally-likely outcomes\n\n\\(X \\sim U\\{{\\color{purple}a},{\\color{purple}b}\\} \\implies \\mathcal{R}_X = \\{a, a+1, \\ldots, b\\}, P(X = k) = \\frac{1}{{\\color{purple}b} - {\\color{purple}a} + 1}\\)\n\nBeta: \\(X \\sim \\text{Beta}({\\color{purple}\\alpha}, {\\color{purple}\\beta})\\): conjugate prior for Bernoulli, Binomial, and Geometric dists.\n\nIntuition: If we use Beta to encode our prior hypothesis, then observe data drawn from Binomial, distribution of our updated hypothesis is still Beta.\n\\(\\underbrace{\\Pr(\\text{biased}) = \\Pr(\\text{unbiased})}_{\\text{Prior: }\\text{Beta}({\\color{purple}\\alpha}, {\\color{purple}\\beta})} \\rightarrow\\) Observe \\(\\underbrace{\\frac{8}{10}\\text{ heads}}_{\\text{Data}} \\rightarrow \\underbrace{\\Pr(\\text{biased}) = 0.65}_{\\text{Posterior: }\\text{Beta}({\\color{purple}\\alpha + 8}, {\\color{purple}\\beta + 2})}\\)\n\nDirichlet: \\(\\mathbf{X} = (X_1, X_2, \\ldots, X_K) \\sim \\text{Dir}({\\color{purple} \\boldsymbol\\alpha})\\)\n\n\\(K\\)-dimensional extension of Beta (thus, conjugate prior for Multinomial)\n\n\n\n\nWe can now use \\(\\text{Beta}(\\alpha + 8, \\beta + 2)\\) as a prior for our next set of trials (encoding our knowledge up to that point), and update further once we know the results (to yet another Beta distribution)."
  },
  {
    "objectID": "w04/index.html#interactive-visualizations",
    "href": "w04/index.html#interactive-visualizations",
    "title": "Week 4: Discrete Probability Distributions",
    "section": "Interactive Visualizations",
    "text": "Interactive Visualizations\n\nSeeing Theory ‚Üí"
  },
  {
    "objectID": "w04/index.html#lab-3-demonstration",
    "href": "w04/index.html#lab-3-demonstration",
    "title": "Week 4: Discrete Probability Distributions",
    "section": "Lab 3 Demonstration",
    "text": "Lab 3 Demonstration\nLab 3 Demo Link"
  },
  {
    "objectID": "w04/index.html#lab-3-assignment-overview",
    "href": "w04/index.html#lab-3-assignment-overview",
    "title": "Week 4: Discrete Probability Distributions",
    "section": "Lab 3 Assignment Overview",
    "text": "Lab 3 Assignment Overview\nLab 3 Assignment"
  },
  {
    "objectID": "w04/index.html#appendix-countability-of-mathbbq",
    "href": "w04/index.html#appendix-countability-of-mathbbq",
    "title": "Week 4: Discrete Probability Distributions",
    "section": "Appendix: Countability of \\(\\mathbb{Q}\\)",
    "text": "Appendix: Countability of \\(\\mathbb{Q}\\)\n\nBad definition: ‚Äú\\(\\mathbb{N}\\) is countable because no \\(x \\in \\mathbb{N}\\) between \\(0\\) and \\(1\\). \\(\\mathbb{R}\\) is uncountable because infinitely-many \\(x \\in \\mathbb{R}\\) between \\(0\\) and \\(1\\).‚Äù (\\(\\implies \\mathbb{Q}\\) uncountable)\nAnd yet, \\(\\mathbb{Q}\\) is countable‚Ä¶\n\n\n\n\n\n\n\n\n\n\\[\n\\begin{align*}\n\\begin{array}{ll}\ns: \\mathbb{N} \\leftrightarrow \\mathbb{Z} & s(n) = (-1)^n \\left\\lfloor \\frac{n+1}{2} \\right\\rfloor \\\\\nh_+: \\mathbb{Z}^+ \\leftrightarrow \\mathbb{Q}^+ & p_1^{a_1}p_2^{a_2}\\cdots \\mapsto p_1^{s(a_1)}p_2^{s(a_2)}\\cdots \\\\\nh: \\mathbb{Z} \\leftrightarrow \\mathbb{Q} & h(n) = \\begin{cases}h_+(n) &n &gt; 0 \\\\ 0 & n = 0 \\\\\n-h_+(-n) & n &lt; 0\\end{cases} \\\\\n(h \\circ s): \\mathbb{N} \\leftrightarrow \\mathbb{Q} & ‚úÖü§Ø\n\\end{array}\n\\end{align*}\n\\]\n\n\n\n\n\n\nImage credit: Rebecca J. Stones, Math StackExchange. Math credit: Thomas Andrews, Math StackExchange"
  },
  {
    "objectID": "w04/index.html#footnotes",
    "href": "w04/index.html#footnotes",
    "title": "Week 4: Discrete Probability Distributions",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe method used in this proof, if you haven‚Äôt seen it before, is called Cantor diagonalization, and it is extremely fun and applicable to a wide variety of levels-of-infinity proofs‚Ü©Ô∏é\nFor intuition: \\(X \\sim U[0,10] \\implies \\Pr(X = \\pi) = \\frac{|\\{v \\in \\mathbb{R}:\\; v = \\pi\\}|}{|\\mathbb{R}|} = \\frac{1}{2^{\\aleph_0}} \\approx 0\\). That is, finding the \\(\\pi\\) needle in the \\(\\mathbb{R}\\) haystack is a one-in-\\(\\left(\\infty^\\infty\\right)\\) event. A similar issue occurs if \\(S\\) is countably-infinite, like \\(S = \\mathbb{N}\\): \\(\\Pr(X = 3) = \\frac{|\\{x \\in \\mathbb{N} : \\; x = 3\\}|}{|\\mathbb{N}|} = \\frac{1}{\\aleph_0}\\).‚Ü©Ô∏é\nA fun way to never have to memorize or compute these: imagine a pyramid like \\(\\genfrac{}{}{0pt}{}{}{\\boxed{\\phantom{1}}}\\genfrac{}{}{0pt}{}{\\boxed{\\phantom{1}}}{}\\genfrac{}{}{0pt}{}{}{\\boxed{\\phantom{1}}}\\), where the boxes are slots for numbers, and put a \\(1\\) in the box at the top. In the bottom row, fill each slot with the sum of the two numbers above-left and above-right of it. Since \\(1 + \\text{(nothing)} = 1\\), this looks like: \\(\\genfrac{}{}{0pt}{}{}{1}\\genfrac{}{}{0pt}{}{1}{}\\genfrac{}{}{0pt}{}{}{1}\\). Continue filling in the pyramid this way, so the next row looks like \\(\\genfrac{}{}{0pt}{}{}{1}\\genfrac{}{}{0pt}{}{1}{}\\genfrac{}{}{0pt}{}{}{2}\\genfrac{}{}{0pt}{}{1}{}\\genfrac{}{}{0pt}{}{}{1}\\), then \\(\\genfrac{}{}{0pt}{}{}{1}\\genfrac{}{}{0pt}{}{1}{}\\genfrac{}{}{0pt}{}{}{3}\\genfrac{}{}{0pt}{}{2}{}\\genfrac{}{}{0pt}{}{}{3}\\genfrac{}{}{0pt}{}{1}{}\\genfrac{}{}{0pt}{}{}{1}\\), and so on. The \\(k\\)th number in the \\(N\\)th row (counting from \\(0\\)) is \\(\\binom{N}{k}\\). For the triangle written out to the 7th row, see Appendix I at end of slideshow.‚Ü©Ô∏é"
  },
  {
    "objectID": "w04/slides.html#discrete-vs.-continuous",
    "href": "w04/slides.html#discrete-vs.-continuous",
    "title": "Week 4: Discrete Probability Distributions",
    "section": "Discrete vs.¬†Continuous",
    "text": "Discrete vs.¬†Continuous\n\n\n\nDiscrete = ‚ÄúEasy mode‚Äù: Based (intuitively) on sets\n\\(\\Pr(A)\\): Four marbles \\(\\{A, B, C, D\\}\\) in box, all equally likely, what is the probability I pull out \\(A\\)?\n\n\n\n\n\n\n\n\n\n\n\\[\n\\Pr(A) = \\underbrace{\\frac{|\\{A\\}|}{|\\Omega|}}_{\\mathclap{\\small \\text{Probability }\\textbf{mass}}} = \\frac{1}{|\\{A,B,C,D\\}|} = \\frac{1}{4}\n\\]\n\n\nContinuous = ‚ÄúHard mode‚Äù: Based (intuitively) on areas\n\\(\\Pr(A)\\): If I throw a dart at this square, what is the probability that I hit region \\(A\\)?\n\n\n\n\n\n\n\n\n\n\n\\[\n\\Pr(A) = \\underbrace{\\frac{\\text{Area}(\\{A\\})}{\\text{Area}(\\Omega)}}_{\\mathclap{\\small \\text{Probability }\\textbf{density}}} = \\frac{\\pi r^2}{s^2} = \\frac{\\pi \\left(\\frac{1}{4}\\right)^2}{4} = \\frac{\\pi}{64}\n\\]"
  },
  {
    "objectID": "w04/slides.html#the-technical-difference-tldr",
    "href": "w04/slides.html#the-technical-difference-tldr",
    "title": "Week 4: Discrete Probability Distributions",
    "section": "The Technical Difference tl;dr",
    "text": "The Technical Difference tl;dr\n\nCountable Sets: Can be put into 1-to-1 correspondence with the natural numbers \\(\\mathbb{N}\\)\n\nWhat are you doing when you‚Äôre counting? Saying ‚Äúfirst‚Äù, ‚Äúsecond‚Äù, ‚Äúthird‚Äù, ‚Ä¶\nYou‚Äôre pairing each object with a natural number! \\(\\{(\\texttt{a},1),(\\texttt{b},2),\\ldots,(\\texttt{z},26)\\}\\) \n\nUncountable Sets: Cannot be put into 1-to-1 correspondence with the natural numbers.\n\n\\(\\mathbb{R}\\) is uncountable. Intuition: Try counting the real numbers. Proof1 \\[\n\\text{Assume }\\exists (f: \\mathbb{R} \\leftrightarrow \\mathbb{N}) =\n\\begin{array}{|c|c|c|c|c|c|c|}\\hline\n\\mathbb{R} & & & & & & \\Leftrightarrow \\mathbb{N} \\\\ \\hline\n\\color{orange}{3} & . & 1 & 4 & 1 & \\cdots & \\Leftrightarrow 1 \\\\\\hline\n4 & . & \\color{orange}{9} & 9 & 9 & \\cdots & \\Leftrightarrow 2 \\\\\\hline\n0 & . & 1 & \\color{orange}{2} & 3 & \\cdots &\\Leftrightarrow 3 \\\\\\hline\n1 & . & 2 & 3 & \\color{orange}{4} & \\cdots & \\Leftrightarrow 4 \\\\\\hline\n\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\\hline\n\\end{array} \\overset{\\color{blue}{y_{[i]}} = \\color{orange}{x_{[i]}} \\overset{\\mathbb{Z}_{10}}{+} 1}{\\longrightarrow} \\color{blue}{y = 4.035 \\ldots} \\Leftrightarrow \\; ?\n\\]\n\n\n\n\nFun math challenge: Is \\(\\mathbb{Q}\\) countable? See this appendix slide for why the answer is yes, despite the fact that \\(\\forall x, y \\in \\mathbb{Q} \\left[ \\frac{x+y}{2} \\in \\mathbb{Q} \\right]\\)\nThe method used in this proof, if you haven‚Äôt seen it before, is called Cantor diagonalization, and it is extremely fun and applicable to a wide variety of levels-of-infinity proofs"
  },
  {
    "objectID": "w04/slides.html#the-practical-difference",
    "href": "w04/slides.html#the-practical-difference",
    "title": "Week 4: Discrete Probability Distributions",
    "section": "The Practical Difference",
    "text": "The Practical Difference\n\nThis part of the course (discrete probability): \\(\\Pr(X = v), v \\in \\mathcal{R}_X \\subseteq \\mathbb{N}\\)\n\nExample: \\(\\Pr(\\)\\() = \\Pr(X = 3), 3 \\in \\{1,2,3,4,5,6\\} \\subseteq \\mathbb{N}\\)\n\nNext part of the course (continuous probability): \\(\\Pr(X \\in V), v \\subseteq \\mathbb{R}\\)\n\nExample: \\(\\Pr(X \\geq 2\\pi) = \\Pr(X \\in [\\pi,\\infty)), [\\pi,\\infty) \\subseteq \\mathbb{R}\\)\n\nWhy do they have to be in separate parts?\n\n\\[\n\\Pr(X = 2\\pi) = \\frac{\\text{Area}(\\overbrace{2\\pi}^{\\mathclap{\\small \\text{Single point}}})}{\\text{Area}(\\underbrace{\\mathbb{R}}_{\\mathclap{\\small \\text{(Uncountably) Infinite set of points}}})} = 0\n\\]"
  },
  {
    "objectID": "w04/slides.html#probability-mass-vs.-probability-density",
    "href": "w04/slides.html#probability-mass-vs.-probability-density",
    "title": "Week 4: Discrete Probability Distributions",
    "section": "Probability Mass vs.¬†Probability Density",
    "text": "Probability Mass vs.¬†Probability Density\n\nCumulative Distribution Function (CDF): \\(F_X(v) = \\Pr(X \\leq v)\\)\nFor discrete RV \\(X\\), Probability Mass Function (pmf) \\(p_X(v)\\): \\[\n\\begin{align*}\np_X(v) &= \\Pr(X = v) = F_X(v) - F_X(v-1) \\\\\n\\implies F_X(v) &= \\sum_{\\{w \\in \\mathcal{R}_X: \\; w \\leq v\\}}p_X(w)\n\\end{align*}\n\\]\nFor continuous RV \\(X\\) (\\(\\mathcal{R}_X \\subseteq \\mathbb{R}\\)), Probability Density Function (pdf) \\(f_X(v)\\): \\[\n\\begin{align*}\nf_X(v) &= \\frac{d}{dx}F_X(v) \\\\\n\\implies F_X(v) &= \\int_{-\\infty}^v f_X(w)dw\n\\end{align*}\n\\]\n\n\n\nFrustratingly, the CDF/pmf/pdf is usually written using \\(X\\) and \\(x\\), like \\(F_X(x) = \\Pr(X \\leq x)\\). To me this is extremely confusing, since the capitalized \\(X\\) is a random variable (not a number) while the lowercase \\(x\\) is some particular value, like \\(3\\). So, to emphasize this difference, I use \\(X\\) for the RV and \\(v\\) for the value at which we‚Äôre checking the CDF/pmf/pdf.\nAlso note the capitalized CDF but lowercase pmf/pdf, matching the mathematical notation where \\(f_X(v)\\) is the derivative of \\(F_X(v)\\)."
  },
  {
    "objectID": "w04/slides.html#probability-density-neq-probability",
    "href": "w04/slides.html#probability-density-neq-probability",
    "title": "Week 4: Discrete Probability Distributions",
    "section": "Probability Density \\(\\neq\\) Probability",
    "text": "Probability Density \\(\\neq\\) Probability\n\nBEWARE: \\(f_X(v) \\neq \\Pr(X = v)\\)!\nLong story short, for continuous variables, \\(\\Pr(X = v) = 0\\)1\nHence, we instead construct a PDF \\(f_X(v)\\) that enables us to calculate \\(\\Pr(X \\in [a,b])\\) by integrating: \\(f_X(v)\\) is whatever function satisfies \\(\\Pr(X \\in [a,b]) = \\int_{a}^bf_X(v)dv\\).\ni.e., instead of \\(p_X(v) = \\Pr(X = v)\\) from discrete world, the relevant function here is \\(f_X(v)\\), the probability density of \\(X\\) at \\(v\\).\nIf we really want to get something like the ‚Äúprobability of a value‚Äù in a continuous space üò™, we can get something kind of like this by using fancy limits \\[\nf_X(v) = \\lim_{\\varepsilon \\to 0}\\frac{P(X \\in [v-\\varepsilon, v + \\varepsilon])}{2\\varepsilon} = \\lim_{\\varepsilon \\to 0}\\frac{F(v + \\varepsilon) - F(v - \\varepsilon)}{2\\varepsilon} = \\frac{d}{dx}F_X(v)\n\\]\n\nFor intuition: \\(X \\sim U[0,10] \\implies \\Pr(X = \\pi) = \\frac{|\\{v \\in \\mathbb{R}:\\; v = \\pi\\}|}{|\\mathbb{R}|} = \\frac{1}{2^{\\aleph_0}} \\approx 0\\). That is, finding the \\(\\pi\\) needle in the \\(\\mathbb{R}\\) haystack is a one-in-\\(\\left(\\infty^\\infty\\right)\\) event. A similar issue occurs if \\(S\\) is countably-infinite, like \\(S = \\mathbb{N}\\): \\(\\Pr(X = 3) = \\frac{|\\{x \\in \\mathbb{N} : \\; x = 3\\}|}{|\\mathbb{N}|} = \\frac{1}{\\aleph_0}\\)."
  },
  {
    "objectID": "w04/slides.html#bernoulli-distribution",
    "href": "w04/slides.html#bernoulli-distribution",
    "title": "Week 4: Discrete Probability Distributions",
    "section": "Bernoulli Distribution",
    "text": "Bernoulli Distribution\n\nSingle trial with two outcomes, ‚Äúsuccess‚Äù (1) or ‚Äúfailure‚Äù (0): basic model of a coin flip (heads = 1, tails = 0)\n\\(X \\sim \\text{Bern}({\\color{purple} p}) \\implies \\mathcal{R}_X = \\{0,1\\}, \\; \\Pr(X = 1) = {\\color{purple}p}\\)."
  },
  {
    "objectID": "w04/slides.html#binomial-distribution",
    "href": "w04/slides.html#binomial-distribution",
    "title": "Week 4: Discrete Probability Distributions",
    "section": "Binomial Distribution",
    "text": "Binomial Distribution\n\nNumber of successes in \\({\\color{purple}N}\\) Bernoulli trials. \\(X \\sim \\text{Binom}({\\color{purple}N},{\\color{purple}k},{\\color{purple}p}) \\implies \\mathcal{R}_X = \\{0, 1, \\ldots, N\\}\\)\n\n\\(P(X = k) = \\binom{N}{k}p^k(1-p)^{N-k}\\): probability of \\(k\\) successes out of \\(N\\) trials.\n\\(\\binom{N}{k} = \\frac{N!}{k!(N-k)!}\\): ‚ÄúBinomial coefficient‚Äù. How many groups of size \\(k\\) can be formed?1\n\n\nA fun way to never have to memorize or compute these: imagine a pyramid like \\(\\genfrac{}{}{0pt}{}{}{\\boxed{\\phantom{1}}}\\genfrac{}{}{0pt}{}{\\boxed{\\phantom{1}}}{}\\genfrac{}{}{0pt}{}{}{\\boxed{\\phantom{1}}}\\), where the boxes are slots for numbers, and put a \\(1\\) in the box at the top. In the bottom row, fill each slot with the sum of the two numbers above-left and above-right of it. Since \\(1 + \\text{(nothing)} = 1\\), this looks like: \\(\\genfrac{}{}{0pt}{}{}{1}\\genfrac{}{}{0pt}{}{1}{}\\genfrac{}{}{0pt}{}{}{1}\\). Continue filling in the pyramid this way, so the next row looks like \\(\\genfrac{}{}{0pt}{}{}{1}\\genfrac{}{}{0pt}{}{1}{}\\genfrac{}{}{0pt}{}{}{2}\\genfrac{}{}{0pt}{}{1}{}\\genfrac{}{}{0pt}{}{}{1}\\), then \\(\\genfrac{}{}{0pt}{}{}{1}\\genfrac{}{}{0pt}{}{1}{}\\genfrac{}{}{0pt}{}{}{3}\\genfrac{}{}{0pt}{}{2}{}\\genfrac{}{}{0pt}{}{}{3}\\genfrac{}{}{0pt}{}{1}{}\\genfrac{}{}{0pt}{}{}{1}\\), and so on. The \\(k\\)th number in the \\(N\\)th row (counting from \\(0\\)) is \\(\\binom{N}{k}\\). For the triangle written out to the 7th row, see Appendix I at end of slideshow."
  },
  {
    "objectID": "w04/slides.html#visualizing-the-binomial",
    "href": "w04/slides.html#visualizing-the-binomial",
    "title": "Week 4: Discrete Probability Distributions",
    "section": "Visualizing the Binomial",
    "text": "Visualizing the Binomial\n\n\nCode\nk &lt;- seq(0, 10)\nprob &lt;- dbinom(k, 10, 0.5)\nbar_data &lt;- tibble(k, prob)\nggplot(bar_data, aes(x=k, y=prob)) +\n  geom_bar(stat=\"identity\", fill=cbPalette[1]) +\n  labs(\n    title=\"Binomial Distribution, N = 10, p = 0.5\",\n    y=\"Probability Mass\"\n  ) +\n  scale_x_continuous(breaks=seq(0,10)) +\n  dsan_theme(\"half\")\n\n\n\n\nSo who can tell me, from this plot, the approximate probability of getting 4 heads when flipping a coin 10 times?"
  },
  {
    "objectID": "w04/slides.html#multiple-classes-multinomial-distribution",
    "href": "w04/slides.html#multiple-classes-multinomial-distribution",
    "title": "Week 4: Discrete Probability Distributions",
    "section": "Multiple Classes: Multinomial Distribution",
    "text": "Multiple Classes: Multinomial Distribution\n\nBernoulli only allows two outcomes: success or failure.\nWhat if we‚Äôre predicting soccer match outcomes?\n\n\\(X_i \\in \\{\\text{Win}, \\text{Loss}, \\text{Draw}\\}\\)\n\nCategorical Distribution: Generalization of Bernoulli to \\(k\\) outcomes. \\(X \\sim \\text{Categorical}(\\mathbf{p} = \\{p_1, p_2, \\ldots, p_k\\}), \\sum_{i=1}^kp_i = 1\\).\n\n\\(P(X = k) = p_k\\)\n\nMultinomial Distribution: Generalization of Binomial to \\(k\\) outcomes.\n\\(\\mathbf{X} \\sim \\text{Multinom}(N,k,\\mathbf{p}=\\{p_1,p_2,\\ldots,p_k\\}), \\sum_{i=1}^kp_i=1\\)\n\n\\(P(\\mathbf{X} = \\{x_1,x_2\\ldots,x_k\\}) = \\frac{N!}{x_1!x_2!\\cdots x_k!}p_1^{x_1}p_2^{x_2}\\cdots p_k^{x_k}\\)\n\\(P(\\text{30 wins}, \\text{4 losses}, \\text{4 draws}) = \\frac{38!}{30!4!4!}p_{\\text{win}}^{30}p_{\\text{lose}}^4p_{\\text{draw}}^4\\)."
  },
  {
    "objectID": "w04/slides.html#geometric-distribution",
    "href": "w04/slides.html#geometric-distribution",
    "title": "Week 4: Discrete Probability Distributions",
    "section": "Geometric Distribution",
    "text": "Geometric Distribution\n\nGeometric: Likelihood that we need \\({\\color{purple}k}\\) trials to get our first success. \\(X \\sim \\text{Geom}({\\color{purple}k},{\\color{purple}p}) \\implies \\mathcal{R}_X = \\{0, 1, \\ldots\\}\\)\n\n\\(P(X = k) = \\underbrace{(1-p)^{k-1}}_{\\small k - 1\\text{ failures}}\\cdot \\underbrace{p}_{\\mathclap{\\small \\text{success}}}\\)\nProbability of \\(k-1\\) failures followed by a success"
  },
  {
    "objectID": "w04/slides.html#less-common-but-important-distributions",
    "href": "w04/slides.html#less-common-but-important-distributions",
    "title": "Week 4: Discrete Probability Distributions",
    "section": "Less Common (But Important) Distributions",
    "text": "Less Common (But Important) Distributions\n\nDiscrete Uniform: \\(N\\) equally-likely outcomes\n\n\\(X \\sim U\\{{\\color{purple}a},{\\color{purple}b}\\} \\implies \\mathcal{R}_X = \\{a, a+1, \\ldots, b\\}, P(X = k) = \\frac{1}{{\\color{purple}b} - {\\color{purple}a} + 1}\\)\n\nBeta: \\(X \\sim \\text{Beta}({\\color{purple}\\alpha}, {\\color{purple}\\beta})\\): conjugate prior for Bernoulli, Binomial, and Geometric dists.\n\nIntuition: If we use Beta to encode our prior hypothesis, then observe data drawn from Binomial, distribution of our updated hypothesis is still Beta.\n\\(\\underbrace{\\Pr(\\text{biased}) = \\Pr(\\text{unbiased})}_{\\text{Prior: }\\text{Beta}({\\color{purple}\\alpha}, {\\color{purple}\\beta})} \\rightarrow\\) Observe \\(\\underbrace{\\frac{8}{10}\\text{ heads}}_{\\text{Data}} \\rightarrow \\underbrace{\\Pr(\\text{biased}) = 0.65}_{\\text{Posterior: }\\text{Beta}({\\color{purple}\\alpha + 8}, {\\color{purple}\\beta + 2})}\\)\n\nDirichlet: \\(\\mathbf{X} = (X_1, X_2, \\ldots, X_K) \\sim \\text{Dir}({\\color{purple} \\boldsymbol\\alpha})\\)\n\n\\(K\\)-dimensional extension of Beta (thus, conjugate prior for Multinomial)\n\n\n\n\nWe can now use \\(\\text{Beta}(\\alpha + 8, \\beta + 2)\\) as a prior for our next set of trials (encoding our knowledge up to that point), and update further once we know the results (to yet another Beta distribution)."
  },
  {
    "objectID": "w04/slides.html#interactive-visualizations",
    "href": "w04/slides.html#interactive-visualizations",
    "title": "Week 4: Discrete Probability Distributions",
    "section": "Interactive Visualizations",
    "text": "Interactive Visualizations\n\nSeeing Theory ‚Üí"
  },
  {
    "objectID": "w04/slides.html#lab-3-demonstration",
    "href": "w04/slides.html#lab-3-demonstration",
    "title": "Week 4: Discrete Probability Distributions",
    "section": "Lab 3 Demonstration",
    "text": "Lab 3 Demonstration\nLab 3 Demo Link"
  },
  {
    "objectID": "w04/slides.html#lab-3-assignment-overview",
    "href": "w04/slides.html#lab-3-assignment-overview",
    "title": "Week 4: Discrete Probability Distributions",
    "section": "Lab 3 Assignment Overview",
    "text": "Lab 3 Assignment Overview\nLab 3 Assignment"
  },
  {
    "objectID": "w04/slides.html#appendix-countability-of-mathbbq",
    "href": "w04/slides.html#appendix-countability-of-mathbbq",
    "title": "Week 4: Discrete Probability Distributions",
    "section": "Appendix: Countability of \\(\\mathbb{Q}\\)",
    "text": "Appendix: Countability of \\(\\mathbb{Q}\\)\n\nBad definition: ‚Äú\\(\\mathbb{N}\\) is countable because no \\(x \\in \\mathbb{N}\\) between \\(0\\) and \\(1\\). \\(\\mathbb{R}\\) is uncountable because infinitely-many \\(x \\in \\mathbb{R}\\) between \\(0\\) and \\(1\\).‚Äù (\\(\\implies \\mathbb{Q}\\) uncountable)\nAnd yet, \\(\\mathbb{Q}\\) is countable‚Ä¶\n\n\n\n\n\n\n\n\n\n\\[\n\\begin{align*}\n\\begin{array}{ll}\ns: \\mathbb{N} \\leftrightarrow \\mathbb{Z} & s(n) = (-1)^n \\left\\lfloor \\frac{n+1}{2} \\right\\rfloor \\\\\nh_+: \\mathbb{Z}^+ \\leftrightarrow \\mathbb{Q}^+ & p_1^{a_1}p_2^{a_2}\\cdots \\mapsto p_1^{s(a_1)}p_2^{s(a_2)}\\cdots \\\\\nh: \\mathbb{Z} \\leftrightarrow \\mathbb{Q} & h(n) = \\begin{cases}h_+(n) &n &gt; 0 \\\\ 0 & n = 0 \\\\\n-h_+(-n) & n &lt; 0\\end{cases} \\\\\n(h \\circ s): \\mathbb{N} \\leftrightarrow \\mathbb{Q} & ‚úÖü§Ø\n\\end{array}\n\\end{align*}\n\\]\n\n\n\n\n\n\n\n\nImage credit: Rebecca J. Stones, Math StackExchange. Math credit: Thomas Andrews, Math StackExchange"
  },
  {
    "objectID": "recordings/index.html",
    "href": "recordings/index.html",
    "title": "Lecture Recordings",
    "section": "",
    "text": "Order By\n       Default\n         \n          Week\n        \n         \n          Title\n        \n         \n          Last Updated - Oldest\n        \n         \n          Last Updated - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nWeek\n\n\nTitle\n\n\nLast Updated\n\n\n\n\n\n\n4\n\n\nWeek 04 Lecture Recording\n\n\nWednesday Sep 13, 2023\n\n\n\n\n3\n\n\nWeek 03 Lecture Recording\n\n\nTuesday Sep 5, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "writeups/computing-probabilities/index.html",
    "href": "writeups/computing-probabilities/index.html",
    "title": "Two Ways to Compute Probabilities in R/Python",
    "section": "",
    "text": "In several of the assignments for DSAN5100, we ask you to ‚Äúcompute‚Äù a probability. This can be ambiguous, as we‚Äôve already learned several different ways to compute probabilities, so here I will try to distinguish between two different approaches to computing a probability using a computer. Hopefully it will help you in general when asking us questions, and will help us clarify which of these approaches we are looking for when we ask you to compute a probability in future assignments."
  },
  {
    "objectID": "writeups/computing-probabilities/index.html#approach-1-using-r-like-a-fancy-spreadsheet-program",
    "href": "writeups/computing-probabilities/index.html#approach-1-using-r-like-a-fancy-spreadsheet-program",
    "title": "Two Ways to Compute Probabilities in R/Python",
    "section": "Approach 1: Using R Like a Fancy Spreadsheet Program",
    "text": "Approach 1: Using R Like a Fancy Spreadsheet Program\nThis approach implicitly assumes the na√Øve definition of probability, from early on in the course:\n\n\n\n\n\n\nNa√Øve Definition of Probability\n\n\n\nGiven a sample space \\(\\Omega\\) and an event \\(E \\subseteq \\Omega\\),\n\\[\n\\Pr(\\underbrace{E}_{\\mathclap{\\text{event}}}) = \\frac{\\text{\\# Outcomes in E}}{\\text{\\# Possible Outcomes}} = \\frac{|E|}{|\\Omega|}\n\\]\n\n\nIt follows the following logic:\n\nIf we want to use our computer to compute a probability, and if the probability space we‚Äôre working in is finite, then\n\nWe can represent the entire sample space \\(\\Omega\\) as a big tibble, and\n\n\nWe can represent the event of interest \\(E\\) as a subset of this tibble1.\n\n\nAs a super simple example that hopefully illustrates this approach, consider the case of rolling a single die, so that the sample space is\n\\[\n\\Omega = \\{1, 2, 3, 4, 5, 6\\}\n\\]\nand then consider the event of interest \\(E\\) to be ‚Äúthe outcome of the roll is even‚Äù, so that\n\\[\nE = \\{2, 4, 6\\} \\subseteq \\Omega\n\\]\nIn this approach we could construct a tibble representing the entire sample space \\(\\Omega\\) like\n\nsource(\"../../_globals.r\")\nlibrary(tibble)\nsample_space_df &lt;- tibble(outcome=seq(1,6))\nsample_space_df\n\n\n\n\n\noutcome\n\n\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n\n\n\n\nThen we could extract a subset of this tibble corresponding to the event \\(E\\) by selecting only those rows whose outcome value is even2:\n\nlibrary(dplyr)\nevent_df &lt;- sample_space_df %&gt;% filter(outcome %% 2 == 0)\nevent_df\n\n\n\n\n\noutcome\n\n\n\n\n2\n\n\n4\n\n\n6\n\n\n\n\n\n\nThese two tibbles together allow us to compute a probability using the na√Øve definition:\n\nnrow(event_df) / nrow(sample_space_df)\n\n[1] 0.5\n\n\nIt‚Äôs a bit silly to use this approach to study dice, since we can figure out the probabilistic properties of dice pretty intuitively (see below), but this approach becomes more useful when we think about problems focusing on data analysis.\nFor example, we might give you a problem that says\nLoad this dataset of country/region GDP data, and keep just the observations for the year 2010. What is the probability that an entity in this dataset had a GDP lower than $10 billion, and a country code containing the letter Z, in this year?\n\nlibrary(readr)\nlibrary(dplyr)\nlibrary(stringr)\ngdp_df &lt;- read_csv(\"https://gist.githubusercontent.com/jpowerj/fecd437b96d0954893de727383f2eaf2/raw/fec58507f7095cb8341b229d6eb74ce53232d663/gdp_2010.csv\")\nbelow_10b_df &lt;- gdp_df %&gt;% filter(value &lt; 10000000000)\nbelow_10b_z_df &lt;- below_10b_df %&gt;% filter(str_detect(code, 'Z'))\nbelow_10b_z_df\n\n\n\n\n\nname\ncode\nyear\nvalue\n\n\n\n\nBelize\nBLZ\n2010\n1397113450\n\n\nKyrgyz Republic\nKGZ\n2010\n4794357795\n\n\nSwaziland\nSWZ\n2010\n4438778424\n\n\n\n\n\n\nSo that now we could compute the probability of a country having these properties:\n\np_event &lt;- nrow(below_10b_z_df) / nrow(gdp_df)\np_event\n\n[1] 0.01470588\n\n\nSo, this approach works well when we‚Äôre dealing with nice, simple, finite sample-spacing and outcomes which are easily definable using and or or (for example), but we won‚Äôt be able to use it to handle fancier cases like continuous probability distributions."
  },
  {
    "objectID": "writeups/computing-probabilities/index.html#approach-2-using-r-like-a-fancy-calculator",
    "href": "writeups/computing-probabilities/index.html#approach-2-using-r-like-a-fancy-calculator",
    "title": "Two Ways to Compute Probabilities in R/Python",
    "section": "Approach 2: Using R Like a Fancy Calculator",
    "text": "Approach 2: Using R Like a Fancy Calculator\nIn this approach, we basically just use R (or Python, or whatever else) as a glorified calculator. We take the hard part‚Äîreasoning about what the sample space might look like, for example, or whether events are independent‚Äîand do it in our heads, then use R to figure out what happens when we multiply/add/divide the things we already figured out.\nContinuing the dice example, for instance, this approach would go something like: we ask you\n\nWhat is the probability that you observe a roll greater than 4, followed by a roll less than or equal to 4, followed by a roll greater than 4?\n\nand you reason through this like,\n\nI already know that there are 6 outcomes, and that they‚Äôre all equally likely, so that the probability of seeing a particular outcome in \\(\\Omega = \\{1, 2, 3, 4, 5, 6\\}\\) is always \\(\\frac{1}{6}\\). Therefore I can define an event \\(E_&gt;\\) representing the event of observing an outcome greater than 4, and another event \\(E_\\leq\\) of observing an outcome less than or equal to 4. I know that\n\n\n\nSince the possible outcomes greater than 4 are 5 and 6, my event \\(E_&gt; = \\{5, 6\\}\\), and since \\(\\Omega = \\{1, 2, 3, 4, 5, 6\\}\\), I can compute the probability of \\(E_&gt;\\) as \\[\n\\Pr(E_&gt;) = \\frac{|E_&gt;|}{|\\Omega|} = \\frac{|\\{5, 6\\}|}{6} = \\frac{1}{3}\n\\]\n\n\n\n\nSince the possible outcomes less than or equal to 4 are 1, 2, 3, and 4, my event \\(E_\\leq = \\{1, 2, 3, 4\\}\\), so I can compute the probability of \\(E_\\leq\\) as \\[\n\\Pr(E_\\leq) = \\frac{|E_\\leq|}{|\\Omega|} = \\frac{|\\{1, 2, 3, 4|}{6} = \\frac{2}{3}\n\\]\n\n\n\nAnd now I can use R with these probabilities I computed in my head to derive the probabilities of them occurring in a particular sequence:\n\n\n# Encode probabilities of my two events\np_greater &lt;- 1/3\np_less_or_equal &lt;- 2/3\n# Use them to compute the probabilities of sequences\np_sequence &lt;- p_greater * p_less_or_equal * p_greater\np_sequence\n\n[1] 0.07407407\n\n\nThis approach may seem boring at first, since again we could have done this just on a calculator, but it becomes interesting when we ask you to vary the numbers to see what happens to the resulting probabilities.\nFor example, if you wrote the above code for some problem, then in the next problem we could say ‚ÄúIt turns out the person providing you with the dice was cheating! They rigged it so that there‚Äôs actually a 99% chance of getting a number greater than 4! Go back and compute the probability of this sequence but using the rigged die‚Äù\nAnd then you could just change the first line of your code, and re-run the whole thing, to obtain the new probability:\n\np_greater &lt;- 0.99\np_less_or_equal &lt;- 0.01\np_sequence &lt;- p_greater * p_less_or_equal * p_greater\np_sequence\n\n[1] 0.009801\n\n\nFinally, we might then ask you something like:\n‚ÄúYour friend actually invented a machine that can generated any rigged die at all, so that the probability of rolling a number greater than 4 is whatever they want it to be.\nYou are worried because you always play a game with this friend where you win if you roll something greater than 4, then less than or equal to 4, then greater than 4. So you want to see how the friend‚Äôs rigging machine could affect your likelihood of winning.\nWrite a function that computes your likelihood of winning (i.e., your likelihood of getting this sequence of three rolls), for any die with any probability of producing an outcome greater than 4.\nIn this case, you could now take your code and transform it into a function, like\n\ncompute_win_prob &lt;- function(p_greater) {\n    p_less_or_equal &lt;- 1 - p_greater\n    p_sequence &lt;- p_greater * p_less_or_equal * p_greater\n    return(p_sequence)\n}\n\nAnd then test it on some values:\n\ncompute_win_prob(0.01)\n\n[1] 9.9e-05\n\ncompute_win_prob(0.5)\n\n[1] 0.125\n\ncompute_win_prob(0.99)\n\n[1] 0.009801\n\n\nThen we could ask you to think about\nWhat value for p_greater do you think would maximize your probability of winning?\nand then\nProduce a plot showing your likelihood of winning for 1000 different values of p_greater, evenly spread between 0 and 1\nand you could use the function you wrote like\n\nlibrary(ggplot2)\np_greater_vals &lt;- seq(from = 0, to = 1, length.out = 1000)\n#p_greater_vals\nwin_prob_vals &lt;- sapply(p_greater_vals, compute_win_prob)\nwin_df &lt;- tibble(p_greater=p_greater_vals, win_prob=win_prob_vals)\nggplot(win_df, aes(x=p_greater, y=win_prob)) +\n  geom_line() +\n  dsan_theme(\"full\") +\n  labs(\n    title = \"Probability of Winning With Rigged Die\",\n    x = \"Pr(Result &gt; 4)\",\n    y = \"Pr(Win)\"\n  )\n\n\n\n\n\n\n\n\nAnd finally, we could ask you to draw a conclusion about this situation:\nLooking at the plot, approximately what value for the rigged die looks like it produces the maximum likelihood of winning? Now write code to compute the actual value, out of the 1000 evenly-spaced values you computed the win probability for, that maximizes your probability of winning. Does it match your guess from earlier?\nFor which you could write code like\n\nmax_prob &lt;- max(win_df$win_prob)\nmax_row &lt;- win_df %&gt;% filter(win_prob == max_prob)\nmax_row\n\n\n\n\n\np_greater\nwin_prob\n\n\n\n\n0.6666667\n0.1481481\n\n\n\n\n\n\nMy point in writing all this out is just to say: that was a whole other way to compute probabilities using a computer, where the focus wasn‚Äôt on making a big tibble and then extracting smaller chunks out of it and dividing their sizes. We did have to construct a tibble, in order to compute the win probabilities for a spectrum of values, but that was just so we could generate a plot (since ggplot() needed to know specifically what points to put at what coordinates).\nSo, in this approach, we don‚Äôt have a single tibble throughout the entire problem that serves as our sample space. We just use tibbles when we need them, as tools for doing math or making plots, while the probabilities themselves are still getting computed using simple multiplication.\nAnd, I mentioned above how the tibble-subsetting method doesn‚Äôt allow us to deal with continuous probability distributions. We‚Äôve now seen, on the other hand, how this ‚Äúfancy calculator‚Äù approach is able to deal with such distributions: we were really looking at the distribution of all possible rigged dice, meaning, the space of all dice with \\(\\Pr(\\text{roll greater than 4}) = p\\) for \\(p \\in [0,1]\\). We had to approximate this continuous space using a grid of 1000 samples, which brought us closer to the fancy-spreadsheet approach, but hopefully this can help make it a bit clearer how the fancy-calculator approach can help us when the fancy-spreadsheet approach can‚Äôt, even if it (the fancy-calculator approach) requires us to do a lot more math in our heads."
  },
  {
    "objectID": "writeups/computing-probabilities/index.html#footnotes",
    "href": "writeups/computing-probabilities/index.html#footnotes",
    "title": "Two Ways to Compute Probabilities in R/Python",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf you‚Äôre a Python absolutist, you can take tibble here and replace it with pd.DataFrame, for example. If you‚Äôre a base-R absolutist you can replace it with data.frame.‚Ü©Ô∏é\nThe %% operator in R performs modular division: a %% b produces the remainder after dividing a by b (for example, 7 %% 3 produces 1).‚Ü©Ô∏é"
  },
  {
    "objectID": "writeups/sampling-from-df/index.html",
    "href": "writeups/sampling-from-df/index.html",
    "title": "Two Ways of Sampling from a Data Frame",
    "section": "",
    "text": "A few students have run into issues when trying to use the sample() function, from base-R, to sample from a full data.frame or tibble. In this writeup I‚Äôll argue that this is a case where using a function from the tidyverse called slice_sample() will make your life much easier, but I will also show how to do this sampling using only base-R functions.\nBefore we start, we make sure to use set.seed(5000) at the beginning, so that your grader gets the same results as you do even when working with random processes!\nset.seed(5000)"
  },
  {
    "objectID": "writeups/sampling-from-df/index.html#creating-a-deck-of-cards-using-expand.grid",
    "href": "writeups/sampling-from-df/index.html#creating-a-deck-of-cards-using-expand.grid",
    "title": "Two Ways of Sampling from a Data Frame",
    "section": "Creating a Deck of Cards Using expand.grid()",
    "text": "Creating a Deck of Cards Using expand.grid()\nThis is done as was introduced in the Bootcamp:\n\n\nCode\nranks &lt;- c(\"Ace\", \"Two\", \"Three\", \"Four\", \"Five\", \"Six\", \"Seven\", \"Eight\",\n           \"Nine\", \"Ten\", \"Jack\", \"Queen\", \"King\")\nsuits &lt;- c(\"Hearts\", \"Diamonds\", \"Clubs\", \"Spades\")\ndeck_df &lt;- expand.grid(ranks, suits)\ncolnames(deck_df) &lt;- c(\"Rank\", \"Suit\")\nhead(deck_df)\n\n\n\n\n\n\nRank\nSuit\n\n\n\n\nAce\nHearts\n\n\nTwo\nHearts\n\n\nThree\nHearts\n\n\nFour\nHearts\n\n\nFive\nHearts\n\n\nSix\nHearts\n\n\n\n\n\n\nAnd we can check the dimensions of deck_df just to make sure it created the right number of cards:\n\ndim(deck_df)\n\n[1] 52  2\n\n\nNow, since sampling without replacement is the default case for both sample functions, to illustrate how to use parameters to these functions I will be sampling with replacement."
  },
  {
    "objectID": "writeups/sampling-from-df/index.html#using-tidyverse-to-sample-with-replacement",
    "href": "writeups/sampling-from-df/index.html#using-tidyverse-to-sample-with-replacement",
    "title": "Two Ways of Sampling from a Data Frame",
    "section": "Using tidyverse to Sample WITH Replacement",
    "text": "Using tidyverse to Sample WITH Replacement\nThe following code uses the pipe operator %&gt;% to take the data.frame, deck_df, and ‚Äúpipe it into‚Äù the slice_sample() function from the tidyverse. We have to provide two arguments:\n\nn: The number of samples we‚Äôd like to take, and\nreplace: If set to TRUE, the sampling is performed with replacement. Otherwise (the default), the sampling is performed without replacement.\n\n\n\nCode\nlibrary(tidyverse)\n# Using the \"fancier\" pipe operator (%&gt;%)\ndeck_sample_df &lt;- deck_df %&gt;% slice_sample(n = 12, replace = TRUE)\ndeck_sample_df\n\n\n\n\n\n\nRank\nSuit\n\n\n\n\nAce\nSpades\n\n\nFive\nHearts\n\n\nFour\nSpades\n\n\nJack\nSpades\n\n\nNine\nClubs\n\n\nSeven\nClubs\n\n\nTen\nClubs\n\n\nQueen\nClubs\n\n\nKing\nSpades\n\n\nTwo\nSpades\n\n\nThree\nSpades\n\n\nJack\nSpades\n\n\n\n\n\n\nHere we can confirm that it sampled with replacement since we see that it selected the Jack of Spades twice (once in slot 4 and once in slot 12).\nNote that, although using the pipe operator %&gt;% is the ‚Äústandard‚Äù way to use tidyverse functions, you can still use the functions without using the pipe operator (long story short, the pipe operator just takes whatever comes before the %&gt;% and ‚Äúplugs it in‚Äù as the first argument to the function that comes after the %&gt;%), by specifying the first argument to the slice_sample() function explicitly:\n\ndeck_sample_df &lt;- slice_sample(deck_df, n = 12, replace = TRUE)\ndeck_sample_df\n\n\n\n\n\nRank\nSuit\n\n\n\n\nFour\nHearts\n\n\nFive\nClubs\n\n\nFour\nClubs\n\n\nFive\nDiamonds\n\n\nKing\nSpades\n\n\nKing\nDiamonds\n\n\nFive\nSpades\n\n\nTwo\nClubs\n\n\nQueen\nHearts\n\n\nEight\nClubs\n\n\nJack\nSpades\n\n\nEight\nClubs\n\n\n\n\n\n\nTo me, one nice aspect of slice_sample() over other base-R functions is (among other things) it ensures that the column names are maintained when you sample, which is not always true for the base-R functions. It‚Äôs also possible to do in base-R (without using tidyverse libraries/functions), though, just less straightforwardly."
  },
  {
    "objectID": "writeups/sampling-from-df/index.html#using-base-r-to-sample-with-replacement",
    "href": "writeups/sampling-from-df/index.html#using-base-r-to-sample-with-replacement",
    "title": "Two Ways of Sampling from a Data Frame",
    "section": "Using Base-R to Sample WITH Replacement",
    "text": "Using Base-R to Sample WITH Replacement\nFirst off, note that just applying sample() to the deck will not produce the outcome we expect, or want, which probably unfortunately goes against our intuitions for how this function should work:\n\nsample(deck_df, 5, replace = TRUE)\n\n\n\n\n\nSuit\nRank\nSuit.1\nRank.1\nRank.2\n\n\n\n\nHearts\nAce\nHearts\nAce\nAce\n\n\nHearts\nTwo\nHearts\nTwo\nTwo\n\n\nHearts\nThree\nHearts\nThree\nThree\n\n\nHearts\nFour\nHearts\nFour\nFour\n\n\nHearts\nFive\nHearts\nFive\nFive\n\n\nHearts\nSix\nHearts\nSix\nSix\n\n\nHearts\nSeven\nHearts\nSeven\nSeven\n\n\nHearts\nEight\nHearts\nEight\nEight\n\n\nHearts\nNine\nHearts\nNine\nNine\n\n\nHearts\nTen\nHearts\nTen\nTen\n\n\nHearts\nJack\nHearts\nJack\nJack\n\n\nHearts\nQueen\nHearts\nQueen\nQueen\n\n\nHearts\nKing\nHearts\nKing\nKing\n\n\nDiamonds\nAce\nDiamonds\nAce\nAce\n\n\nDiamonds\nTwo\nDiamonds\nTwo\nTwo\n\n\nDiamonds\nThree\nDiamonds\nThree\nThree\n\n\nDiamonds\nFour\nDiamonds\nFour\nFour\n\n\nDiamonds\nFive\nDiamonds\nFive\nFive\n\n\nDiamonds\nSix\nDiamonds\nSix\nSix\n\n\nDiamonds\nSeven\nDiamonds\nSeven\nSeven\n\n\nDiamonds\nEight\nDiamonds\nEight\nEight\n\n\nDiamonds\nNine\nDiamonds\nNine\nNine\n\n\nDiamonds\nTen\nDiamonds\nTen\nTen\n\n\nDiamonds\nJack\nDiamonds\nJack\nJack\n\n\nDiamonds\nQueen\nDiamonds\nQueen\nQueen\n\n\nDiamonds\nKing\nDiamonds\nKing\nKing\n\n\nClubs\nAce\nClubs\nAce\nAce\n\n\nClubs\nTwo\nClubs\nTwo\nTwo\n\n\nClubs\nThree\nClubs\nThree\nThree\n\n\nClubs\nFour\nClubs\nFour\nFour\n\n\nClubs\nFive\nClubs\nFive\nFive\n\n\nClubs\nSix\nClubs\nSix\nSix\n\n\nClubs\nSeven\nClubs\nSeven\nSeven\n\n\nClubs\nEight\nClubs\nEight\nEight\n\n\nClubs\nNine\nClubs\nNine\nNine\n\n\nClubs\nTen\nClubs\nTen\nTen\n\n\nClubs\nJack\nClubs\nJack\nJack\n\n\nClubs\nQueen\nClubs\nQueen\nQueen\n\n\nClubs\nKing\nClubs\nKing\nKing\n\n\nSpades\nAce\nSpades\nAce\nAce\n\n\nSpades\nTwo\nSpades\nTwo\nTwo\n\n\nSpades\nThree\nSpades\nThree\nThree\n\n\nSpades\nFour\nSpades\nFour\nFour\n\n\nSpades\nFive\nSpades\nFive\nFive\n\n\nSpades\nSix\nSpades\nSix\nSix\n\n\nSpades\nSeven\nSpades\nSeven\nSeven\n\n\nSpades\nEight\nSpades\nEight\nEight\n\n\nSpades\nNine\nSpades\nNine\nNine\n\n\nSpades\nTen\nSpades\nTen\nTen\n\n\nSpades\nJack\nSpades\nJack\nJack\n\n\nSpades\nQueen\nSpades\nQueen\nQueen\n\n\nSpades\nKing\nSpades\nKing\nKing\n\n\n\n\n\n\nA way to avoid this is to make sure that you are using the sample() function NOT on the entire data.frame object, but just to select a subset of the rows of the data.frame, like the following:\n\ndeck_df[sample(nrow(deck_df), 15, replace = TRUE),]\n\n\n\n\n\n\nRank\nSuit\n\n\n\n\n24\nJack\nDiamonds\n\n\n19\nSix\nDiamonds\n\n\n25\nQueen\nDiamonds\n\n\n19.1\nSix\nDiamonds\n\n\n3\nThree\nHearts\n\n\n10\nTen\nHearts\n\n\n31\nFive\nClubs\n\n\n11\nJack\nHearts\n\n\n47\nEight\nSpades\n\n\n39\nKing\nClubs\n\n\n39.1\nKing\nClubs\n\n\n43\nFour\nSpades\n\n\n22\nNine\nDiamonds\n\n\n9\nNine\nHearts\n\n\n27\nAce\nClubs\n\n\n\n\n\n\nFirst off, notice how here we can again confirm that it sampled with replacement since it had to create additional ids like 34.1 and 34.2 to represent the fact that card #34 (the Eight of Clubs) ended up in our sample 3 times.\nAlso note how, rather than sampling from the data.frame, which may be intuitively/linguistically how we would describe what we want, we are actually sampling from the set of indices of the data.frame, then asking R to give us the rows corresponding to those sampled indices. Concretely, to see what‚Äôs going on, let‚Äôs just look at the row filter we‚Äôve provided (the portion of the full code that is within the square brackets [], before the comma):\n\nsample(nrow(deck_df), 15, replace = TRUE)\n\n [1] 30 32 30 46  7 30 46 27 21 42 25 26 17 35  4\n\n\nWe see that, in fact, we are not really sampling from the data.frame itself, so much as sampling from a list of its indices (from 1 to 52), and then after performing this sample we are going and asking R to give us the rows at the indices that ended up in this sample. Keeping this distinction in mind (between the rows themselves and their indices) can be helpful for debugging code like this."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DSAN 5100, Section 03 (Thursdays)",
    "section": "",
    "text": "This is a ‚Äúhub‚Äù collecting relevant links for each week, for students in Prof.¬†Jeff‚Äôs Thursday section (Section 03) of DSAN 5100: Probabilistic Modeling and Statistical Computing, Fall 2023 at Georgetown University. Sections take place in Car Barn room 201 on Thursdays from 12:30pm to 3:30pm.\nThis page is not a replacement for the Main Course Page or the course‚Äôs Canvas Page, which are shared across all sections!\nUse the menu on the left, or the table below, to view the resources for a specific week.\n\n\n\n\n\n\n\n\n\n\nTitle\n\n\nDate\n\n\n\n\n\n\nWeek 1: Welcome to DSAN 5100!\n\n\nThursday, August 24, 2023\n\n\n\n\nWeek 2: Introduction to Probabilistic Modeling\n\n\nWednesday, September 6, 2023\n\n\n\n\nWeek 3: Conditional Probability\n\n\nThursday, September 7, 2023\n\n\n\n\nWeek 4: Discrete Probability Distributions\n\n\nThursday, September 14, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "w01/index.html",
    "href": "w01/index.html",
    "title": "Week 1: Welcome to DSAN 5100!",
    "section": "",
    "text": "(Week 1 of DSAN 5100 was a joint session across all individual sections, taught on Zoom.)\n\n\n\n\n\n\nToday‚Äôs Links\n\n\n\n\nWeek 1 Lecture Notes\nWeek 1 Lecture Recording"
  }
]