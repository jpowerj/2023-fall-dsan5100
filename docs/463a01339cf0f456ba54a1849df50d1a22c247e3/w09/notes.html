<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>notes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../favicon.png" rel="icon" type="image/png">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-RFMTL6Q5J5"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-RFMTL6Q5J5', { 'anonymize_ip': true});
</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item">
      DSAN 5100 Section 03 (Thursdays, 12:30-3pm)&lt;br&gt;
&lt;span class="subtitle"&gt;Car Barn 201&lt;/span&gt;&lt;br&gt;
&lt;span class="subtitle"&gt;Fall 2023, Georgetown&lt;/span&gt;&lt;br&gt;
&lt;span class="subtitle"&gt;
  Prof. Jeff&amp;nbsp;
  &lt;a href='mailto:jj1088@georgetown.edu' target='_blank'&gt;
    &lt;i class='bi bi-envelope-at ps-1 pe-1'&gt;&lt;/i&gt;
  &lt;/a&gt;
&lt;/span&gt;&lt;br&gt;
&lt;span class='subtitle'&gt;
  &lt;a href='https://georgetown.zoom.us/j/97102554837' target='_blank' class='icon-link' style='text-decoration: none !important;'&gt;
  &lt;span class='icon pe-1'&gt;
    &lt;svg class="bi" xmlns="http://www.w3.org/2000/svg" x="0px" y="0px" width="20" height="20" viewBox="0 0 50 50"&gt;
      &lt;path d="M33.619,4H16.381C9.554,4,4,9.554,4,16.381v17.238C4,40.446,9.554,46,16.381,46h17.238C40.446,46,46,40.446,46,33.619	V16.381C46,9.554,40.446,4,33.619,4z M30,30.386C30,31.278,29.278,32,28.386,32H15.005C12.793,32,11,30.207,11,27.995v-9.382	C11,17.722,11.722,17,12.614,17h13.382C28.207,17,30,18.793,30,21.005V30.386z M39,30.196c0,0.785-0.864,1.264-1.53,0.848l-5-3.125	C32.178,27.736,32,27.416,32,27.071v-5.141c0-0.345,0.178-0.665,0.47-0.848l5-3.125C38.136,17.54,39,18.019,39,18.804V30.196z"&gt;&lt;/path&gt;
    &lt;/svg&gt;&lt;/span&gt;Recurring Zoom Link &lt;i class='bi bi-box-arrow-up-right ps-1'&gt;&lt;/i&gt;
  &lt;/a&gt;
&lt;/span&gt;
      </li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">DSAN 5100 Section 03 (Thursdays, 12:30-3pm)<br> <span class="subtitle">Car Barn 201</span><br> <span class="subtitle">Fall 2023, Georgetown</span><br> <span class="subtitle"> Prof.&nbsp;Jeff&nbsp; <a href="mailto:jj1088@georgetown.edu" target="_blank"> <i class="bi bi-envelope-at ps-1 pe-1"></i> </a> </span><br> <span class="subtitle"> <a href="https://georgetown.zoom.us/j/97102554837" target="_blank" class="icon-link" style="text-decoration: none !important;"> <span class="icon pe-1"> <svg class="bi" xmlns="http://www.w3.org/2000/svg" x="0px" y="0px" width="20" height="20" viewbox="0 0 50 50"> <path d="M33.619,4H16.381C9.554,4,4,9.554,4,16.381v17.238C4,40.446,9.554,46,16.381,46h17.238C40.446,46,46,40.446,46,33.619    V16.381C46,9.554,40.446,4,33.619,4z M30,30.386C30,31.278,29.278,32,28.386,32H15.005C12.793,32,11,30.207,11,27.995v-9.382    C11,17.722,11.722,17,12.614,17h13.382C28.207,17,30,18.793,30,21.005V30.386z M39,30.196c0,0.785-0.864,1.264-1.53,0.848l-5-3.125  C32.178,27.736,32,27.416,32,27.071v-5.141c0-0.345,0.178-0.665,0.47-0.848l5-3.125C38.136,17.54,39,18.019,39,18.804V30.196z"></path> </svg></span>Recurring Zoom Link <i class="bi bi-box-arrow-up-right ps-1"></i> </a> </span></a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="https://jfh.georgetown.domains/dsan5100/" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><i class="bi bi-arrow-left" style="margin-right: 4px"></i>Main Course Page</span></a>
  </div>
</li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><i class="bi bi-house pe-1"></i> Section 03 Home</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../w01/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 1: Aug 24</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../w02/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 2: Aug 31</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../w03/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 3: Sep 7</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../w04/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 4: Sep 14</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../w05/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 5: Sep 20</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../w06/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 6: Sep 27</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../w07/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 7: Oct 5</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../recordings/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lecture Recordings</span></a>
  </div>
</li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../extra-videos/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Extra Videos</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../writeups/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Extra Writeups</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../cheatsheet-math.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Math Cheatsheet</span></a>
  </div>
</li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="https://georgetown.instructure.com/courses/173333" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Course Canvas Page <i class="bi bi-box-arrow-up-right ps-1"></i></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="https://calendar.app.google/yMnESMtMNPdKEaPz5" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Book Office Hours <i class="bi bi-box-arrow-up-right ps-1"></i></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="https://forms.gle/QBT621Vmcez4Y99r5" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Suggestion Box <i class="bi bi-box-arrow-up-right ps-1"></i></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="https://jjacobs.me/dsps" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Textbook <i class="bi bi-box-arrow-up-right ps-1"></i></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#statistical-inference" id="toc-statistical-inference" class="nav-link active" data-scroll-target="#statistical-inference">Statistical Inference</a>
  <ul class="collapse">
  <li><a href="#what-are-we-covering-today" id="toc-what-are-we-covering-today" class="nav-link" data-scroll-target="#what-are-we-covering-today">What are we covering today?</a></li>
  <li><a href="#large-random-samples" id="toc-large-random-samples" class="nav-link" data-scroll-target="#large-random-samples">Large Random Samples</a></li>
  <li><a href="#what-is-the-law-of-large-numbers" id="toc-what-is-the-law-of-large-numbers" class="nav-link" data-scroll-target="#what-is-the-law-of-large-numbers">What Is the Law of Large Numbers?</a></li>
  <li><a href="#statistical-convergence" id="toc-statistical-convergence" class="nav-link" data-scroll-target="#statistical-convergence">Statistical convergence</a></li>
  <li><a href="#quote" id="toc-quote" class="nav-link" data-scroll-target="#quote">Quote</a></li>
  <li><a href="#applications-of-the-law-of-large-numbers" id="toc-applications-of-the-law-of-large-numbers" class="nav-link" data-scroll-target="#applications-of-the-law-of-large-numbers">Applications of the Law of Large Numbers</a></li>
  <li><a href="#applications-of-the-law-of-large-numbers-1" id="toc-applications-of-the-law-of-large-numbers-1" class="nav-link" data-scroll-target="#applications-of-the-law-of-large-numbers-1">Applications of the Law of Large Numbers</a></li>
  <li><a href="#the-law-of-large-numbers" id="toc-the-law-of-large-numbers" class="nav-link" data-scroll-target="#the-law-of-large-numbers">The Law of Large Numbers</a></li>
  <li><a href="#properties-of-the-sample-mean" id="toc-properties-of-the-sample-mean" class="nav-link" data-scroll-target="#properties-of-the-sample-mean">Properties of the Sample Mean</a></li>
  <li><a href="#properties-of-the-sample-mean-1" id="toc-properties-of-the-sample-mean-1" class="nav-link" data-scroll-target="#properties-of-the-sample-mean-1">Properties of the Sample Mean</a></li>
  <li><a href="#extra-notes-converges-in-probability" id="toc-extra-notes-converges-in-probability" class="nav-link" data-scroll-target="#extra-notes-converges-in-probability">Extra Notes: Converges in Probability</a></li>
  <li><a href="#extra-notes-convergence-in-probability" id="toc-extra-notes-convergence-in-probability" class="nav-link" data-scroll-target="#extra-notes-convergence-in-probability">Extra Notes: Convergence in probability</a></li>
  <li><a href="#extra-notes-weak-law-of-large-numbers" id="toc-extra-notes-weak-law-of-large-numbers" class="nav-link" data-scroll-target="#extra-notes-weak-law-of-large-numbers">Extra Notes: Weak Law of Large Numbers</a></li>
  <li><a href="#extra-notes-strong-law-of-large-numbers" id="toc-extra-notes-strong-law-of-large-numbers" class="nav-link" data-scroll-target="#extra-notes-strong-law-of-large-numbers">Extra Notes: Strong Law of Large Numbers</a></li>
  <li><a href="#extra-notes-wlln-slln" id="toc-extra-notes-wlln-slln" class="nav-link" data-scroll-target="#extra-notes-wlln-slln">Extra Notes: WLLN &amp; SLLN</a></li>
  <li><a href="#extra-notes-strong-law-of-large-numbers-1" id="toc-extra-notes-strong-law-of-large-numbers-1" class="nav-link" data-scroll-target="#extra-notes-strong-law-of-large-numbers-1">Extra Notes: Strong Law of large numbers</a></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary">Summary</a></li>
  <li><a href="#law-of-large-numbers" id="toc-law-of-large-numbers" class="nav-link" data-scroll-target="#law-of-large-numbers">Law of large numbers</a></li>
  </ul></li>
  <li><a href="#central-limit-theorem" id="toc-central-limit-theorem" class="nav-link" data-scroll-target="#central-limit-theorem">Central limit theorem</a>
  <ul class="collapse">
  <li><a href="#motivation" id="toc-motivation" class="nav-link" data-scroll-target="#motivation">Motivation</a></li>
  <li><a href="#visualization" id="toc-visualization" class="nav-link" data-scroll-target="#visualization">Visualization</a></li>
  <li><a href="#statement" id="toc-statement" class="nav-link" data-scroll-target="#statement">Statement</a></li>
  <li><a href="#clt" id="toc-clt" class="nav-link" data-scroll-target="#clt">CLT</a></li>
  <li><a href="#extra-notes-convergence-in-distribution" id="toc-extra-notes-convergence-in-distribution" class="nav-link" data-scroll-target="#extra-notes-convergence-in-distribution">Extra Notes: Convergence in Distribution</a></li>
  <li><a href="#clt-1" id="toc-clt-1" class="nav-link" data-scroll-target="#clt-1">CLT</a></li>
  <li><a href="#example" id="toc-example" class="nav-link" data-scroll-target="#example">Example</a></li>
  <li><a href="#clt-2" id="toc-clt-2" class="nav-link" data-scroll-target="#clt-2">CLT</a></li>
  <li><a href="#clt-applications" id="toc-clt-applications" class="nav-link" data-scroll-target="#clt-applications">CLT Applications</a></li>
  <li><a href="#motivating-example" id="toc-motivating-example" class="nav-link" data-scroll-target="#motivating-example">Motivating example</a></li>
  <li><a href="#example-1" id="toc-example-1" class="nav-link" data-scroll-target="#example-1">Example</a></li>
  <li><a href="#summary-1" id="toc-summary-1" class="nav-link" data-scroll-target="#summary-1">Summary</a></li>
  <li><a href="#sample" id="toc-sample" class="nav-link" data-scroll-target="#sample">Sample</a></li>
  <li><a href="#sums-of-normally-distributed-random-variables" id="toc-sums-of-normally-distributed-random-variables" class="nav-link" data-scroll-target="#sums-of-normally-distributed-random-variables">Sums of Normally Distributed Random Variables</a></li>
  <li><a href="#example-1-1" id="toc-example-1-1" class="nav-link" data-scroll-target="#example-1-1">Example-1</a></li>
  <li><a href="#example-2" id="toc-example-2" class="nav-link" data-scroll-target="#example-2">Example-2</a></li>
  </ul></li>
  <li><a href="#estimation" id="toc-estimation" class="nav-link" data-scroll-target="#estimation">Estimation</a>
  <ul class="collapse">
  <li><a href="#statistical-inference-1" id="toc-statistical-inference-1" class="nav-link" data-scroll-target="#statistical-inference-1">Statistical Inference</a></li>
  <li><a href="#statistical-inference-2" id="toc-statistical-inference-2" class="nav-link" data-scroll-target="#statistical-inference-2">Statistical Inference</a></li>
  <li><a href="#definition" id="toc-definition" class="nav-link" data-scroll-target="#definition">Definition</a></li>
  <li><a href="#statistical-decision-problems" id="toc-statistical-decision-problems" class="nav-link" data-scroll-target="#statistical-decision-problems">Statistical Decision Problems</a></li>
  <li><a href="#experimental-design" id="toc-experimental-design" class="nav-link" data-scroll-target="#experimental-design">Experimental Design</a></li>
  <li><a href="#other-inferences" id="toc-other-inferences" class="nav-link" data-scroll-target="#other-inferences">Other Inferences</a></li>
  <li><a href="#statistical-model" id="toc-statistical-model" class="nav-link" data-scroll-target="#statistical-model">Statistical Model</a></li>
  <li><a href="#parameterparameter-space" id="toc-parameterparameter-space" class="nav-link" data-scroll-target="#parameterparameter-space">Parameter/Parameter space</a></li>
  <li><a href="#example-3" id="toc-example-3" class="nav-link" data-scroll-target="#example-3">Example</a></li>
  <li><a href="#statistic" id="toc-statistic" class="nav-link" data-scroll-target="#statistic">Statistic</a></li>
  <li><a href="#maximum-likelihood-estimation-mle" id="toc-maximum-likelihood-estimation-mle" class="nav-link" data-scroll-target="#maximum-likelihood-estimation-mle">Maximum Likelihood Estimation (MLE)</a></li>
  <li><a href="#overview" id="toc-overview" class="nav-link" data-scroll-target="#overview">Overview</a></li>
  <li><a href="#recall-joint-probability-density-function" id="toc-recall-joint-probability-density-function" class="nav-link" data-scroll-target="#recall-joint-probability-density-function">Recall: Joint probability density function</a></li>
  <li><a href="#likelihood-function" id="toc-likelihood-function" class="nav-link" data-scroll-target="#likelihood-function">Likelihood Function</a></li>
  <li><a href="#likelihood-function-1" id="toc-likelihood-function-1" class="nav-link" data-scroll-target="#likelihood-function-1">Likelihood function</a></li>
  <li><a href="#visualization-1" id="toc-visualization-1" class="nav-link" data-scroll-target="#visualization-1">Visualization</a></li>
  <li><a href="#mle" id="toc-mle" class="nav-link" data-scroll-target="#mle">MLE</a></li>
  <li><a href="#example-poisson-distribution" id="toc-example-poisson-distribution" class="nav-link" data-scroll-target="#example-poisson-distribution">Example: Poisson distribution</a></li>
  <li><a href="#example-exponential-distribution" id="toc-example-exponential-distribution" class="nav-link" data-scroll-target="#example-exponential-distribution">Example: Exponential distribution</a></li>
  <li><a href="#calculation" id="toc-calculation" class="nav-link" data-scroll-target="#calculation">Calculation</a></li>
  <li><a href="#example-bernoulli-distribution" id="toc-example-bernoulli-distribution" class="nav-link" data-scroll-target="#example-bernoulli-distribution">Example: Bernoulli distribution</a></li>
  <li><a href="#exponential-families" id="toc-exponential-families" class="nav-link" data-scroll-target="#exponential-families">Exponential Families</a></li>
  <li><a href="#example-cauchy-distribution" id="toc-example-cauchy-distribution" class="nav-link" data-scroll-target="#example-cauchy-distribution">Example: Cauchy Distribution</a></li>
  <li><a href="#likelihood-function-2" id="toc-likelihood-function-2" class="nav-link" data-scroll-target="#likelihood-function-2">Likelihood function</a></li>
  <li><a href="#likelihood-function-for-normal-distribution" id="toc-likelihood-function-for-normal-distribution" class="nav-link" data-scroll-target="#likelihood-function-for-normal-distribution">Likelihood function for normal distribution</a></li>
  <li><a href="#log-likelihood" id="toc-log-likelihood" class="nav-link" data-scroll-target="#log-likelihood">Log Likelihood</a></li>
  <li><a href="#calculation-1" id="toc-calculation-1" class="nav-link" data-scroll-target="#calculation-1">Calculation</a></li>
  <li><a href="#maximum-likelihood" id="toc-maximum-likelihood" class="nav-link" data-scroll-target="#maximum-likelihood">Maximum likelihood</a></li>
  <li><a href="#maximum-likelihood-estimate" id="toc-maximum-likelihood-estimate" class="nav-link" data-scroll-target="#maximum-likelihood-estimate">Maximum Likelihood Estimate</a></li>
  <li><a href="#arg-max-function" id="toc-arg-max-function" class="nav-link" data-scroll-target="#arg-max-function">Arg-max function</a></li>
  <li><a href="#maximum-likelihood-1" id="toc-maximum-likelihood-1" class="nav-link" data-scroll-target="#maximum-likelihood-1">Maximum Likelihood</a></li>
  <li><a href="#example-exponential-distribution-1" id="toc-example-exponential-distribution-1" class="nav-link" data-scroll-target="#example-exponential-distribution-1">Example: Exponential Distribution</a></li>
  <li><a href="#examples-of-mles" id="toc-examples-of-mles" class="nav-link" data-scroll-target="#examples-of-mles">Examples of MLEs</a></li>
  <li><a href="#normal-distribution" id="toc-normal-distribution" class="nav-link" data-scroll-target="#normal-distribution">Normal Distribution</a></li>
  <li><a href="#uniform-distribution" id="toc-uniform-distribution" class="nav-link" data-scroll-target="#uniform-distribution">Uniform Distribution</a></li>
  <li><a href="#example-logistic-regression" id="toc-example-logistic-regression" class="nav-link" data-scroll-target="#example-logistic-regression">Example: Logistic Regression</a></li>
  <li><a href="#example-logistic-regression-1" id="toc-example-logistic-regression-1" class="nav-link" data-scroll-target="#example-logistic-regression-1">Example: Logistic Regression</a></li>
  <li><a href="#example-logistic-regression-2" id="toc-example-logistic-regression-2" class="nav-link" data-scroll-target="#example-logistic-regression-2">Example: Logistic Regression</a></li>
  <li><a href="#notes" id="toc-notes" class="nav-link" data-scroll-target="#notes">Notes</a></li>
  <li><a href="#overview-1" id="toc-overview-1" class="nav-link" data-scroll-target="#overview-1">Overview</a></li>
  <li><a href="#method-of-moments-estimation" id="toc-method-of-moments-estimation" class="nav-link" data-scroll-target="#method-of-moments-estimation">Method of Moments Estimation</a></li>
  <li><a href="#example-uniform-distribution" id="toc-example-uniform-distribution" class="nav-link" data-scroll-target="#example-uniform-distribution">Example: Uniform Distribution</a></li>
  <li><a href="#example-beta-distribution" id="toc-example-beta-distribution" class="nav-link" data-scroll-target="#example-beta-distribution">Example: Beta Distribution</a></li>
  <li><a href="#estimation-using-method-of-moments" id="toc-estimation-using-method-of-moments" class="nav-link" data-scroll-target="#estimation-using-method-of-moments">Estimation using Method of Moments</a></li>
  <li><a href="#resulting-estimators" id="toc-resulting-estimators" class="nav-link" data-scroll-target="#resulting-estimators">Resulting Estimators</a></li>
  <li><a href="#bias-and-variance" id="toc-bias-and-variance" class="nav-link" data-scroll-target="#bias-and-variance">Bias and Variance</a></li>
  <li><a href="#bias-and-variance-1" id="toc-bias-and-variance-1" class="nav-link" data-scroll-target="#bias-and-variance-1">Bias and Variance</a></li>
  <li><a href="#unbiased-estimator" id="toc-unbiased-estimator" class="nav-link" data-scroll-target="#unbiased-estimator">Unbiased Estimator</a></li>
  <li><a href="#unbiased-estimator-1" id="toc-unbiased-estimator-1" class="nav-link" data-scroll-target="#unbiased-estimator-1">Unbiased Estimator</a></li>
  <li><a href="#unbiased-estimator-2" id="toc-unbiased-estimator-2" class="nav-link" data-scroll-target="#unbiased-estimator-2">Unbiased Estimator</a></li>
  <li><a href="#example-poisson-distribution-1" id="toc-example-poisson-distribution-1" class="nav-link" data-scroll-target="#example-poisson-distribution-1">Example: Poisson Distribution</a></li>
  <li><a href="#exponential-distribution" id="toc-exponential-distribution" class="nav-link" data-scroll-target="#exponential-distribution">Exponential Distribution</a></li>
  <li><a href="#calculation-1-1" id="toc-calculation-1-1" class="nav-link" data-scroll-target="#calculation-1-1">Calculation-1</a></li>
  <li><a href="#calculation-2" id="toc-calculation-2" class="nav-link" data-scroll-target="#calculation-2">Calculation-2</a></li>
  <li><a href="#unbiased-estimation-of-the-variance" id="toc-unbiased-estimation-of-the-variance" class="nav-link" data-scroll-target="#unbiased-estimation-of-the-variance">Unbiased Estimation of the Variance</a></li>
  <li><a href="#example-4" id="toc-example-4" class="nav-link" data-scroll-target="#example-4">Example</a></li>
  <li><a href="#mean-square-error" id="toc-mean-square-error" class="nav-link" data-scroll-target="#mean-square-error">Mean Square Error</a></li>
  <li><a href="#mse" id="toc-mse" class="nav-link" data-scroll-target="#mse">MSE</a></li>
  <li><a href="#mse-1" id="toc-mse-1" class="nav-link" data-scroll-target="#mse-1">MSE</a></li>
  <li><a href="#efficiency" id="toc-efficiency" class="nav-link" data-scroll-target="#efficiency">Efficiency</a></li>
  <li><a href="#note" id="toc-note" class="nav-link" data-scroll-target="#note">Note</a></li>
  <li><a href="#efficiency-1" id="toc-efficiency-1" class="nav-link" data-scroll-target="#efficiency-1">Efficiency</a></li>
  <li><a href="#example-mean-and-median" id="toc-example-mean-and-median" class="nav-link" data-scroll-target="#example-mean-and-median">Example: Mean and Median</a></li>
  <li><a href="#example-5" id="toc-example-5" class="nav-link" data-scroll-target="#example-5">Example</a></li>
  <li><a href="#calculation-3" id="toc-calculation-3" class="nav-link" data-scroll-target="#calculation-3">Calculation</a></li>
  <li><a href="#bias-variance-trade-off---a-look-ahead" id="toc-bias-variance-trade-off---a-look-ahead" class="nav-link" data-scroll-target="#bias-variance-trade-off---a-look-ahead">Bias-Variance Trade Off - a look ahead</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">



<section id="statistical-inference" class="level1">
<h1>Statistical Inference</h1>
<p>Modified from Dr.&nbsp;Purna Gamage ANLY-511 slides</p>
<section id="what-are-we-covering-today" class="level2">
<h2 class="anchored" data-anchor-id="what-are-we-covering-today">What are we covering today?</h2>
<ol type="1">
<li>Large Random Samples (Chapter 6)</li>
</ol>
<ul>
<li>Law of Large Numbers</li>
<li>The Central Limit Theorem</li>
</ul>
<ol start="2" type="1">
<li>Estimation</li>
</ol>
<ul>
<li>Maximum Likelihood Estimators</li>
<li>Method of Moments</li>
<li>Unbiased Estimators</li>
<li>Efficiency</li>
</ul>
</section>
<section id="large-random-samples" class="level2">
<h2 class="anchored" data-anchor-id="large-random-samples">Large Random Samples</h2>
<p><img src="images/image-20231016073553820.png" class="img-fluid"></p>
</section>
<section id="what-is-the-law-of-large-numbers" class="level2">
<h2 class="anchored" data-anchor-id="what-is-the-law-of-large-numbers">What Is the Law of Large Numbers?</h2>
<ul>
<li>The law of large numbers, in probability and statistics, states that as a sample size grows, its mean gets closer to the average of the whole population.</li>
<li>In the 16th century, mathematician Gerolama Cardano recognized the Law of Large Numbers but never proved it.</li>
<li>In 1713, Swiss mathematician Jakob Bernoulli proved this theorem in his book, Ars Conjectandi.</li>
<li>It was later refined by other noted mathematicians, such as Pafnuty Chebyshev, founder of the St.&nbsp;Petersburg mathematical school. https://www.investopedia.com/terms/l/lawoflargenumbers.asp</li>
</ul>
<p><img src="images/image-20231016073535050.png" class="img-fluid"></p>
</section>
<section id="statistical-convergence" class="level2">
<h2 class="anchored" data-anchor-id="statistical-convergence">Statistical convergence</h2>
<ul>
<li>Statistical convergence involves the tendency of a sequence of random variables to stabilize in distribution as the sample size increases, indicating a likelihood of approaching a limiting behavior in a probabilistic sense rather than pointwise certainty.</li>
</ul>
<p><img src="images/image-20231016073711190.png" class="img-fluid"></p>
<p><strong>Good visual resource</strong>: https://seeing-theory.brown.edu/basic-probability/index.html#section1</p>
</section>
<section id="quote" class="level2">
<h2 class="anchored" data-anchor-id="quote">Quote</h2>
<p>“<a href="#the-law-of-large-numbers">The Law of Large Numbers</a> has nothing whatever to do with growth. What it actually says is that as a large number of samples of a random variable are taken from a population, the mean of the samples approaches the expected value of the population. In other (and simplified) terms, the larger your sample the better your estimate of the actual value… the basis of all sampling, polling, and inferential statistics…</p>
<p>“So what do we call the principle that the growth rate of things tends to slow as they get larger? The idea is kind of obvious, which may be why it doesn’t have a name [so] I propose we call it the logistic principle.”</p>
<ul>
<li>Steve Wildstrom (via Techpinions, highlights courtesy of Annotote)</li>
</ul>
</section>
<section id="applications-of-the-law-of-large-numbers" class="level2">
<h2 class="anchored" data-anchor-id="applications-of-the-law-of-large-numbers">Applications of the Law of Large Numbers</h2>
<ol type="1">
<li>Vehicle Automation</li>
</ol>
<p>AI development for self-driving vehicles takes the law of large numbers quite literally, and runs with it (pun intended). Tesla for example, parses and collates data from countless Tesla car users, “using billions of miles to train neural networks”.</p>
<p>In this example, car mileage data is averaged to plot out and optimize paths and driving policies. Recorded video and images are repeatedly analyzed by the AI, so that it eventually predicts visual elements with a reliable rate of probability. Even data involving the driving decisions of other cars on the road, is averaged to help the AI make better predictions of what other drivers are most likely to do in the near future.</p>
<p><img src="images/image-20231016073815505.png" class="img-fluid"></p>
<p>https://www.analyticssteps.com/blogs/how-tesla-making-use-artificial-intelligence-its-operations</p>
<p>Model Y Unveil: Elon Musk</p>
<p>https://www.youtube.com/watch?v=Tb_Wn6K0uVs&amp;feature=emb_logo</p>
<p>Tesla has taken excellent use of AI and Big Data for expanding its customer base. The firm has made use of existing customer databases for its data analytics using it to comprehend customer requirements and regularly updating their systems accordingly</p>
<p>https://medium.com/kambria-network/the-importance-of-the-law-of-large-numbers-in-ai-ea55d8af21cf</p>
</section>
<section id="applications-of-the-law-of-large-numbers-1" class="level2">
<h2 class="anchored" data-anchor-id="applications-of-the-law-of-large-numbers-1">Applications of the Law of Large Numbers</h2>
<p>Other notable demonstrations of the law of large numbers in <span class="math inline">\(\mathrm{AI}\)</span> that are potential game changers, such as deep learning-based weather prediction and the ever-improving gambling AI, are also bound to shape the future of our world in some way, and could take us to directions we have yet to even begin to consider. As one Google Translate engineer put it, “when you go from 10,000 training examples to 10 billion training examples, it all starts to work. Data trumps everything.” Garry Kasparov, yes the man defeated in chess by the AI Deep_Blue, mentions this quote from his book Deep Thinking: Where Machine Intelligence Ends and Human Creativity Begins. This one sentence sums up succinctly why the law of large numbers is inevitably intertwined with AI.</p>
<p>https://medium.com/kambria-network/the-importance-of-the-law-of-large-numbers-in-ai-ea55d8af21cf</p>
</section>
<section id="the-law-of-large-numbers" class="level2">
<h2 class="anchored" data-anchor-id="the-law-of-large-numbers">The Law of Large Numbers</h2>
<p><em>The average of a random sample of i.i.d. random variables is called their sample mean.</em></p>
<p><em>The</em> <em>sample mean</em> <em>is useful for</em> <em>summarizing the information in a random sample</em> <em>in much the same way that the mean of a probability distribution summarizes the information in the distribution.</em></p>
<p><em>In this section, we present some results that illustrate the connection between the sample mean and the expected value of the individual random variables that comprise the random sample.</em></p>
<p>Fun Interactive Viz:</p>
<p>https://seeing-theory.brown.edu/basic-probability/index.html#section1</p>
</section>
<section id="properties-of-the-sample-mean" class="level2">
<h2 class="anchored" data-anchor-id="properties-of-the-sample-mean">Properties of the Sample Mean</h2>
<p>In Definition 5.6.3, we defined the sample mean of <span class="math inline">\(n\)</span> random variables <span class="math inline">\(X_1, \ldots, X_n\)</span> to be their average, <span class="math display">\[
\bar{X}_n=\frac{1}{n}\left(X_1+\cdots+X_n\right) .
\]</span> The mean and the variance of <span class="math inline">\(\bar{X}_n\)</span> are easily computed.</p>
</section>
<section id="properties-of-the-sample-mean-1" class="level2">
<h2 class="anchored" data-anchor-id="properties-of-the-sample-mean-1">Properties of the Sample Mean</h2>
<p>Mean and Variance of the Sample Mean. Let <span class="math inline">\(X_1, \ldots, X_n\)</span> be a random sample fron a distribution with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>. Let <span class="math inline">\(\bar{X}_n\)</span> be the sample mean. The <span class="math inline">\(E\left(\bar{X}_n\right)=\mu\)</span> and <span class="math inline">\(\operatorname{Var}\left(\bar{X}_n\right)=\sigma^2 / n\)</span>. Proof It follows from Theorems 4.2.1 and 4.2.4 that <span class="math display">\[
E\left(\bar{X}_n\right)=\frac{1}{n} \sum_{i=1}^n E\left(X_i\right)=\frac{1}{n} \cdot n \mu=\mu .
\]</span> Furthermore, since <span class="math inline">\(X_1, \ldots, X_n\)</span> are independent, Theorems 4.3.4 and 4.3.5 say that <span class="math display">\[
\begin{aligned}
\operatorname{Var}\left(\bar{X}_n\right) &amp; =\frac{1}{n^2} \operatorname{Var}\left(\sum_{i=1}^n X_i\right) \\
&amp; =\frac{1}{n^2} \sum_{i=1}^n \operatorname{Var}\left(X_i\right)=\frac{1}{n^2} \cdot n \sigma^2=\frac{\sigma^2}{n}
\end{aligned}
\]</span></p>
</section>
<section id="extra-notes-converges-in-probability" class="level2">
<h2 class="anchored" data-anchor-id="extra-notes-converges-in-probability">Extra Notes: Converges in Probability</h2>
<p>Convergence in Probability. A sequence <span class="math inline">\(Z_1, Z_2, \ldots\)</span> of random variables converges to <span class="math inline">\(b\)</span> in probability if for every number <span class="math inline">\(\varepsilon&gt;0\)</span>, <span class="math display">\[
\lim _{n \rightarrow \infty} \operatorname{Pr}\left(\left|Z_n-b\right|&lt;\varepsilon\right)=1 .
\]</span> This property is denoted by <span class="math display">\[
Z_n \stackrel{p}{\longrightarrow} b,
\]</span> and is sometimes stated simply as <span class="math inline">\(Z_n\)</span> converges to <span class="math inline">\(b\)</span> in probability.</p>
<p>In other words, <span class="math inline">\(Z_n\)</span> converges to <span class="math inline">\(b\)</span> in probability if the probability that <span class="math inline">\(Z_n\)</span> lies in each given interval around <span class="math inline">\(b\)</span>, no matter how small this interval may be, approaches 1 as <span class="math inline">\(n \rightarrow \infty\)</span>.</p>
</section>
<section id="extra-notes-convergence-in-probability" class="level2">
<h2 class="anchored" data-anchor-id="extra-notes-convergence-in-probability">Extra Notes: Convergence in probability</h2>
<p>Let <span class="math inline">\(S_n=\frac{1}{n} \sum_{j=1}^n X_j\)</span> be the sample mean of the first <span class="math inline">\(n\)</span> observations. if you have a sample of independent and identically distributed random variables, as the sample size grows larger, the sample mean will tend toward the population mean. <span class="math display">\[
P\left(\left|S_n-\mu\right|&gt;\epsilon\right) \rightarrow 0 \quad \text { for any } \quad \epsilon&gt;0
\]</span> - This is called “convergence in probability”. - The probability of seeing the event <span class="math inline">\(\left|S_n-\mu\right|&gt;\epsilon\)</span> becomes very small as <span class="math inline">\(n\)</span> becomes large. - <span class="math inline">\(\quad\)</span> Box plot, histograms of the <span class="math inline">\(S_n\)</span> etc. all become closer and closer to the constant <span class="math inline">\(\mu\)</span>. - This is a statement about individual observations: Eventually most <span class="math inline">\(S_n\)</span> are close to <span class="math inline">\(\mu\)</span>.</p>
</section>
<section id="extra-notes-weak-law-of-large-numbers" class="level2">
<h2 class="anchored" data-anchor-id="extra-notes-weak-law-of-large-numbers">Extra Notes: Weak Law of Large Numbers</h2>
<p>(Weak Law of Large Numbers) Let <span class="math inline">\(X_1, X_2, \ldots, X_n\)</span> be a sequence of mutually independent and identically distributed random variables each of which has a finite mean <span class="math inline">\(E\left[X_k\right]=\mu_X&lt;\infty, k=1,2, \ldots, n\)</span>. Let <span class="math inline">\(S_n\)</span> be the linear sum of the <span class="math inline">\(n\)</span> random variables; that is, <span class="math display">\[
S_n=X_1+X_2+\cdots+X_n
\]</span> Then for any <span class="math inline">\(\varepsilon&gt;0\)</span>, <span class="math display">\[
\lim _{n \rightarrow \infty} P\left[\left|\frac{S_n}{n}-\mu_X\right| \geq \varepsilon\right] \rightarrow 0
\]</span> Alternatively, <span class="math display">\[
\lim _{n \rightarrow \infty} P\left[\left|\frac{S_n}{n}-\mu_X\right|&lt;\varepsilon\right] \rightarrow 1
\]</span> Proof: By definition, <span class="math display">\[
\begin{gathered}
S_n=X_1+X_2+\cdots+X_n \\
\bar{S}_n=\frac{S_n}{n}=\frac{X_1+X_2+\cdots+X_n}{n}=\frac{n \mu_X}{n}=\mu_X \\
\operatorname{Var}\left(\bar{S}_n\right)=\operatorname{Var}\left\{\frac{X_1+X_2+\cdots+X_n}{n}\right\} \\
=\frac{1}{n^2}\left\{\operatorname{Var}\left(X_1\right)+\operatorname{Var}\left(X_2\right)+\cdots+\operatorname{Var}\left(X_n\right)\right\}=\frac{n \sigma_X^2}{n^2}
\end{gathered}
\]</span> https://www.sciencedirect.com/book/9780128008522/fundamentals-of-applied-probability-and-random-processes</p>
</section>
<section id="extra-notes-strong-law-of-large-numbers" class="level2">
<h2 class="anchored" data-anchor-id="extra-notes-strong-law-of-large-numbers">Extra Notes: Strong Law of Large Numbers</h2>
<p>(Strong Law of Large Numbers) Let <span class="math inline">\(X_1, X_2, \ldots, X_n\)</span> be a sequence of mutually independent and identically distributed random variables each of which has a finite mean <span class="math inline">\(E\left[X_k\right]=\mu_X&lt;\infty, k=1,2, \ldots, n\)</span>. Let <span class="math inline">\(S_n\)</span> be the linear sum of the <span class="math inline">\(n\)</span> random variables; that is, <span class="math display">\[
S_n=X_1+X_2+\cdots+X_n
\]</span> Then for any <span class="math inline">\(\varepsilon&gt;0\)</span>, <span class="math display">\[
P\left[\lim _{n \rightarrow \infty}\left|\bar{S}_n-\mu_X\right|&gt;\varepsilon\right]=0
\]</span> where <span class="math inline">\(\bar{S}_n=S_n / n\)</span>. An alternativ statement of the law is <span class="math display">\[
P\left[\lim _{n \rightarrow \infty}\left|\bar{S}_n-\mu_X\right| \leq \varepsilon\right]=1
\]</span> https://www.sciencedirect.com/book/9780128008522/fundamentals-of-applied-probability-and-random-processes</p>
</section>
<section id="extra-notes-wlln-slln" class="level2">
<h2 class="anchored" data-anchor-id="extra-notes-wlln-slln">Extra Notes: WLLN &amp; SLLN</h2>
<p>The weak law of large numbers essentially states that for any nonzero specified margin, no matter how small, there is a high probability that the average of a sufficiently large number of observations will be close to the expected value within the margin. That is, <span class="math display">\[
\lim _{n \rightarrow \infty} \bar{S}_n \rightarrow \mu_X
\]</span> Alternatively, the arithmetic average <span class="math inline">\(\bar{S}_n\)</span> of a sequence of independent observations of a random variable <span class="math inline">\(X\)</span> converges with probability <span class="math inline">\(I\)</span> to the expected value <span class="math inline">\(\mu_X\)</span> of <span class="math inline">\(X\)</span>. Thus, the weak law is a convergence statement about a sequence of probabilities; it states that the sequence of random variables <span class="math inline">\(\left\{\bar{S}_n\right\}\)</span> converges in probability to the population mean <span class="math inline">\(\mu_X\)</span> as <span class="math inline">\(n\)</span> becomes very large.</p>
<p>The strong law of large numbers states that with probability 1 the sequence of sample means <span class="math inline">\(\bar{S}_n\)</span> converges to a constant value <span class="math inline">\(\mu_x\)</span>, which is the population mean of the random variables, as <span class="math inline">\(n\)</span> becomes very large. This validates the relative-frequency definition of probability.</p>
<p>https://www.sciencedirect.com/book/9780128008522/fundamentals-of-applied-probability-and-random-processes</p>
</section>
<section id="extra-notes-strong-law-of-large-numbers-1" class="level2">
<h2 class="anchored" data-anchor-id="extra-notes-strong-law-of-large-numbers-1">Extra Notes: Strong Law of large numbers</h2>
<p><span class="math inline">\(S_n \rightarrow \mu \quad\)</span> with probability 1</p>
<ul>
<li>This is called “almost sure convergence. The probability that <span class="math inline">\(S_n\)</span> does not converge to 0 is zero.</li>
<li>This is also a statement about individual observations: Eventually practically every <span class="math inline">\(S_n\)</span> is close to <span class="math inline">\(\mu\)</span>.</li>
</ul>
</section>
<section id="summary" class="level2">
<h2 class="anchored" data-anchor-id="summary">Summary</h2>
<p><strong>Strong Law of Large Number</strong> The strong law of large numbers states that with probability 1 the sequence of sample means <span class="math inline">\(S^{-} n\)</span> converges to a constant value <span class="math inline">\(\mu \mathrm{X}\)</span>, which is the population mean of the random variables, as <span class="math inline">\(n\)</span> becomes very large. From: Fundamentals of Applied Probability and Random Processes (Second Edition), 2014</p>
<p><strong>Weak Law of Large Number</strong> The weak law of large numbers essentially states that for any nonzero specified margin, no matter how small, there is a high probability that the average of a sufficiently large number of observations will be close to the expected value within the margin. From: Fundamentals of Applied Probability and Random Processes (Second Edition), 2014</p>
<p>https://www.sciencedirect.com/topics/mathematics/strong-law-of-large-number</p>
<p>https://www.sciencedirect.com/topics/mathematics/weak-law-of-large-number#:~:text=6.9%20Laws%20of%20Large%20Numbers&amp;text=One%20law%20is%20called%20the,variables%20behaves%20in%20the%20limit.</p>
</section>
<section id="law-of-large-numbers" class="level2">
<h2 class="anchored" data-anchor-id="law-of-large-numbers">Law of large numbers</h2>
<p>Law of Large Numbers. Suppose that <span class="math inline">\(X_1, \ldots, X_n\)</span> form a random sample from a distribution for which the mean is <span class="math inline">\(\mu\)</span> and for which the variance is finite. Let <span class="math inline">\(\bar{X}_n\)</span> denote the sample mean. Then <span class="math display">\[
\bar{X}_n \stackrel{p}{\longrightarrow} \mu \text {. }
\]</span></p>
</section>
</section>
<section id="central-limit-theorem" class="level1">
<h1>Central limit theorem</h1>
<section id="motivation" class="level2">
<h2 class="anchored" data-anchor-id="motivation">Motivation</h2>
<p>The Central Limit Theorem states that, regardless of the original distribution, the sum (or average) of a sufficiently large number of independent and identically distributed random variables will converge to a normal distribution.</p>
<p><img src="images/image-20231016074710429.png" class="img-fluid"></p>
</section>
<section id="visualization" class="level2">
<h2 class="anchored" data-anchor-id="visualization">Visualization</h2>
<p>Good Interactive Viz: https://seeing-theory.brown.edu/probability-distributions/index.html#section3</p>
<p><img src="images/image-20231016075500257.png" class="img-fluid"></p>
</section>
<section id="statement" class="level2">
<h2 class="anchored" data-anchor-id="statement">Statement</h2>
<p>The central limit theorem states the distribution of sample means should be approximately normal.</p>
<p>Central Limit Theorem (Lindeberg and Lévy). If the random variables <span class="math inline">\(X_1, \ldots, X_n\)</span> form a random sample of size <span class="math inline">\(n\)</span> from a given distribution with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span> <span class="math inline">\(\left(0&lt;\sigma^2&lt;\infty\right)\)</span>, then for each fixed number <span class="math inline">\(x\)</span>, <span class="math display">\[
\lim _{n \rightarrow \infty} \operatorname{Pr}\left[\frac{\bar{X}_n-\mu}{\sigma / n^{1 / 2}} \leq x\right]=\Phi(x),
\]</span> where <span class="math inline">\(\Phi\)</span> denotes the c.d.f. of the standard normal distribution.</p>
</section>
<section id="clt" class="level2">
<h2 class="anchored" data-anchor-id="clt">CLT</h2>
<p>Let <span class="math inline">\(S_n=\frac{1}{n} \sum_{j=1}^n X_j\)</span> be the sample mean of the first <span class="math inline">\(n\)</span> observations. For large <span class="math inline">\(n\)</span> <span class="math display">\[
\sqrt{n} \frac{S_n-\mu}{\sigma} \sim N(0,1) \quad \text { approximately }
\]</span> The distribution of <span class="math inline">\(\sqrt{n} \frac{S_n-\mu}{\sigma}\)</span> is close to <span class="math inline">\(N(0,1)\)</span> for large <span class="math inline">\(n\)</span>. Formally, <span class="math display">\[
P\left(a \leq \sqrt{n} \frac{S_n-\mu}{\sigma} \leq b\right) \rightarrow P(a \leq Z \leq b)
\]</span> as <span class="math inline">\(n \rightarrow \infty\)</span>, for any <span class="math inline">\(a, b\)</span>, where <span class="math inline">\(Z \sim N(0,1)\)</span>. - Box plot, histograms, etc. all become closer and closer to the constant <span class="math inline">\(\mu\)</span> and also become more bell-shaped. - <span class="math inline">\(\quad\)</span> qqnorm() plots become straight lines. - <span class="math inline">\(\quad\)</span> This is exact if the <span class="math inline">\(X_i\)</span> already have normal distributions.</p>
</section>
<section id="extra-notes-convergence-in-distribution" class="level2">
<h2 class="anchored" data-anchor-id="extra-notes-convergence-in-distribution">Extra Notes: Convergence in Distribution</h2>
<ul>
<li>This is called convergence in distribution. We can say something about the probability distribution of the <span class="math inline">\(S_n\)</span> as <span class="math inline">\(n\)</span> becomes large.</li>
<li>It’s not a statement about individual observations.</li>
<li>But it is a stronger (more precise) statement that the Law of Large Numbers.</li>
</ul>
</section>
<section id="clt-1" class="level2">
<h2 class="anchored" data-anchor-id="clt-1">CLT</h2>
<ul>
<li>The sampling distribution of the sample means approaches a normal distribution as the sample size gets larger - no matter what the shape of the population distribution.</li>
<li>If you sample batches of data from any distribution and take the mean of each batch. Then the distribution of the means is going to resemble a Gaussian distribution. (Same goes for taking the sum)</li>
</ul>
</section>
<section id="example" class="level2">
<h2 class="anchored" data-anchor-id="example">Example</h2>
<p><img src="images/image-20231016075839312.png" class="img-fluid"></p>
</section>
<section id="clt-2" class="level2">
<h2 class="anchored" data-anchor-id="clt-2">CLT</h2>
<ul>
<li>No matter what is the population distribution is, by CLT for large samples, sample mean will follow a normal distribution.</li>
</ul>
<p><img src="images/image-20231016075905197.png" class="img-fluid"></p>
</section>
<section id="clt-applications" class="level2">
<h2 class="anchored" data-anchor-id="clt-applications">CLT Applications</h2>
<p><img src="images/image-20231016080042972.png" class="img-fluid"></p>
</section>
<section id="motivating-example" class="level2">
<h2 class="anchored" data-anchor-id="motivating-example">Motivating example</h2>
<p><img src="images/2023-10-16-08-02-56.png" class="img-fluid"></p>
</section>
<section id="example-1" class="level2">
<h2 class="anchored" data-anchor-id="example-1">Example</h2>
<ul>
<li>Possible samples and sample means of samples of size 2</li>
</ul>
<p><img src="images/2023-10-16-08-04-56.png" class="img-fluid"></p>
</section>
<section id="summary-1" class="level2">
<h2 class="anchored" data-anchor-id="summary-1">Summary</h2>
<p>We all understand intuitively that the average of many measurements of the same unknown quantity tends to give a better estimate than a single measurement. Intuitively, this is because the random error of each measurement cancels out in the average. In these notes we will make this intuition precise in two ways: the law of large numbers (LoLN) and the central limit theorem (CLT).</p>
<p>Briefly, both the law of large numbers and central limit theorem are about many independent samples from same distribution. The LoLN tells us two things: 1. The average of many independent samples is (with high probability) close to the mean of the underlying distribution. 2. This density histogram of many independent samples is (with high probability) close to the graph of the density of the underlying distribution.</p>
<p><a href="https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/readings/%20MIT18_05S14_Reading6b.pdf">https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/readings/ MIT18_05S14_Reading6b.pdf</a></p>
<ul>
<li>LoLN: As <span class="math inline">\(n\)</span> grows, the probability that <span class="math inline">\(X_n\)</span> is close to <span class="math inline">\(\mu\)</span> goes to 1 .</li>
<li>CLT: As <span class="math inline">\(n\)</span> grows, the distribution of <span class="math inline">\(\bar{X}_n\)</span> converges to the normal distribution <span class="math inline">\(N\left(\mu, \sigma^2 / n\right)\)</span>. Before giving a more formal statement of the LoLN, let’s unpack its meaning through a concrete example (we’ll return to the CLT later on).</li>
</ul>
</section>
<section id="sample" class="level2">
<h2 class="anchored" data-anchor-id="sample">Sample</h2>
<ul>
<li>A good sample must be….</li>
<li>Representative of the population,</li>
<li>Big enough to draw conclusions from, which in statistics is a sample size greater or equal to 30 .</li>
<li>Picked at random, so you’re not biased towards certain characteristics in the population.</li>
<li>Also number of samples taken should represent the Population.</li>
</ul>
<p><img src="images/2023-10-16-08-08-24.png" class="img-fluid"></p>
</section>
<section id="sums-of-normally-distributed-random-variables" class="level2">
<h2 class="anchored" data-anchor-id="sums-of-normally-distributed-random-variables">Sums of Normally Distributed Random Variables</h2>
<ul>
<li><p>Suppose <span class="math inline">\(X_1, X_2\)</span> are independent and <span class="math inline">\(X_1 \sim N\left(\mu_1, \sigma_1^2\right), X_2 \sim N\left(\mu_2, \sigma_2^2\right)\)</span>. Then <span class="math display">\[
X_1+X_2 \sim N\left(\mu_1+\mu_2, \sigma_1^2+\sigma_2^2\right)
\]</span> l.e. <span class="math inline">\(\operatorname{Var}\left(X_1+X_2\right)=\sigma_1^2+\sigma_2^2\)</span>.</p></li>
<li><p>This generalizes to sums of several independent normally distributed random variable.</p></li>
<li><p>Suppose <span class="math inline">\(X_1, \ldots, X_n\)</span> are identically <span class="math inline">\(N\left(\mu . \sigma^2\right)\)</span> distributed. Then</p></li>
</ul>
<p><span class="math display">\[
\begin{aligned}
\frac{X_j-\mu}{\sigma} &amp; \sim N(0,1) \\
\sum_{j=1}^n X_j &amp; \sim N\left(n \mu, n \sigma^2\right) \\
S_n &amp; \sim N\left(\mu, \frac{\sigma^2}{n}\right) \\
\sqrt{n} \frac{S_n-\mu}{\sigma} &amp; \sim N(0,1)
\end{aligned}
\]</span></p>
</section>
<section id="example-1-1" class="level2">
<h2 class="anchored" data-anchor-id="example-1-1">Example-1</h2>
<p>SAT Math scores of a group of Science &amp; Engineering majors had a normal distribution with mean 609 and SD 80. We will randomly select <span class="math inline">\(\mathbf{1 0}\)</span> students from these majors.</p>
<ol type="a">
<li>What is the sampling distribution of the sample average?</li>
</ol>
<p>Since the population has a Normal distribution and we took a simple random sample, <span class="math inline">\(\bar{y}\)</span> has a normal distribution (regardless of the sample size).</p>
<ul>
<li>Shape: normal</li>
<li>Mean: <span class="math inline">\(\mu=609\)</span></li>
<li>SD: <span class="math inline">\(\sigma_{\bar{y}}=\frac{\sigma}{\sqrt{n}}=\frac{80}{\sqrt{10}}=25.298\)</span></li>
</ul>
<ol start="2" type="a">
<li>How often would this average be more than 630 ?</li>
</ol>
<p><span class="math display">\[
\begin{gathered}
z=\frac{\bar{y}-\mu}{\sigma / \sqrt{n}}=\frac{630-609}{25.298}=0.83 \\
P(Z&gt;0.83)=1-0.7967=0.2033
\end{gathered}
\]</span></p>
</section>
<section id="example-2" class="level2">
<h2 class="anchored" data-anchor-id="example-2">Example-2</h2>
<p>Let <span class="math inline">\(X_1, X_2, \ldots, X_9 \sim \operatorname{iidN}\left(7,3^2\right)\)</span> and let <span class="math inline">\(Y_1, Y_2, \ldots, Y_{12} \sim \operatorname{iidN}\left(10,5^2\right)\)</span>. Let <span class="math inline">\(W=\bar{X}-\bar{Y}\)</span> be the difference of the sample means. This is similar to the homework problem but not the same. I. Find the exact sampling distribution of <span class="math inline">\(W\)</span> approximately. Solution <span class="math inline">\(\bar{X}\)</span> has a mean 7 and variance <span class="math inline">\(9 / 9=1\)</span>. <span class="math inline">\(\bar{Y}\)</span> has a mean 10 and variance <span class="math inline">\(25 / 12=\)</span>. By the CLT (Which applies already for normal distribution) we have a normal distribution. Therefore <span class="math inline">\(W\)</span> has a normal distribution with mean <span class="math inline">\(\mu=7-10=-3, \sigma^2=1+25 / 12\)</span></p>
</section>
</section>
<section id="estimation" class="level1">
<h1>Estimation</h1>
<section id="statistical-inference-1" class="level2">
<h2 class="anchored" data-anchor-id="statistical-inference-1">Statistical Inference</h2>
<ul>
<li>What would we say is the probability that a future patient will respond successfully to treatment after we observe the results from a collection of other patients?</li>
<li>This is the kind of question that statistical inference is designed to address.</li>
<li>In general, statistical inference consists of making probabilistic statements about unknown quantities.</li>
<li>For example, we can compute means, variances, quantiles, probabilities, and some other quantities yet to be introduced concerning unobserved random variables and unknown parameters of distributions.</li>
<li>Our goal will be to say what we have learned about the unknown quantities after observing some data that we believe contain relevant information.</li>
</ul>
</section>
<section id="statistical-inference-2" class="level2">
<h2 class="anchored" data-anchor-id="statistical-inference-2">Statistical Inference</h2>
<p>Here are some other examples of questions that statistical inference can try to answer.</p>
<ul>
<li>What can we say about whether a machine is functioning properly after we observe some of its output?</li>
<li>In a civil lawsuit, what can we say about whether there was discrimination after observing how different ethnic groups were treated?</li>
</ul>
<p>The methods of statistical inference, which we shall develop to address these questions, are built upon the theory of probability covered in the earlier chapters of this text.</p>
</section>
<section id="definition" class="level2">
<h2 class="anchored" data-anchor-id="definition">Definition</h2>
<p>Statistical Inference. A statistical inference is a procedure that produces a probabilistic statement about some or all parts of a statistical model.</p>
<p>By a “probabilistic statement” we mean a statement that makes use of any of the concepts of probability theory that were discussed earlier in the text or are yet to be discussed later in the text. Some examples include a mean, a conditional mean, a quantile, a variance, a conditional distribution for a random variable given another, the probability of an event, a conditional probability of an event given something, and so on. In Example 7.1.1, here are some examples of statistical inferences that one might wish to make: - Produce a random variable <span class="math inline">\(Y\)</span> (a function of <span class="math inline">\(\left.X_1, \ldots, X_m\right)\)</span> such that <span class="math inline">\(\operatorname{Pr}(Y \geq\)</span> <span class="math inline">\(\theta \mid \theta)=0.9\)</span>. - Produce a random variable <span class="math inline">\(Y\)</span> that we expect to be close to <span class="math inline">\(\theta\)</span>. - Compute how likely it is that the average of the next 10 lifetimes, <span class="math inline">\(\frac{1}{10} \sum_{i=m+1}^{m+10} X_i\)</span>, is at least 2. - Say something about how confident we are that <span class="math inline">\(\theta \leq 0.4\)</span> after observing <span class="math inline">\(X_1, \ldots\)</span>, <span class="math inline">\(X_m\)</span>.</p>
<p>All of these types of inference and others will be discussed in more detail later in this book.</p>
</section>
<section id="statistical-decision-problems" class="level2">
<h2 class="anchored" data-anchor-id="statistical-decision-problems">Statistical Decision Problems</h2>
<p>In many statistical inference problems, after the experimental data have been analyzed, we must choose a decision from some available class of decisions with the property that the consequences of each available decision depend on the unknown value of some parameter. - For example, we might have to estimate the unknown failure rate <span class="math inline">\(\theta\)</span> of our electronic components when the consequences depend on how close our estimate is to the correct value <span class="math inline">\(\theta\)</span>. - As another example, we might have to decide whether the unknown proportion <span class="math inline">\(P\)</span> of patients in the imipramine group (Example 7.1.3) is larger or smaller than some specified constant when the consequences depend on where <span class="math inline">\(P\)</span> lies relative to the constant. - This last type of inference is closely related to hypothesis testing, the subject of Chapter 9.</p>
</section>
<section id="experimental-design" class="level2">
<h2 class="anchored" data-anchor-id="experimental-design">Experimental Design</h2>
<p>In some statistical inference problems, we have some control over the type or the amount of experimental data that will be collected. - For example, consider an experiment to determine the mean tensile strength of a certain type of alloy as a function of the pressure and temperature at which the alloy is produced. - Within the limits of certain budgetary and time constraints, it may be possible for the experimenter to choose the levels of pressure and temperature at which experimental specimens of the alloy are to be produced, and also to specify the number of specimens to be produced at each of these levels. - Such a problem, in which the experimenter can choose (at least to some extent) the particular experiment that is to be carried out, is called a problem of experimental design. - Of course, the design of an experiment and the statistical analysis of the experimental data are closely related. - One cannot design an effective experiment without considering the subsequent statistical analysis that is to be carried out on the data that will be obtained. - And one cannot carry out a meaningful statistical analysis of experimental data without considering the particular type of experiment from which the data were derived.</p>
</section>
<section id="other-inferences" class="level2">
<h2 class="anchored" data-anchor-id="other-inferences">Other Inferences</h2>
<ul>
<li>The general classes of problems described above, as well as the more specific examples that appeared earlier, are intended as illustrations of types of statistical inferences that we will be able to perform with the theory and methods introduced in this text.</li>
<li>The range of possible models, inferences, and methods that can arise when data are observed in real research problems far exceeds what we can introduce here.</li>
<li>It is hoped that gaining an understanding of the problems that we can cover here will give you an appreciation for what needs to be done when a more challenging statistical problem arises.</li>
</ul>
</section>
<section id="statistical-model" class="level2">
<h2 class="anchored" data-anchor-id="statistical-model">Statistical Model</h2>
<p>Statistical Model. A statistical model consists of an identification of random variables of interest (both observable and only hypothetically observable), a specification of a joint distribution or a family of possible joint distributions for the observable random variables, the identification of any parameters of those distributions that are assumed unknown and possibly hypothetically observable, and (if desired) a specification for a (joint) distribution for the unknown parameter(s). When we treat the unknown parameter(s) <span class="math inline">\(\theta\)</span> as random, then the joint distribution of the observable random variables indexed by <span class="math inline">\(\theta\)</span> is understood as the conditional distribution of the observable random variables given <span class="math inline">\(\theta\)</span>.</p>
<p><sup> <strong>Source</strong>: DeGroot and Schervish, Definition 7.1.1 <sup></sup></sup></p>
</section>
<section id="parameterparameter-space" class="level2">
<h2 class="anchored" data-anchor-id="parameterparameter-space">Parameter/Parameter space</h2>
<ul>
<li><p>Parameter/Parameter space. In a problem of statistical inference, a characteristic or combination of characteristics that determine the joint distribution for the random variables of interest is called a parameter of the distribution. The set <span class="math inline">\(\Omega\)</span> of all possible values of a parameter <span class="math inline">\(\theta\)</span> or of a vector of parameters <span class="math inline">\(\left(\theta_1, \ldots, \theta_k\right)\)</span> is called the parameter space.</p></li>
<li><p>All of the families of distributions introduced earlier (and to be introduced later) in this book have parameters that are included in the names of the individual members of the family. For example, the family of binomial distributions has parameters that we called <span class="math inline">\(n\)</span> and <span class="math inline">\(p\)</span>, the family of normal distributions is parameterized by the mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span> of each distribution, the family of uniform distributions on intervals is parameterized by the endpoints of the intervals, the family of exponential distributions is parameterized by the rate parameter <span class="math inline">\(\theta\)</span>, and so on.</p></li>
</ul>
<p><img src="images/2023-10-16-08-23-27.png" class="img-fluid"></p>
<p><sup> <strong>Source</strong>: DeGroot and Schervish, Definition 7.1.3 <sup></sup></sup></p>
</section>
<section id="example-3" class="level2">
<h2 class="anchored" data-anchor-id="example-3">Example</h2>
<p>A Clinical Trial. Suppose that 40 patients are going to be given a treatment for a condition and that we will observe for each patient whether or not they recover from the condition. We are most likely also intersted in a large collection of additional patients besides the 40 to be observed. To be specific, for each patient <span class="math inline">\(i=1,2\)</span>, let <span class="math inline">\(X_i=1\)</span> if patient <span class="math inline">\(i\)</span> recovers, and let <span class="math inline">\(X_i=0\)</span> if not. As a collection of possible distributions for <span class="math inline">\(X_1, X_2, \ldots\)</span>, we could choose to say that the <span class="math inline">\(X_i\)</span> are i.i.d. having the Bernoulli distribution with parameter <span class="math inline">\(p\)</span> for <span class="math inline">\(0 \leq p \leq 1\)</span>. In this case, the parameter <span class="math inline">\(p\)</span> is known to lie in the closed interval <span class="math inline">\([0,1]\)</span>, and this interval could be taken as the parameter space. Notice also that the law of large numbers (Theorem 6.2.4) says that <span class="math inline">\(p\)</span> is the limit as <span class="math inline">\(n\)</span> goes to infinity of the proportion of the first <span class="math inline">\(n\)</span> patients who recover.</p>
</section>
<section id="statistic" class="level2">
<h2 class="anchored" data-anchor-id="statistic">Statistic</h2>
<p>Statistic. Suppose that the observable random variables of interest are <span class="math inline">\(X_1, \ldots, X_n\)</span>. Let <span class="math inline">\(r\)</span> be an arbitrary real-valued function of <span class="math inline">\(n\)</span> real variables. Then the random variable <span class="math inline">\(T=r\left(X_1, \ldots, X_n\right)\)</span> is called a statistic.</p>
<p><sup> <strong>Source</strong>: DeGroot and Schervish, Definition 7.1.4 <sup></sup></sup></p>
</section>
<section id="maximum-likelihood-estimation-mle" class="level2">
<h2 class="anchored" data-anchor-id="maximum-likelihood-estimation-mle">Maximum Likelihood Estimation (MLE)</h2>
</section>
<section id="overview" class="level2">
<h2 class="anchored" data-anchor-id="overview">Overview</h2>
<ul>
<li><strong>Objective:</strong> Estimate parameters that maximize the likelihood of observed data.</li>
<li><strong>Assumption:</strong> Data follows a certain distribution.</li>
<li><strong>Process:</strong> Find parameter values that make the observed data most probable.</li>
<li><strong>Method:</strong> Maximize the likelihood function.</li>
<li><strong>Properties:</strong> Asymptotically efficient and consistent.</li>
<li><strong>Output:</strong> Point estimates for model parameters.</li>
</ul>
</section>
<section id="recall-joint-probability-density-function" class="level2">
<h2 class="anchored" data-anchor-id="recall-joint-probability-density-function">Recall: Joint probability density function</h2>
<ul>
<li><p>Given a random variable <span class="math inline">\(X\)</span> with probability mass / density function <span class="math inline">\(f(x \mid \theta)\)</span>, where <span class="math inline">\(\theta\)</span> is some parameter.</p></li>
<li><p>Distribution of <span class="math inline">\(n\)</span> independent observations <span class="math inline">\(X_1, \ldots, X_n\)</span> : Joint pdf / pmf <span class="math display">\[
f_{\text {joint }}\left(x_1, \ldots, x_n \mid \theta\right)=f\left(x_1 \mid \theta\right) \cdots f\left(x_n \mid \theta\right)
\]</span></p></li>
<li><p>Probability Theory: Assume that <span class="math inline">\(\theta\)</span> is given and the <span class="math inline">\(x_i\)</span> are variables (have not yet been observed).</p></li>
<li><p>Example: Exponential distribution</p></li>
</ul>
</section>
<section id="likelihood-function" class="level2">
<h2 class="anchored" data-anchor-id="likelihood-function">Likelihood Function</h2>
<p>Let the random variables <span class="math inline">\(X_1, \ldots, X_n\)</span> form a random sample from a discrete distribution or a continuous distribution for which the p.f. or the p.d.f. is <span class="math inline">\(f(x \mid \theta)\)</span>, where the parameter <span class="math inline">\(\theta\)</span> belongs to some parameter space <span class="math inline">\(\Omega\)</span>. Here, <span class="math inline">\(\theta\)</span> can be either a real-valued parameter or a vector. For every observed vector <span class="math inline">\(\boldsymbol{x}=\left(x_1, \ldots, x_n\right)\)</span> in the sample, the value of the joint p.f. or joint p.d.f. will, as usual, be denoted by <span class="math inline">\(f_n(\boldsymbol{x} \mid \theta)\)</span>. Because of its importance in this section, we repeat Definition 7.2.3.</p>
<p>Likelihood Function. When the joint p.d.f. or the joint p.f. <span class="math inline">\(f_n(\boldsymbol{x} \mid \theta)\)</span> of the observations in a random sample is regarded as a function of <span class="math inline">\(\theta\)</span> for given values of <span class="math inline">\(x_1, \ldots, x_n\)</span>, it is called the likelihood function.</p>
<p><sup> <strong>Source</strong>: DeGroot and Schervish, Definition 7.5.1 <sup></sup></sup></p>
</section>
<section id="likelihood-function-1" class="level2">
<h2 class="anchored" data-anchor-id="likelihood-function-1">Likelihood function</h2>
<p>Given a random variable <span class="math inline">\(X\)</span> with probability mass / density function <span class="math inline">\(f(x \mid \theta)\)</span>, where <span class="math inline">\(\theta\)</span> is some parameter. Assume a sample of <span class="math inline">\(n\)</span> independent observations <span class="math inline">\(X=x_1, \ldots, X=x_n\)</span> is given. Likelihood function <span class="math display">\[
L\left(\theta \mid x_1, \ldots, x_n\right)=f\left(x_1 \mid \theta\right) \cdots f\left(x_n \mid \theta\right)
\]</span> This is the same as the joint probability density/mass function. Assume now that the <span class="math inline">\(x_i\)</span> are given (have been observed) and <span class="math inline">\(\theta\)</span> is unknown.</p>
</section>
<section id="visualization-1" class="level2">
<h2 class="anchored" data-anchor-id="visualization-1">Visualization</h2>
<p>In statistics, the likelihood function has a very precise definition: <span class="math display">\[
L(\theta \mid x)=P(x \mid \theta)
\]</span> The concept of likelihood plays a fundamental role in both Bayesian and frequentist statistics.</p>
<p><img src="images/2023-10-16-08-29-59.png" class="img-fluid"></p>
<p>Good Interactive Viz:https://seeing-theory.brown.edu/bayesian-inference/index.html#section2</p>
</section>
<section id="mle" class="level2">
<h2 class="anchored" data-anchor-id="mle">MLE</h2>
<p><img src="images/2023-10-16-08-34-52.png" class="img-fluid"></p>
<p><sup> <strong>Source</strong>: <a href="https://www.youtube.com/watch?v=XepXtl9YKwc">https://www.youtube.com/watch?v=XepXtl9YKwc</a> <sup></sup></sup></p>
</section>
<section id="example-poisson-distribution" class="level2">
<h2 class="anchored" data-anchor-id="example-poisson-distribution">Example: Poisson distribution</h2>
<ul>
<li><p>Discrete distribution on <span class="math inline">\(\{0,1,2, \ldots\}\)</span>, parameter <span class="math inline">\(\lambda=\)</span> intensity</p></li>
<li><p>The pmf is <span class="math inline">\(f(x \mid \lambda)=e^{-\lambda} \frac{\lambda^x}{x !}\)</span> for <span class="math inline">\(x=0,1,2, \ldots\)</span></p></li>
<li><p>The joint pmf of <span class="math inline">\(n\)</span> independent observations is <span class="math display">\[
\begin{aligned}
f_{\text {joint }}\left(x_1, \ldots, x_n \mid \lambda\right) &amp; =e^{-n \lambda} \frac{\lambda^{x_1}}{x_{1} !} \ldots \frac{\lambda^{x_n}}{x_{n} !} \\
&amp; =e^{-n \lambda} \frac{\lambda^{x_1+x_2+\cdots+x_n}}{x_{1} ! x_{2} ! \ldots x_{n} !} \\
&amp; =L\left(\lambda \mid x_1, \ldots, x_n\right)
\end{aligned}
\]</span></p></li>
<li><p>This is also the likelihood function.</p></li>
<li><p>Three parts: <span class="math inline">\(e^{-n \lambda}\)</span> depends only on <span class="math inline">\(\lambda\)</span>, the <span class="math inline">\(x_i\)</span> ! depend only on the data, the term <span class="math inline">\(\lambda^{x_1+x_2+\cdots+x_n}\)</span> depends both on <span class="math inline">\(\lambda\)</span> and the data.</p></li>
</ul>
</section>
<section id="example-exponential-distribution" class="level2">
<h2 class="anchored" data-anchor-id="example-exponential-distribution">Example: Exponential distribution</h2>
<ul>
<li>Continuous distribution on <span class="math inline">\([0, \infty)\)</span>, parameter <span class="math inline">\(\lambda=\)</span> rate</li>
<li>The pdf is <span class="math inline">\(f(x \mid \lambda)=\lambda e^{-\lambda x}\)</span> for <span class="math inline">\(x \geq 0\)</span></li>
<li>The joint pdf of <span class="math inline">\(n\)</span> independent observations is</li>
</ul>
<p><span class="math display">\[
\begin{aligned}
f_{\text {joint }}\left(x_1, \ldots, x_n \mid \lambda\right) &amp; =\lambda^n e^{-\lambda x_1} \ldots \lambda e^{-\lambda x_n} \\
&amp; =\lambda^n e^{-\lambda x_1-\lambda x_2-\ldots \lambda x_n} \\
&amp; =\lambda^n e^{-\lambda \sum_i x_i} \\
&amp; =L\left(\lambda \mid x_1, \ldots, x_n\right)
\end{aligned}
\]</span></p>
<ul>
<li>This is also the likelihood function.</li>
<li>Two parts: A term depending only on <span class="math inline">\(\lambda\)</span> and another term depending on <span class="math inline">\(\lambda\)</span> and the data, specifically <span class="math inline">\(\sum_i x_i\)</span>.</li>
</ul>
</section>
<section id="calculation" class="level2">
<h2 class="anchored" data-anchor-id="calculation">Calculation</h2>
<p><img src="images/2023-10-16-08-48-35.png" class="img-fluid"></p>
</section>
<section id="example-bernoulli-distribution" class="level2">
<h2 class="anchored" data-anchor-id="example-bernoulli-distribution">Example: Bernoulli distribution</h2>
<p>Discrete distribution on <span class="math inline">\(\{0,1\}\)</span>, parameter <span class="math inline">\(p=\)</span> success probability Pmf: <span class="math inline">\(f(x \mid p)=p^x(1-p)^{1-x}=\left\{\begin{array}{ll}1-p &amp; (x=0) \\ p &amp; (x=1)\end{array} \quad\right.\)</span> for <span class="math inline">\(x=0,1\)</span> The joint pmf of <span class="math inline">\(n\)</span> independent observations is <span class="math display">\[
\begin{aligned}
f_{\text {joint }}\left(x_1, \ldots, x_n \mid p\right) &amp; =\prod_{i=1}^n p^{x_i}(1-p)^{1-x_i} \\
&amp; =p^{\sum_{i=1}^n x_i}(1-p)^{\sum_{i=1}^n\left(1-x_i\right)} \\
&amp; =p^{\sum_i x_i}(1-p)^{n-\sum_i x_i} \\
&amp; =(1-p)^n\left(\frac{p}{1-p}\right)^{\sum_i x_i}=L\left(p \mid x_1, \ldots, x_n\right)
\end{aligned}
\]</span> This is also the likelihood function.</p>
</section>
<section id="exponential-families" class="level2">
<h2 class="anchored" data-anchor-id="exponential-families">Exponential Families</h2>
<p>In all these cases, the likelihood function for a sample <span class="math inline">\(\mathbf{x}=\left(x_1, \ldots, x_n\right)\)</span> has two or three parts:</p>
<ul>
<li>a function <span class="math inline">\(g(\mathbf{x})\)</span> that depends on the sample (could be 1)</li>
<li>a function <span class="math inline">\(h(\theta)\)</span> that depends on <span class="math inline">\(\theta\)</span> but not on the sample</li>
<li>a term that depends on the sample and on <span class="math inline">\(\theta\)</span> that has a special exponential form.</li>
</ul>
<p>These are examples of exponential families of distributions (well studied in statistics).</p>
</section>
<section id="example-cauchy-distribution" class="level2">
<h2 class="anchored" data-anchor-id="example-cauchy-distribution">Example: Cauchy Distribution</h2>
<p>Continuous distribution on <span class="math inline">\((-\infty, \infty)\)</span>, location parameter <span class="math inline">\(\theta\)</span> The pdf is <span class="math inline">\(f(x \mid \theta)=\frac{1}{\pi\left((x-\theta)^2+1\right)}\)</span> The joint pdf of <span class="math inline">\(n\)</span> independent observations is <span class="math display">\[
f_{\text {joint }}\left(x_1, \ldots, x_n \mid \theta\right)=\frac{1}{\pi^n} \prod_{i=1}^n \frac{1}{\left.\left(x_i-\theta\right)^2+1\right)}
\]</span> This is also the likelihood function. Not clear how to simplify this! This is not an exponential family.</p>
</section>
<section id="likelihood-function-2" class="level2">
<h2 class="anchored" data-anchor-id="likelihood-function-2">Likelihood function</h2>
<p>The part of the likelihood function that connects the data and the parameter often depends only on a statistic <span class="math inline">\(T(\mathbf{x})\)</span>. Exponential distribution: <span class="math display">\[
L\left(\lambda \mid x_1, \ldots, x_n\right)=\lambda^n e^{-\lambda x_1-\lambda x_2 \cdots-\lambda x_n}
\]</span> depends only on <span class="math inline">\(T(\mathbf{x})=x_1+\cdots+x_n=n \bar{x}\)</span>. Poisson distribution: <span class="math display">\[
L\left(\lambda \mid x_1, \ldots, x_n\right)=e^{-n \lambda} \frac{\lambda^{x_1+x_2+\cdots+x_n}}{x_{1} ! x_{2} ! \ldots x_{n} !}
\]</span> The term connecting the data and <span class="math inline">\(\lambda\)</span> depends only on <span class="math inline">\(T(\mathbf{x})=x_1+\cdots+x_n\)</span>.</p>
</section>
<section id="likelihood-function-for-normal-distribution" class="level2">
<h2 class="anchored" data-anchor-id="likelihood-function-for-normal-distribution">Likelihood function for normal distribution</h2>
<p>Two parameters: Mean <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\sigma\)</span>. Probability density: <span class="math display">\[
f(x \mid \mu, \sigma)=\frac{1}{\sqrt{2 \pi} \sigma} e^{-\frac{(x-\mu)^2}{2 \sigma^2}}
\]</span> Likelihood function: <span class="math display">\[
\begin{aligned}
L\left(\mu, \sigma \mid x_1, \ldots, x_n\right) &amp; =\frac{1}{(\sqrt{2 \pi})^n \sigma^n} \prod_{i=1}^n e^{-\frac{\left(x_i-\mu\right)^2}{2 \sigma^2}} \\
&amp; =\frac{1}{(\sqrt{2 \pi})^n \sigma^n} e^{-\sum_{i=1}^n \frac{\left(x_i-\mu\right)^2}{2 \sigma^2}}
\end{aligned}
\]</span> The data and the parameters <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> are connected through <span class="math inline">\(T_1(\mathbf{x})=\sum_i x_i\)</span> and <span class="math inline">\(T_2(\mathbf{x})=\sum_i x_i^2\)</span>.</p>
</section>
<section id="log-likelihood" class="level2">
<h2 class="anchored" data-anchor-id="log-likelihood">Log Likelihood</h2>
<ul>
<li>Take the logarithm of the likelihood function.</li>
</ul>
<p>Poisson distribution <span class="math display">\[
\log L=-n \lambda+\left(\sum_i x_i\right) \log \lambda-\sum_i \log x_{i} !
\]</span> Exponential distribution <span class="math display">\[
\log L=n \log \lambda-\lambda\left(\sum_i x_i\right)
\]</span> Bernoulli distribution <span class="math display">\[
\log L=\log p\left(\sum_i x_i\right)+\log (1-p)\left(n-\sum_i x_i\right)
\]</span></p>
</section>
<section id="calculation-1" class="level2">
<h2 class="anchored" data-anchor-id="calculation-1">Calculation</h2>
<p><img src="images/2023-10-16-08-51-41.png" class="img-fluid"></p>
</section>
<section id="maximum-likelihood" class="level2">
<h2 class="anchored" data-anchor-id="maximum-likelihood">Maximum likelihood</h2>
<p>We need to find a value of <span class="math inline">\(\theta\)</span> for which the probability density <span class="math inline">\(f_n(\boldsymbol{x} \mid \theta)\)</span> is large and to use this value as an estimate of <span class="math inline">\(\boldsymbol{\theta}\)</span>. For each possible observed vector <span class="math inline">\(x\)</span>, we are led by this reasoning to consider a value of <span class="math inline">\(\theta\)</span> for which the likelihood function <span class="math inline">\(f_n(\boldsymbol{x} \mid \theta)\)</span> is a maximum and to use this value as an estimate of <span class="math inline">\(\theta\)</span>. This concept is formalized in the following definition.</p>
</section>
<section id="maximum-likelihood-estimate" class="level2">
<h2 class="anchored" data-anchor-id="maximum-likelihood-estimate">Maximum Likelihood Estimate</h2>
<ul>
<li>Maximum Likelihood Estimator/Estimate. For each possible observed vector <span class="math inline">\(\boldsymbol{x}\)</span>, let <span class="math inline">\(\delta(\boldsymbol{x}) \in \Omega\)</span> denote a value of <span class="math inline">\(\theta \in \Omega\)</span> for which the likelihood function <span class="math inline">\(f_n(\boldsymbol{x} \mid \theta)\)</span> is a maximum, and let <span class="math inline">\(\hat{\theta}=\delta(\boldsymbol{X})\)</span> be the estimator of <span class="math inline">\(\theta\)</span> defined in this way.</li>
<li>The estimator <span class="math inline">\(\hat{\theta}\)</span> is called a maximum likelihood estimator of <span class="math inline">\(\theta\)</span>. After <span class="math inline">\(\boldsymbol{X}=\boldsymbol{x}\)</span> is observed, the value <span class="math inline">\(\delta(\boldsymbol{x})\)</span> is called a maximum likelihood estimate of <span class="math inline">\(\theta\)</span>.</li>
</ul>
<p><sup> <strong>Source</strong>: DeGroot and Schervish, Definition 7.5.2 <sup></sup></sup></p>
</section>
<section id="arg-max-function" class="level2">
<h2 class="anchored" data-anchor-id="arg-max-function">Arg-max function</h2>
<ul>
<li>The argmax function identifies the argument that maximizes a given function.</li>
<li>In mathematical terms, it finds the input value that yields the maximum output of the function.</li>
<li>It is crucial for optimization problems and statistical estimation.</li>
</ul>
<p>In mathematics, the arguments of the maxima (abbreviated arg max or argmax) are the points, or elements, of the domain of some function at which the function values are maximized.[note 1] In contrast to global maxima, which refers to the largest outputs of a function, arg max refers to the inputs, or arguments, at which the function outputs are as large as possible.</p>
<p><img src="images/2023-10-16-08-55-26.png" class="img-fluid"></p>
</section>
<section id="maximum-likelihood-1" class="level2">
<h2 class="anchored" data-anchor-id="maximum-likelihood-1">Maximum Likelihood</h2>
<p>*Observe the graphs of the likelihood functions. Where are the maxima? Maximum Likelihood Estimation Estimate the unknown parameter <span class="math inline">\(\theta\)</span> by using the maximum of the likelihood function, <span class="math display">\[
\hat{\theta}_{M L E}=\operatorname{argmax}_\theta L\left(\theta \mid x_1, \ldots, x_n\right)
\]</span> Equivalently we can try to maximize the log likelihood. Use Optimization Theory to work out the maximum or to compute it numerically.</p>
</section>
<section id="example-exponential-distribution-1" class="level2">
<h2 class="anchored" data-anchor-id="example-exponential-distribution-1">Example: Exponential Distribution</h2>
<p>Given a sample of size <span class="math inline">\(n\)</span> and <span class="math inline">\(\sum_i x_i\)</span>, the log likelihood is <span class="math display">\[
\log L(\lambda)=n \log \lambda-\lambda \cdot \sum_i x_i=n \log \lambda-\lambda n \bar{x}
\]</span> Differentiate wrt. <span class="math inline">\(\lambda\)</span> : <span class="math display">\[
\frac{d}{d \lambda} \log L(\lambda)=\frac{n}{\lambda}-n \bar{x}
\]</span> Set this <span class="math inline">\(=0\)</span> and solve for <span class="math inline">\(\lambda\)</span>. Calculus shows that this is the maximum, the maximum likelihood estimate of <span class="math inline">\(\lambda\)</span>. <span class="math display">\[
\hat{\lambda}_{M L E}=\frac{1}{\bar{x}}
\]</span></p>
</section>
<section id="examples-of-mles" class="level2">
<h2 class="anchored" data-anchor-id="examples-of-mles">Examples of MLEs</h2>
<ul>
<li>Poisson distribution: <span class="math inline">\(\hat{\lambda}_{M L E}=\bar{x}\)</span></li>
<li>Exponential distribution: <span class="math inline">\(\hat{\lambda}_{M L E}=\frac{1}{\bar{x}}\)</span></li>
<li>Bernoulli distribution: <span class="math inline">\(\hat{p}_{M L E}=\bar{x}\)</span></li>
<li>Theoretical justification of intuitive choices</li>
<li>Shows how to reduce data</li>
<li>General method</li>
</ul>
</section>
<section id="normal-distribution" class="level2">
<h2 class="anchored" data-anchor-id="normal-distribution">Normal Distribution</h2>
<p>Consider normal distribution <span class="math inline">\(N\left(\mu, \sigma^2\right)\)</span>. The likelihood function depends on two parameters, namely <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span>. <span class="math display">\[
\begin{aligned}
L\left(\mu, \sigma \mid x_1, \ldots, x_n\right) &amp; =\frac{1}{(\sqrt{2 \pi})^n \sigma^n} \prod_{i=1}^n e^{-\frac{\left(x_i-\mu\right)^2}{2 \sigma^2}} \\
&amp; =\frac{1}{(\sqrt{2 \pi})^n \sigma^n} e^{-\sum_{i=1}^n \frac{\left(x_i-\mu\right)^2}{2 \sigma^2}}
\end{aligned}
\]</span> Need calculus of several variables to minimize. Maximum likelihood estimates: <span class="math display">\[
\hat{\mu}_{M L E}=\bar{x}, \quad \hat{\sigma}^2 M L E=\frac{1}{n} \sum_{i=1}^n\left(x_i-\bar{x}\right)^2
\]</span></p>
</section>
<section id="uniform-distribution" class="level2">
<h2 class="anchored" data-anchor-id="uniform-distribution">Uniform Distribution</h2>
<p>Consider uniform distribution on <span class="math inline">\((0, a)\)</span> where <span class="math inline">\(a\)</span> is unknown. Likelihood function depends on <span class="math inline">\(a\)</span>. <span class="math display">\[
L\left(a \mid x_1, \ldots, x_n\right)= \begin{cases}\frac{1}{a^n} &amp; \left(0 \leq x_1, x_2, \ldots, x_n \leq a\right) \\ 0 &amp; \text { otherwise }\end{cases}
\]</span> Given a sample, we should pick the smallest a such that the first condition is true, since this will maximize the likelihood. Maximum likelihood estimates: <span class="math display">\[
\hat{a}_{M L E}=\max _i x_i
\]</span> Does this make sense? This is always biased - why?</p>
</section>
<section id="example-logistic-regression" class="level2">
<h2 class="anchored" data-anchor-id="example-logistic-regression">Example: Logistic Regression</h2>
<p>12.2.1 Likelihood Function for Logistic Regression Because logistic regression predicts probabilities, rather than just classes, we can fit it using likelihood. For each training data-point, we have a vector of features, <span class="math inline">\(x_i\)</span>, and an observed class, <span class="math inline">\(y_i\)</span>. The probability of that class was either <span class="math inline">\(p\)</span>, if <span class="math inline">\(y_i=1\)</span>, or <span class="math inline">\(1-p\)</span>, if <span class="math inline">\(y_i=0\)</span>. The likelihood is then <span class="math display">\[
L\left(\beta_0, \beta\right)=\prod_{i=1}^n p\left(x_i\right)^{y_i}\left(1-p\left(x_i\right)^{1-y_i}\right.
\]</span></p>
<p>Reference: http://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch12.pdf</p>
</section>
<section id="example-logistic-regression-1" class="level2">
<h2 class="anchored" data-anchor-id="example-logistic-regression-1">Example: Logistic Regression</h2>
<p>(I could substitute in the actual equation for <span class="math inline">\(p\)</span>, but things will be clearer in a moment if I don’t.) The log-likelihood turns products into sums: <span class="math display">\[
\begin{aligned}
\ell\left(\beta_0, \beta\right) &amp; =\sum_{i=1}^n y_i \log p\left(x_i\right)+\left(1-y_i\right) \log 1-p\left(x_i\right) \\
&amp; =\sum_{i=1}^n \log 1-p\left(x_i\right)+\sum_{i=1}^n y_i \log \frac{p\left(x_i\right)}{1-p\left(x_i\right)} \\
&amp; =\sum_{i=1}^n \log 1-p\left(x_i\right)+\sum_{i=1}^n y_i\left(\beta_0+x_i \cdot \beta\right) \\
&amp; =\sum_{i=1}^n-\log 1+e^{\beta_0+x_i \cdot \beta}+\sum_{i=1}^n y_i\left(\beta_0+x_i \cdot \beta\right)
\end{aligned}
\]</span> where in the next-to-last step we finally use equation 12.4. Reference: http://www.stat.cmu.edu/ cshalizi/uADA/12/lectures/ch12.pdf</p>
</section>
<section id="example-logistic-regression-2" class="level2">
<h2 class="anchored" data-anchor-id="example-logistic-regression-2">Example: Logistic Regression</h2>
<p>Typically, to find the maximum likelihood estimates we’d differentiate the log likelihood with respect to the parameters, set the derivatives equal to zero, and solve. To start that, take the derivative with respect to one component of <span class="math inline">\(\beta\)</span>, say <span class="math inline">\(\beta_j\)</span>. <span class="math display">\[
\begin{aligned}
\frac{\partial \ell}{\partial \beta_j} &amp; =-\sum_{i=1}^n \frac{1}{1+e^{\beta_0+x_i \cdot \beta}} e^{\beta_0+x_i \cdot \beta} x_{i j}+\sum_{i=1}^n y_i x_{i j} \\
&amp; =\sum_{i=1}^n\left(y_i-p\left(x_i ; \beta_0, \beta\right)\right) x_{i j}
\end{aligned}
\]</span> We are not going to be able to set this to zero and solve exactly. (That’s a transcendental equation, and there is no closed-form solution.) We can however approximately solve it numerically.</p>
</section>
<section id="notes" class="level2">
<h2 class="anchored" data-anchor-id="notes">Notes</h2>
<p>It should be noted that in some problems, for certain observed vectors <span class="math inline">\(\boldsymbol{x}\)</span>, the maximum value of <span class="math inline">\(f_n(\boldsymbol{x} \mid \theta)\)</span> may not actually be attained for any point <span class="math inline">\(\theta \in \Omega\)</span>. In such a case, an M.L.E. of <span class="math inline">\(\theta\)</span> does not exist. For certain other observed vectors <span class="math inline">\(\boldsymbol{x}\)</span>, the maximum value of <span class="math inline">\(f_n(\boldsymbol{x} \mid \theta)\)</span> may actually be attained at more than one point in the space <span class="math inline">\(\Omega\)</span>. In such a case, the M.L.E. is not uniquely defined, and any one of these points can be chosen as the value of the estimator <span class="math inline">\(\hat{\theta}\)</span>. In many practical problems, however, the M.L.E. exists and is uniquely defined.</p>
<p># Method of Moments (MOM)</p>
</section>
<section id="overview-1" class="level2">
<h2 class="anchored" data-anchor-id="overview-1">Overview</h2>
<ul>
<li>The method of moments is an intuitive method for estimating parameters when other, more attractive, methods may be too difficult</li>
</ul>
<p>Method of Moments. Assume that <span class="math inline">\(X_1, \ldots, X_n\)</span> form a random sample from a distribution that is indexed by a <span class="math inline">\(k\)</span>-dimensional parameter <span class="math inline">\(\theta\)</span> and that has at least <span class="math inline">\(k\)</span> finite moments. For <span class="math inline">\(j=1, \ldots, k\)</span>, let <span class="math inline">\(\mu_j(\theta)=E\left(X_1^j \mid \theta\right)\)</span>. Suppose that the function <span class="math inline">\(\mu(\theta)=\left(\mu_1(\theta), \ldots, \mu_k(\theta)\right)\)</span> is a one-to-one function of <span class="math inline">\(\theta\)</span>. Let <span class="math inline">\(M\left(\mu_1, \ldots, \mu_k\right)\)</span> denote the inverse function, that is, for all <span class="math inline">\(\theta\)</span>, <span class="math display">\[
\theta=M\left(\mu_1(\theta), \ldots, \mu_k(\theta)\right) \text {. }
\]</span> Define the sample moments by <span class="math inline">\(m_j=\frac{1}{n} \sum_{i=1}^n X_i^j\)</span> for <span class="math inline">\(j=1, \ldots, k\)</span>. The method of moments estimator of <span class="math inline">\(\theta\)</span> is <span class="math inline">\(M\left(m_1, \ldots, m_j\right)\)</span>.</p>
<p>The usual way of implementing the method of moments is to set up the <span class="math inline">\(k\)</span> equations <span class="math inline">\(m_j=\mu_j(\theta)\)</span> and then solve for <span class="math inline">\(\theta\)</span>.</p>
<p><sup> <strong>Source</strong>: DeGroot and Schervish, Definition 7.6.3 <sup></sup></sup></p>
</section>
<section id="method-of-moments-estimation" class="level2">
<h2 class="anchored" data-anchor-id="method-of-moments-estimation">Method of Moments Estimation</h2>
<p>Given a random variable <span class="math inline">\(X\)</span> whose distribution depends on a parameter <span class="math inline">\(\theta\)</span>. To estimate <span class="math inline">\(\theta\)</span>,</p>
<ul>
<li>Express a moment <span class="math inline">\(\mathcal{E}(X)\)</span> or <span class="math inline">\(\mathcal{E}\left(X^2\right)\)</span> or … in terms of <span class="math inline">\(\theta\)</span>, e.g.&nbsp;<span class="math inline">\(\mathcal{E}(X)=H(\theta)\)</span></li>
<li>Estimate this moment from the sample</li>
<li>Solve the equation relating the moment and the parameter, e.g.&nbsp;solve <span class="math inline">\(\bar{x}=H(\hat{\theta})\)</span> for <span class="math inline">\(\hat{\theta}\)</span>.</li>
</ul>
<p>Similar to a plug-in estimation</p>
<p>Avoids calculus or likelihood functions, only algebra is needed</p>
</section>
<section id="example-uniform-distribution" class="level2">
<h2 class="anchored" data-anchor-id="example-uniform-distribution">Example: Uniform Distribution</h2>
<p>Consider uniform distribution on <span class="math inline">\((0, a)\)</span> where <span class="math inline">\(a\)</span> is unknown. Then <span class="math inline">\(E(X)=\frac{a}{2}\)</span>. Given a sample, compute the sample mean. Then use the method of moments: <span class="math display">\[
E(X)=\frac{a}{2} \text { becomes } \quad \bar{x}=\frac{\hat{a}}{2} \Rightarrow \hat{a}=2 \bar{x}
\]</span> Method of moments estimate: <span class="math display">\[
\hat{a}_{M o M}=2 \bar{x}
\]</span> This is not biased. Does this make sense?</p>
</section>
<section id="example-beta-distribution" class="level2">
<h2 class="anchored" data-anchor-id="example-beta-distribution">Example: Beta Distribution</h2>
<p>Continuous distribution on <span class="math inline">\((0,1)\)</span>, parameters <span class="math inline">\(\alpha, \beta&gt;0\)</span> The pdf is <span class="math display">\[
f(x \mid \alpha, \beta)=\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha) \Gamma(\beta)} x^{\alpha-1}(1-x)^{\beta-1}
\]</span> for <span class="math inline">\(0&lt;x&lt;1\)</span> Likelihood function is complicated. Calculus minimization is challenging, due to <span class="math inline">\(\Gamma\)</span> function.</p>
</section>
<section id="estimation-using-method-of-moments" class="level2">
<h2 class="anchored" data-anchor-id="estimation-using-method-of-moments">Estimation using Method of Moments</h2>
<p>Known for the beta distribution: <span class="math display">\[
\mathcal{E}(X)=\frac{\alpha}{\alpha+\beta}, \quad \operatorname{var}(X)=\frac{\alpha \beta}{(\alpha+\beta)^2(\alpha+\beta+1)}
\]</span> This is equivalent to formulae for the moments <span class="math inline">\(\mathcal{E}(X)\)</span> and <span class="math inline">\(\mathcal{E}\left(X^2\right)\)</span>, since <span class="math inline">\(\mathcal{E}\left(X^2\right)=\operatorname{var}(X)+\mathcal{E}(X)^2\)</span>.</p>
<p>MoM approach: Use sample mean <span class="math inline">\(\bar{x}\)</span> and sample variance <span class="math inline">\(\bar{v}\)</span>. Solve the equations <span class="math display">\[
\bar{x}=\frac{\alpha}{\alpha+\beta}, \quad \bar{v}=\frac{\alpha \beta}{(\alpha+\beta)^2(\alpha+\beta+1)}
\]</span></p>
</section>
<section id="resulting-estimators" class="level2">
<h2 class="anchored" data-anchor-id="resulting-estimators">Resulting Estimators</h2>
<ul>
<li><p>After some algebra … <span class="math display">\[
\begin{aligned}
&amp; \hat{\alpha}=\bar{x}\left(\frac{\bar{x}(1-\bar{x})}{\bar{v}}-1\right) \\
&amp; \hat{\beta}=(1-\bar{x})\left(\frac{\bar{x}(1-\bar{x})}{\bar{v}}-1\right)
\end{aligned}
\]</span></p></li>
<li><p>What if <span class="math inline">\(\bar{v}&gt;\bar{x}(1-\bar{x})\)</span> ? The estimates then are negative! R packages such as EnvStats uses a numerical method to maximize the likelihood.</p></li>
</ul>
</section>
<section id="bias-and-variance" class="level2">
<h2 class="anchored" data-anchor-id="bias-and-variance">Bias and Variance</h2>
<p><img src="images/2023-10-16-09-05-02.png" class="img-fluid"></p>
</section>
<section id="bias-and-variance-1" class="level2">
<h2 class="anchored" data-anchor-id="bias-and-variance-1">Bias and Variance</h2>
<p>Bias is systematic error, variance is random error. Bias can sometimes be estimated and corrected, variance can only be estimated. Formal Definition Suppose <span class="math inline">\(\hat{\theta}\)</span> is an estimator (based on a random sample) for <span class="math inline">\(\theta\)</span>. The bias is defined as <span class="math display">\[
\operatorname{bias}(\hat{\theta})=\mathcal{E}(\hat{\theta})-\theta
\]</span> Theoretical evaluation and simulation approach may both be possible.</p>
</section>
<section id="unbiased-estimator" class="level2">
<h2 class="anchored" data-anchor-id="unbiased-estimator">Unbiased Estimator</h2>
<p>Let <span class="math inline">\(\delta\)</span> be an estimator of a function <span class="math inline">\(g\)</span> of a parameter <span class="math inline">\(\theta\)</span>. We say that <span class="math inline">\(\delta\)</span> is unbiased if <span class="math inline">\(E_\theta[\delta(\boldsymbol{X})]=g(\theta)\)</span> for all values of <span class="math inline">\(\theta\)</span>. This section provides several examples of unbiased estimators.</p>
<p>Let <span class="math inline">\(\boldsymbol{X}=\left(X_1, \ldots, X_n\right)\)</span> be a random sample from a distribution that involves a parameter (or parameter vector) <span class="math inline">\(\theta\)</span> whose value is unknown. Suppose that we wish to estimate a function <span class="math inline">\(g(\theta)\)</span> of the parameter. In a problem of this type, it is desirable to use an estimator <span class="math inline">\(\delta(\boldsymbol{X})\)</span> that, with high probability, will be close to <span class="math inline">\(g(\theta)\)</span>. In other words,</p>
</section>
<section id="unbiased-estimator-1" class="level2">
<h2 class="anchored" data-anchor-id="unbiased-estimator-1">Unbiased Estimator</h2>
<p>it is desirable to use an estimator <span class="math inline">\(\delta\)</span> whose distribution changes with the value of <span class="math inline">\(\theta\)</span> in such a way that no matter what the true value of <span class="math inline">\(\theta\)</span> is, the probability distribution of <span class="math inline">\(\delta\)</span> is concentrated around <span class="math inline">\(g(\theta)\)</span>. For example, suppose that <span class="math inline">\(\boldsymbol{X}=\left(X_1, \ldots, X_n\right)\)</span> form a random sample from a normal distribution for which the mean <span class="math inline">\(\theta\)</span> is unknown and the variance is 1 . In this case, the M.L.E. of <span class="math inline">\(\theta\)</span> is the sample mean <span class="math inline">\(\bar{X}_n\)</span>. The estimator <span class="math inline">\(\bar{X}_n\)</span> is a reasonably good estimator of <span class="math inline">\(\theta\)</span> because its distribution is the normal distribution with mean <span class="math inline">\(\theta\)</span> and variance <span class="math inline">\(1 / n\)</span>. This distribution is concentrated around the unknown value of <span class="math inline">\(\theta\)</span>, no matter how large or how small <span class="math inline">\(\theta\)</span> is.</p>
</section>
<section id="unbiased-estimator-2" class="level2">
<h2 class="anchored" data-anchor-id="unbiased-estimator-2">Unbiased Estimator</h2>
<p>Unbiased Estimator/Bias. An estimator <span class="math inline">\(\delta(\boldsymbol{X})\)</span> is an unbiased estimator of a function <span class="math inline">\(g(\theta)\)</span> of the parameter <span class="math inline">\(\theta\)</span> if <span class="math inline">\(E_\theta[\delta(\boldsymbol{X})]=g(\theta)\)</span> for every possible value of <span class="math inline">\(\theta\)</span>. An estimator that is not unbiased is called a biased estimator. The difference between the expectation of an estimator and <span class="math inline">\(g(\theta)\)</span> is called the bias of the estimator. That is, the bias of <span class="math inline">\(\delta\)</span> as an estimator of <span class="math inline">\(g(\theta)\)</span> is <span class="math inline">\(E_\theta[\delta(\boldsymbol{X})]-g(\theta)\)</span>, and <span class="math inline">\(\delta\)</span> is unbiased if and only if the bias is 0 for all <span class="math inline">\(\theta\)</span>.</p>
<p>In the case of a sample from a normal distribution with unknown mean <span class="math inline">\(\theta, \bar{X}_n\)</span> is an unbiased estimator of <span class="math inline">\(\theta\)</span> because <span class="math inline">\(E_\theta\left(\bar{X}_n\right)=\theta\)</span> for <span class="math inline">\(-\infty&lt;\theta&lt;\infty\)</span></p>
<p><sup> <strong>Source</strong>: DeGroot and Schervish, Definition 8.7.1 <sup></sup></sup></p>
</section>
<section id="example-poisson-distribution-1" class="level2">
<h2 class="anchored" data-anchor-id="example-poisson-distribution-1">Example: Poisson Distribution</h2>
<ul>
<li><p>The maximum likelihood estimator for <span class="math inline">\(\lambda\)</span> is the sample mean, <span class="math inline">\(\hat{\lambda}=\bar{X}\)</span>. We know that <span class="math display">\[
\mathcal{E}\left(X_i\right)=\lambda \Longrightarrow \mathcal{E}(\bar{X})=\lambda .
\]</span></p></li>
<li><p>Therefore,</p></li>
</ul>
<p><span class="math display">\[
\mathcal{E}(\hat{\lambda})-\lambda=0
\]</span></p>
<ul>
<li>This estimator is unbiased.</li>
</ul>
</section>
<section id="exponential-distribution" class="level2">
<h2 class="anchored" data-anchor-id="exponential-distribution">Exponential Distribution</h2>
<p>The maximum likelihood estimator for <span class="math inline">\(\lambda\)</span> is <span class="math inline">\(\hat{\lambda}=\frac{1}{\bar{\chi}}\)</span>. We know that <span class="math display">\[
\mathcal{E}\left(X_i\right)=\frac{1}{\lambda} \Longrightarrow \mathcal{E}(\bar{X})=\frac{1}{\lambda} .
\]</span> But one can show that <span class="math display">\[
\mathcal{E}(\hat{\lambda})=\mathcal{E}\left(\frac{1}{\bar{X}}\right)=\frac{n}{n-1} \lambda .
\]</span> Can also assess and correct the bias with a simulation (“parametric bootstrap”).</p>
</section>
<section id="calculation-1-1" class="level2">
<h2 class="anchored" data-anchor-id="calculation-1-1">Calculation-1</h2>
<p><img src="images/2023-10-16-09-08-10.png" class="img-fluid"></p>
</section>
<section id="calculation-2" class="level2">
<h2 class="anchored" data-anchor-id="calculation-2">Calculation-2</h2>
<p><img src="images/2023-10-16-09-08-28.png" class="img-fluid"></p>
</section>
<section id="unbiased-estimation-of-the-variance" class="level2">
<h2 class="anchored" data-anchor-id="unbiased-estimation-of-the-variance">Unbiased Estimation of the Variance</h2>
<p>Sampling from a General Distribution. Let <span class="math inline">\(\boldsymbol{X}=\left(X_1, \ldots, X_n\right)\)</span> be a random sample from a distribution that depends on a parameter (or parameter vector) <span class="math inline">\(\theta\)</span>. Assume that the variance of the distribution is finite. Define <span class="math inline">\(g(\theta)=\operatorname{Var}_\theta\left(X_1\right)\)</span>. The following statistic is an unbiased estimator of the variance <span class="math inline">\(g(\theta)\)</span> :</p>
<p><span class="math display">\[
\hat{\sigma}_1^2=\frac{1}{n-1} \sum_{i=1}^n\left(X_i-\bar{X}_n\right)^2
\]</span></p>
<p><sup> <strong>Source</strong>: DeGroot and Schervish, Theorem 8.7.1 <sup></sup></sup></p>
</section>
<section id="example-4" class="level2">
<h2 class="anchored" data-anchor-id="example-4">Example</h2>
<p>Sampling from a Specific Family of Distributions When it can be assumed that <span class="math inline">\(X_1, \ldots, X_n\)</span> form a random sample from a specific family of distributions, such as the family of Poisson distributions, it will generally be desirable to consider not only <span class="math inline">\(\hat{\sigma}_1^2\)</span> but also other unbiased estimators of the variance.</p>
<p>Sample from a Poisson Distribution. Suppose that we observe a random sample from the Poisson distribution for which the mean <span class="math inline">\(\theta\)</span> is unknown. We have already seen that <span class="math inline">\(\bar{X}_n\)</span> will be an unbiased estimator of the mean <span class="math inline">\(\theta\)</span>. Moreover, since the variance of a Poisson distribution is also equal to <span class="math inline">\(\theta\)</span>, it follows that <span class="math inline">\(\bar{X}_n\)</span> is also an unbiased estimator of the variance. In this example, therefore, both <span class="math inline">\(\bar{X}_n\)</span> and <span class="math inline">\(\hat{\sigma}_1^2\)</span> are unbiased estimators of the unknown variance <span class="math inline">\(\theta\)</span>. Furthermore, any combination of <span class="math inline">\(\bar{X}_n\)</span> and <span class="math inline">\(\hat{\sigma}_1^2\)</span> having the form <span class="math inline">\(\alpha \bar{X}_n+(1-\alpha) \hat{\sigma}_1^2\)</span>, where <span class="math inline">\(\alpha\)</span> is a given constant <span class="math inline">\((-\infty&lt;\alpha&lt;\infty)\)</span>, will also be an unbiased estimator of <span class="math inline">\(\theta\)</span> because its expectation will be <span class="math display">\[
E\left[\alpha \bar{X}_n+(1-\alpha) \hat{\sigma}_1^2\right]=\alpha E\left(\bar{X}_n\right)+(1-\alpha) E\left(\hat{\sigma}_1^2\right)=\alpha \theta+(1-\alpha) \theta=\theta
\]</span> Other unbiased estimators of <span class="math inline">\(\theta\)</span> can also be constructed.</p>
</section>
<section id="mean-square-error" class="level2">
<h2 class="anchored" data-anchor-id="mean-square-error">Mean Square Error</h2>
<p>Combine variance and bias to assess quality of an estimator: MSE</p>
<p>For an estimator <span class="math inline">\(\hat{\theta}\)</span>, <span class="math display">\[
\operatorname{MSE}(\hat{\theta})=\mathcal{E}\left((\hat{\theta}-\theta)^2\right)=\operatorname{var}(\hat{\theta})+\operatorname{bias}(\hat{\theta})^2
\]</span></p>
</section>
<section id="mse" class="level2">
<h2 class="anchored" data-anchor-id="mse">MSE</h2>
<p>Proposition 6.4 MSE <span class="math inline">\([\hat{\theta}]=\operatorname{Var}[\hat{\theta}]+\operatorname{Bias}[\hat{\theta}]^2\)</span>. Proof <span class="math display">\[
\begin{aligned}
\operatorname{MSE}[\hat{\theta}] &amp; =\mathrm{E}\left[(\hat{\theta}-\theta)^2\right] \\
&amp; =\mathrm{E}\left[(\hat{\theta}-\mathrm{E}[\hat{\theta}]+\mathrm{E}[\hat{\theta}]-\theta)^2\right] \\
&amp; =\mathrm{E}\left[((\hat{\theta}-\mathrm{E}[\hat{\theta}])+(\mathrm{E}[\hat{\theta}]-\theta))^2\right] \\
&amp; =\mathrm{E}\left[(\hat{\theta}-\mathrm{E}[\hat{\theta}])^2\right]+2 \mathrm{E}[\hat{\theta}-\mathrm{E}[\hat{\theta}]](\mathrm{E}[\hat{\theta}]-\theta)+(\mathrm{E}[\hat{\theta}]-\theta)^2 \\
&amp; =\operatorname{Var}[\hat{\theta}]+(\operatorname{Bias}[\hat{\theta}])^2
\end{aligned}
\]</span> Also, if <span class="math inline">\(\hat{\theta}\)</span> is unbiased, then <span class="math inline">\(\operatorname{MSE}[\theta]=\operatorname{Var}[\hat{\theta}]\)</span>. So, the unbiased estimator <span class="math inline">\(\hat{\theta}_1\)</span> of <span class="math inline">\(\theta\)</span> is more efficient than the unbiased estimator <span class="math inline">\(\hat{\theta}_2\)</span> if and only if <span class="math inline">\(\operatorname{MSE}\left[\hat{\theta}_1\right]&lt;\operatorname{MSE}\left[\hat{\theta}_2\right]\)</span>.</p>
</section>
<section id="mse-1" class="level2">
<h2 class="anchored" data-anchor-id="mse-1">MSE</h2>
<p>Definition. Let <span class="math inline">\(T\)</span> be an estimator for a parameter <span class="math inline">\(\theta\)</span>. The mean squared error of <span class="math inline">\(T\)</span> is the number <span class="math inline">\(\operatorname{MSE}(T)=\mathrm{E}\left[(T-\theta)^2\right]\)</span>. According to this criterion, an estimator <span class="math inline">\(T_1\)</span> performs better than an estimator <span class="math inline">\(T_2\)</span> if <span class="math inline">\(\operatorname{MSE}\left(T_1\right)&lt;\operatorname{MSE}\left(T_2\right)\)</span>. Note that <span class="math display">\[
\begin{aligned}
\operatorname{MSE}(T) &amp; =\mathrm{E}\left[(T-\theta)^2\right] \\
&amp; =\mathrm{E}\left[(T-\mathrm{E}[T]+\mathrm{E}[T]-\theta)^2\right] \\
&amp; =\mathrm{E}\left[(T-\mathrm{E}[T])^2\right]+2 \mathrm{E}[T-\mathrm{E}[T]](\mathrm{E}[T]-\theta)+(\mathrm{E}[T]-\theta)^2 \\
&amp; =\operatorname{Var}(T)+(\mathrm{E}[T]-\theta)^2 .
\end{aligned}
\]</span></p>
</section>
<section id="efficiency" class="level2">
<h2 class="anchored" data-anchor-id="efficiency">Efficiency</h2>
<p>EFFICIENCY. Let <span class="math inline">\(T_1\)</span> and <span class="math inline">\(T_2\)</span> be two unbiased estimators for the same parameter <span class="math inline">\(\theta\)</span>. Then estimator <span class="math inline">\(T_2\)</span> is called more efficient than estimator <span class="math inline">\(T_1\)</span> if <span class="math inline">\(\operatorname{Var}\left(T_2\right)&lt;\operatorname{Var}\left(T_1\right)\)</span>, irrespective of the value of <span class="math inline">\(\theta\)</span>.</p>
</section>
<section id="note" class="level2">
<h2 class="anchored" data-anchor-id="note">Note</h2>
<p>So the MSE of <span class="math inline">\(T\)</span> turns out to be the variance of <span class="math inline">\(T\)</span> plus the square of the bias of <span class="math inline">\(T\)</span>. In particular, when <span class="math inline">\(T\)</span> is unbiased, the MSE of <span class="math inline">\(T\)</span> is just the variance of <span class="math inline">\(T\)</span>. This means that we already used mean squared errors to compare the estimators <span class="math inline">\(T_1\)</span> and <span class="math inline">\(T_2\)</span> in the previous section. We extend the notion of efficiency by saying that estimator <span class="math inline">\(T_2\)</span> is more efficient than estimator <span class="math inline">\(T_1\)</span> (for the same parameter of interest), if the MSE of <span class="math inline">\(T_2\)</span> is smaller than the MSE of <span class="math inline">\(T_1\)</span>.</p>
</section>
<section id="efficiency-1" class="level2">
<h2 class="anchored" data-anchor-id="efficiency-1">Efficiency</h2>
<ul>
<li><p>Given two estimators <span class="math inline">\(\hat{\theta}_1, \hat{\theta}_2\)</span> for the same parameter. If both are unbiased, the one with smaller variance is better (“more efficient”).</p></li>
<li><p>Relative Efficiency of <span class="math inline">\(\hat{\theta}_1\)</span> wrt. <span class="math inline">\(\hat{\theta}_2\)</span></p></li>
<li><p>Assuming <span class="math inline">\(\mathcal{E}\left(\hat{\theta}_1\right)=\mathcal{E}\left(\hat{\theta}_2\right)=\theta\)</span>, this is defined as <span class="math display">\[
E=\operatorname{var}\left(\hat{\theta}_2\right) / \operatorname{var}\left(\hat{\theta}_1\right)
\]</span></p></li>
<li><p>If <span class="math inline">\(\hat{\theta}_2\)</span> is used instead of <span class="math inline">\(\hat{\theta}_1\)</span>, the sample size must be increased by a factor <span class="math inline">\(E\)</span> to get the same accuracy.</p></li>
</ul>
</section>
<section id="example-mean-and-median" class="level2">
<h2 class="anchored" data-anchor-id="example-mean-and-median">Example: Mean and Median</h2>
<ul>
<li><p>Consider data from a normal distribution, <span class="math inline">\(N(\mu, 1)\)</span>. Can estimate <span class="math inline">\(\mu\)</span> in two ways from a sample <span class="math inline">\(x=\left(x_1, \ldots, x_n\right)\)</span> : <span class="math display">\[
\hat{\mu}_1=\bar{x}, \quad \hat{\mu}_2=\operatorname{median}(x)
\]</span></p></li>
<li><p>What is the relative efficiency?</p></li>
</ul>
</section>
<section id="example-5" class="level2">
<h2 class="anchored" data-anchor-id="example-5">Example</h2>
<p>Exercise 6.4 #28 in Chihara/Hesterberg. 28. Let <span class="math inline">\(\hat{\theta}_1\)</span> and <span class="math inline">\(\hat{\theta}_2\)</span> be two estimators of <span class="math inline">\(\theta\)</span> with <span class="math inline">\(\mathrm{E}\left[\hat{\theta}_1\right]=0.9 \theta\)</span> and <span class="math inline">\(\mathrm{E}\left[\hat{\theta}_2\right]=1.2 \theta\)</span>. Also, suppose <span class="math inline">\(\operatorname{Var}\left[\hat{\theta}_1\right]=3\)</span> and <span class="math inline">\(\operatorname{Var}\left[\hat{\theta}_2\right]=2\)</span>. Find two unbiased estimators of <span class="math inline">\(\theta\)</span> and determine which one is more efficient. Solution Set <span class="math inline">\(\tilde{\theta}_1=\frac{\hat{\theta}_1}{0.9}\)</span> and <span class="math inline">\(\tilde{\theta}_2=\frac{\hat{\theta}_2}{1.2}\)</span>. Then <span class="math inline">\(E\left(\tilde{\theta}_1\right)=\frac{0.9}{0.9} \theta=\theta\)</span> and similarly <span class="math inline">\(E\left(\tilde{\theta}_2\right)=\theta\)</span>, so both are unbiased. Also, <span class="math inline">\(\operatorname{Var}\left(\tilde{\theta}_1\right)=\frac{3}{0.9^2} \approx 3.76\)</span> and <span class="math inline">\(\operatorname{Var}\left(\tilde{\theta}_2\right)=\frac{2}{1.2^2} \approx 1.39\)</span>, so <span class="math inline">\(\tilde{\theta}_2\)</span> is more efficient.</p>
</section>
<section id="calculation-3" class="level2">
<h2 class="anchored" data-anchor-id="calculation-3">Calculation</h2>
<p><img src="images/2023-10-16-09-14-11.png" class="img-fluid"></p>
</section>
<section id="bias-variance-trade-off---a-look-ahead" class="level2">
<h2 class="anchored" data-anchor-id="bias-variance-trade-off---a-look-ahead">Bias-Variance Trade Off - a look ahead</h2>
<p>Consider <span class="math inline">\(M S E^2=\)</span> bias <span class="math inline">\(^2+\)</span> var . More complex models (with more parameters) tend to have less bias (are more flexible) and more variance (are more susceptible to noise).</p>
<p><img src="images/2023-10-16-09-13-23.png" class="img-fluid"></p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>